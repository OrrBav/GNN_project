{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a90b41d",
   "metadata": {},
   "source": [
    "# Geneal code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d7d4703",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/dsi/sbm/OrrBavly/colon_data/graphs/G_overlap_lcc_254_gnn_new.pkl\", 'rb') as f:\n",
    "    G_new = pickle.load(f)\n",
    "with open(\"/dsi/sbm/OrrBavly/colon_data/graphs/G_overlap_lcc_246_gnn.pkl\", 'rb') as f:\n",
    "    G_old = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5565d281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Nodes in old but not new: 13\n",
      "Number of Nodes in new but not old: 21\n",
      "Number of shared nodes: 233\n",
      "Number of Edges in old but not new: 308\n",
      "Number of Edges in new but not old: 316\n",
      "Number of shared edges: 297\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old LCC size: 246\n",
      "New LCC size: 254\n"
     ]
    }
   ],
   "source": [
    "# Node and edge sets\n",
    "old_nodes = set(G_old.nodes())\n",
    "new_nodes = set(G_new.nodes())\n",
    "\n",
    "old_edges = set(G_old.edges())\n",
    "new_edges = set(G_new.edges())\n",
    "\n",
    "print(\"Number of Nodes in old but not new:\", len(old_nodes - new_nodes))\n",
    "print(\"Number of Nodes in new but not old:\", len(new_nodes - old_nodes))\n",
    "print(\"Number of shared nodes:\", len(old_nodes & new_nodes))\n",
    "\n",
    "print(\"Number of Edges in old but not new:\", len(old_edges - new_edges))\n",
    "print(\"Number of Edges in new but not old:\", len(new_edges - old_edges))\n",
    "print(\"Number of shared edges:\", len(old_edges & new_edges))\n",
    "\n",
    "# Degree comparison\n",
    "import matplotlib.pyplot as plt\n",
    "old_degrees = list(dict(G_old.degree()).values())\n",
    "new_degrees = list(dict(G_new.degree()).values())\n",
    "\n",
    "plt.hist([old_degrees, new_degrees],\n",
    "         bins=20, label=[\"Old\", \"New\"], alpha=0.7)\n",
    "plt.legend()\n",
    "plt.title(\"Node Degree Distributions\")\n",
    "plt.xlabel(\"Node Degree\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "# Connected components\n",
    "print(\"Old LCC size:\", len(max(nx.connected_components(G_old), key=len)))\n",
    "print(\"New LCC size:\", len(max(nx.connected_components(G_new), key=len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1f4e961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degrees of lost nodes (in old graph): [3, 1, 4, 1, 8, 1, 8, 1, 8, 1, 9, 4, 4]\n",
      "Degrees of gained nodes (in new graph): [1, 1, 1, 3, 4, 2, 3, 2, 2, 1, 2, 9, 2, 5, 1, 1, 1, 8, 3, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "lost_nodes = list(old_nodes - new_nodes)\n",
    "gained_nodes = list(new_nodes - old_nodes)\n",
    "\n",
    "print(\"Degrees of lost nodes (in old graph):\", [G_old.degree(n) for n in lost_nodes])\n",
    "print(\"Degrees of gained nodes (in new graph):\", [G_new.degree(n) for n in gained_nodes])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "245878bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lost edges: 308\n",
      "Lost edges connecting high-degree nodes (>5):\n",
      "CASSPGQTNQPQHF (deg=8) -- CASSLSGRSSYEQYF (deg=8)\n",
      "CASSPSDTQYF (deg=6) -- CASSLGSLYGYTF (deg=8)\n",
      "CASSFSGGNTEAFF (deg=7) -- CASSGTNEKLFF (deg=8)\n",
      "CASSYDRDQPQHF (deg=10) -- CSVDRANTGELFF (deg=9)\n",
      "CASSPTGGGQPQHF (deg=15) -- CASSGNEQFF (deg=12)\n",
      "CASSLQGRETQYF (deg=11) -- CASSLGGIYEQYF (deg=13)\n",
      "CASSLVPGNTEAFF (deg=9) -- CASSRTGYGYTF (deg=15)\n",
      "CASSLGRGYNEQFF (deg=8) -- CASSLAGAEETQYF (deg=8)\n",
      "CASSRRENTEAFF (deg=9) -- CASSLGQGFSYEQYF (deg=9)\n",
      "CASSLDGGSGNTIYF (deg=8) -- CASSIGETQYF (deg=12)\n",
      "CASSYDRDQPQHF (deg=10) -- CASSPLAGGPTDTQYF (deg=9)\n",
      "CASSLQGRETQYF (deg=11) -- CASSPGHSTDTQYF (deg=15)\n",
      "CASSLVPGNTEAFF (deg=9) -- CASSRGQGGSYEQYF (deg=12)\n",
      "CASSLGGQGTDTQYF (deg=11) -- CASSTTGNTEAFF (deg=7)\n",
      "CASSRRENTEAFF (deg=9) -- CASSLNFYEQYF (deg=14)\n",
      "CASSRGADTQYF (deg=6) -- CASSFGQGTYEQYF (deg=15)\n",
      "CASSSGTAYGYTF (deg=6) -- CASSRQSTDTQYF (deg=9)\n",
      "CASSLLSSGANVLTF (deg=6) -- CASSLSSGNNEQFF (deg=9)\n",
      "CASSLQGRETQYF (deg=11) -- CASSLGGGTYNEQFF (deg=7)\n",
      "CASSLLSSGANVLTF (deg=6) -- CASSGGRGTDTQYF (deg=12)\n",
      "CASGNSNQPQHF (deg=7) -- CASSLAPGRNTEAFF (deg=8)\n",
      "CASSGTNEKLFF (deg=8) -- CASSLAGAEETQYF (deg=8)\n",
      "CASSLGQGNEQYF (deg=7) -- CASSLGGLEQYF (deg=13)\n",
      "CASSLERANEKLFF (deg=7) -- CASSPGAGQETQYF (deg=9)\n",
      "CASSLGRGSGANVLTF (deg=16) -- CASSLQTQETQYF (deg=7)\n",
      "CASSLNFYEQYF (deg=14) -- CASSFTGNQPQHF (deg=11)\n",
      "CASSLNTQETQYF (deg=8) -- CASSFGQGTYEQYF (deg=15)\n",
      "CASSGGRGTDTQYF (deg=12) -- CASSLSSGNNEQFF (deg=9)\n",
      "CASSFSGGNTEAFF (deg=7) -- CASSLGRGYNEQFF (deg=8)\n",
      "CASSPTGGGQPQHF (deg=15) -- CASSFGQGTYEQYF (deg=15)\n",
      "CASSPTGANQPQHF (deg=8) -- CASSVGPQETQYF (deg=6)\n",
      "CASSLGGVTEAFF (deg=10) -- CASSRTGYGYTF (deg=15)\n",
      "CASSFGQGTYEQYF (deg=15) -- CASSGNEQFF (deg=12)\n",
      "CASSLQTQETQYF (deg=7) -- CASSRTGYGYTF (deg=15)\n",
      "CASSLVQGMNTEAFF (deg=13) -- CASSFGQGTYEQYF (deg=15)\n",
      "CASSRQSTDTQYF (deg=9) -- CASSLGSLYGYTF (deg=8)\n",
      "CASSLLRDTQYF (deg=7) -- CASSLGQGFSYEQYF (deg=9)\n",
      "CASSHSTDTQYF (deg=6) -- CASSIGETQYF (deg=12)\n",
      "CSVDRANTGELFF (deg=9) -- CASSLGGVNQPQHF (deg=7)\n",
      "CASSYDRDQPQHF (deg=10) -- CASSLGGVNQPQHF (deg=7)\n",
      "CASSLGRGSGANVLTF (deg=16) -- CASSSQGSTEAFF (deg=10)\n",
      "CASSPTGGNTEAFF (deg=6) -- CASSLVPGNTEAFF (deg=9)\n",
      "CASSLGGIYEQYF (deg=13) -- CASSFGANTEAFF (deg=9)\n",
      "CASSPSDTQYF (deg=6) -- CASSRQSTDTQYF (deg=9)\n",
      "CASSLGRGSGANVLTF (deg=16) -- CASSPRGGNSPLHF (deg=7)\n",
      "CASSSGGLNTEAFF (deg=6) -- CASSIGETQYF (deg=12)\n",
      "CASSVGYTGELFF (deg=13) -- CASSFAGTGELFF (deg=13)\n",
      "CASSPTGGGQPQHF (deg=15) -- CASSVGYTGELFF (deg=13)\n",
      "CASSPGLAGADEQFF (deg=11) -- CASSGNEQFF (deg=12)\n",
      "CASSPGAGQETQYF (deg=9) -- CASSGTNEKLFF (deg=8)\n",
      "CASSLGRGYNEQFF (deg=8) -- CASSGTNEKLFF (deg=8)\n",
      "CASSLVQGMNTEAFF (deg=13) -- CASSVGYTGELFF (deg=13)\n",
      "CASSLQTQETQYF (deg=7) -- CASSLVPGNTEAFF (deg=9)\n",
      "CASSPLAGGPTDTQYF (deg=9) -- CASSLGGVNQPQHF (deg=7)\n",
      "CASSLEVYGYTF (deg=11) -- CASSYGGSTDTQYF (deg=6)\n",
      "CASSLAGAEAFF (deg=9) -- CASSRTGYGYTF (deg=15)\n",
      "CASSLGRGSGANVLTF (deg=16) -- CASSRTGYGYTF (deg=15)\n",
      "CASSTTGNTEAFF (deg=7) -- CASSIGETQYF (deg=12)\n",
      "CASSRQGAGELFF (deg=7) -- CASSPGHSTDTQYF (deg=15)\n",
      "CASSPLAGGPTDTQYF (deg=9) -- CASSFTGNQPQHF (deg=11)\n",
      "CASSLGGIYEQYF (deg=13) -- CASSRTGYGYTF (deg=15)\n",
      "CASSYLETQYF (deg=6) -- CASSIGETQYF (deg=12)\n",
      "CASSYLETQYF (deg=6) -- CASSSPPPYEQYF (deg=12)\n",
      "CASSSGTAYGYTF (deg=6) -- CASSLVREQFF (deg=6)\n",
      "CASSLLRDTQYF (deg=7) -- CASSPLAGGPTDTQYF (deg=9)\n",
      "CASSPGHSTDTQYF (deg=15) -- CASSRTGYGYTF (deg=15)\n",
      "CASSLGGIYEQYF (deg=13) -- CASSRGQGGSYEQYF (deg=12)\n",
      "CASSGTNEKLFF (deg=8) -- CASSPGRGNTIYF (deg=7)\n",
      "CASSLERANEKLFF (deg=7) -- CASSGTNEKLFF (deg=8)\n",
      "CASSFGTGYEQYF (deg=11) -- CASSLGGLEQYF (deg=13)\n",
      "CASSLGQGNEQYF (deg=7) -- CASSSGQGEAFF (deg=7)\n",
      "CASSPGHSTDTQYF (deg=15) -- CASSRGQGGSYEQYF (deg=12)\n",
      "CASSLQGRETQYF (deg=11) -- CASSRTGYGYTF (deg=15)\n",
      "CASSLERANEKLFF (deg=7) -- CASSPGRGNTIYF (deg=7)\n",
      "CASSLEVYGYTF (deg=11) -- CASSLGTGDTQYF (deg=6)\n",
      "CASSVGYTGELFF (deg=13) -- CASSGNEQFF (deg=12)\n",
      "CASSSQGSTEAFF (deg=10) -- CASSFGANTEAFF (deg=9)\n",
      "CASSLGRGSGANVLTF (deg=16) -- CASSLGGVTEAFF (deg=10)\n",
      "CASSLQGRETQYF (deg=11) -- CASSRGQGGSYEQYF (deg=12)\n",
      "CASSRRENTEAFF (deg=9) -- CSVDRANTGELFF (deg=9)\n",
      "CASSRDRGNEKLFF (deg=14) -- CASSFTGNQPQHF (deg=11)\n",
      "CASSLGGQGTDTQYF (deg=11) -- CASSHSTDTQYF (deg=6)\n",
      "CASSPTGGGQPQHF (deg=15) -- CASSFAGTGELFF (deg=13)\n",
      "CASSRRENTEAFF (deg=9) -- CASSPLAGGPTDTQYF (deg=9)\n",
      "CASSLVQGMNTEAFF (deg=13) -- CASSFAGTGELFF (deg=13)\n",
      "CASSLGRGSGANVLTF (deg=16) -- CASSFGANTEAFF (deg=9)\n",
      "CASSLGAEETQYF (deg=11) -- CASSHSTDTQYF (deg=6)\n",
      "CASSSPPPYEQYF (deg=12) -- CASSIGETQYF (deg=12)\n",
      "CASSVGDQPQHF (deg=12) -- CASSQGSEAFF (deg=6)\n",
      "CSVDRANTGELFF (deg=9) -- CASSFTGNQPQHF (deg=11)\n",
      "CASSYDRDQPQHF (deg=10) -- CASSFTGNQPQHF (deg=11)\n",
      "CASSLEGPSYEQYF (deg=10) -- CASSLVREQFF (deg=6)\n",
      "CASSYDRDQPQHF (deg=10) -- CASSLNFYEQYF (deg=14)\n",
      "CASSLERANEKLFF (deg=7) -- CASSLGRGYNEQFF (deg=8)\n",
      "CASSLGGQGTDTQYF (deg=11) -- CASSIGETQYF (deg=12)\n",
      "CASSLGGQGTDTQYF (deg=11) -- CASSSPPPYEQYF (deg=12)\n",
      "CASSLVQGGNSPLHF (deg=13) -- CASSFGTGYEQYF (deg=11)\n",
      "CASSLVPGNTEAFF (deg=9) -- CASSPGHSTDTQYF (deg=15)\n",
      "CASSFAGTGELFF (deg=13) -- CASRRTATNEKLFF (deg=8)\n",
      "CASGTANTGELFF (deg=7) -- CASSQGGQGNQPQHF (deg=7)\n",
      "CASSLGGVTEAFF (deg=10) -- CASSLVPGNTEAFF (deg=9)\n",
      "CASSLQGRETQYF (deg=11) -- CASSFGANTEAFF (deg=9)\n",
      "CASSVGYTGELFF (deg=13) -- CASRRTATNEKLFF (deg=8)\n",
      "CASSLGRGYNEQFF (deg=8) -- CASSPGRGNTIYF (deg=7)\n",
      "CASSTTGNTEAFF (deg=7) -- CASSLGDRDTGELFF (deg=12)\n",
      "CASSLLRDTQYF (deg=7) -- CSVDRANTGELFF (deg=9)\n",
      "CASSLNTQETQYF (deg=8) -- CASSGNEQFF (deg=12)\n",
      "CASSFAGTGELFF (deg=13) -- CASSLGRDTGELFF (deg=8)\n",
      "CASSYLETQYF (deg=6) -- CASSLGDRDTGELFF (deg=12)\n",
      "CASSVGYTGELFF (deg=13) -- CASSLGRDTGELFF (deg=8)\n",
      "CASSRRENTEAFF (deg=9) -- CASSLGGVNQPQHF (deg=7)\n",
      "CASSLGAEETQYF (deg=11) -- CASSLDGGSGNTIYF (deg=8)\n",
      "CASSLVQGMNTEAFF (deg=13) -- CASSGNEQFF (deg=12)\n",
      "CASSRRENTEAFF (deg=9) -- CASSFTGNQPQHF (deg=11)\n",
      "CASSVGDQPQHF (deg=12) -- CASSLGGLEQYF (deg=13)\n",
      "CASSRQGAGELFF (deg=7) -- CASSRTGYGYTF (deg=15)\n",
      "CASSPGLAGADEQFF (deg=11) -- CASSFGQGTYEQYF (deg=15)\n",
      "CASSRDRGNEKLFF (deg=14) -- CASSLGQGFSYEQYF (deg=9)\n",
      "CASSSGTAYGYTF (deg=6) -- CASSLEGPSYEQYF (deg=10)\n",
      "CASSLGGGTYNEQFF (deg=7) -- CASSRTGYGYTF (deg=15)\n",
      "CASSLGRGSGANVLTF (deg=16) -- CASSLVPGNTEAFF (deg=9)\n",
      "CASSPTGGNTEAFF (deg=6) -- CASSRGQGGSYEQYF (deg=12)\n",
      "CASSLGGIYEQYF (deg=13) -- CASSLVPGNTEAFF (deg=9)\n",
      "CASSLNTQETQYF (deg=8) -- CASRRTATNEKLFF (deg=8)\n",
      "CSVDRANTGELFF (deg=9) -- CASSLGQGFSYEQYF (deg=9)\n",
      "CASSRGQGGSYEQYF (deg=12) -- CASSRTGYGYTF (deg=15)\n",
      "CASSYDRDQPQHF (deg=10) -- CASSLGQGFSYEQYF (deg=9)\n",
      "CASSPTGGGQPQHF (deg=15) -- CASRRTATNEKLFF (deg=8)\n",
      "CASSLVQGGNSPLHF (deg=13) -- CASSLGGLEQYF (deg=13)\n",
      "CASSSPPPYEQYF (deg=12) -- CASSLGDRDTGELFF (deg=12)\n",
      "CASSLEVYGYTF (deg=11) -- CASSLGGLEQYF (deg=13)\n",
      "CASSLSGSQPQHF (deg=10) -- CASSLAPGRNTEAFF (deg=8)\n",
      "CASSLVQGMNTEAFF (deg=13) -- CASRRTATNEKLFF (deg=8)\n",
      "CASSLVPGNTEAFF (deg=9) -- CASSFGANTEAFF (deg=9)\n",
      "CASSPTGGGQPQHF (deg=15) -- CASSLNTQETQYF (deg=8)\n",
      "CASSLGGQGTDTQYF (deg=11) -- CASSLGDRDTGELFF (deg=12)\n",
      "CASSPRGGNSPLHF (deg=7) -- CASSRTGYGYTF (deg=15)\n",
      "CASSPTGGGQPQHF (deg=15) -- CASSLGRDTGELFF (deg=8)\n",
      "CASSLLRDTQYF (deg=7) -- CASSFTGNQPQHF (deg=11)\n",
      "CASSLLRDTQYF (deg=7) -- CASSLNFYEQYF (deg=14)\n",
      "CASSLGAEETQYF (deg=11) -- CASSIGETQYF (deg=12)\n",
      "CASSPGQTNQPQHF (deg=8) -- CASSLGSLYGYTF (deg=8)\n",
      "CASSLVQGMNTEAFF (deg=13) -- CASSLGRDTGELFF (deg=8)\n",
      "CASSPSDTQYF (deg=6) -- CASSLEGPSYEQYF (deg=10)\n",
      "CASSYLETQYF (deg=6) -- CASSLGGQGTDTQYF (deg=11)\n",
      "CASSQGSEAFF (deg=6) -- CASSLGGLEQYF (deg=13)\n",
      "CASSLGTGDTQYF (deg=6) -- CASSLGGLEQYF (deg=13)\n",
      "CASSLGRGYNEQFF (deg=8) -- CASSPGAGQETQYF (deg=9)\n"
     ]
    }
   ],
   "source": [
    "lost_edges = list(set(G_old.edges()) - set(G_new.edges()))\n",
    "\n",
    "# Print basic stats:\n",
    "print(f\"Number of lost edges: {len(lost_edges)}\")\n",
    "\n",
    "# Degree of both endpoints in old graph\n",
    "critical_lost_edges = []\n",
    "for u, v in lost_edges:\n",
    "    deg_u = G_old.degree(u)\n",
    "    deg_v = G_old.degree(v)\n",
    "    if deg_u > 5 and deg_v > 5:  # Change threshold as needed\n",
    "        critical_lost_edges.append((u, v, deg_u, deg_v))\n",
    "\n",
    "print(\"Lost edges connecting high-degree nodes (>5):\")\n",
    "for u, v, deg_u, deg_v in critical_lost_edges:\n",
    "    print(f\"{u} (deg={deg_u}) -- {v} (deg={deg_v})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc646033",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixcr_dir = \"/dsi/sbm/OrrBavly/colon_data/new_mixcr/TRB/\"\n",
    "meta_file = \"/home/dsi/orrbavly/GNN_project/data/colon_meta_time.csv\"\n",
    "\n",
    "time_threshold = 750\n",
    "meta_df = pd.read_csv(meta_file)\n",
    "meta_df = meta_df[[\"Sample_ID\", \"extraction_time\"]]\n",
    "\n",
    "colon_meta = pd.read_csv(\"/home/dsi/orrbavly/GNN_project/data/metadata/colon_meta.csv\")\n",
    "colon_meta = colon_meta.loc[:, ~colon_meta.columns.str.contains('^Unnamed')]\n",
    "exclude_samples = ['P2-S8', 'P2-S9', 'P2-S19', 'P8-S18'] # 'P4-S1'\n",
    "relevant_values = ['0', '1', '1a', '1b', '2']\n",
    "colon_meta = colon_meta[colon_meta['N'].isin(relevant_values)]\n",
    "colon_meta = colon_meta[~colon_meta['sample_id'].isin(exclude_samples)].copy()\n",
    "valid_samples = colon_meta[colon_meta['N'].isin(['0', '1', '1a', '1b', '2'])]['sample_id'].tolist()\n",
    "\n",
    "\n",
    "# Columns to keep from MiXCR files\n",
    "mixcr_cols = [\n",
    "    \"aaSeqCDR3\", \"nSeqCDR3\", \"readCount\", 'readFraction',\n",
    "    \"allVHitsWithScore\", \"allDHitsWithScore\", \"allJHitsWithScore\"\n",
    "]\n",
    "clonotype_dfs = []\n",
    "\n",
    "for fname in os.listdir(mixcr_dir):\n",
    "    if fname.endswith(\".tsv\") and os.path.isfile(os.path.join(mixcr_dir, fname)):\n",
    "        sample_prefix = fname.split(\"_\")[0]\n",
    "        file_path = os.path.join(mixcr_dir, fname)\n",
    "        if sample_prefix in valid_samples:\n",
    "            df = pd.read_csv(file_path, sep=\"\\t\", usecols=mixcr_cols)\n",
    "            df[\"Sample_ID\"] = sample_prefix\n",
    "            clonotype_dfs.append(df)\n",
    "\n",
    "clonotype_df_test = pd.concat(clonotype_dfs, ignore_index=True)\n",
    "\n",
    "# Merge with metadata to get extraction time\n",
    "clonotype_df_test = clonotype_df_test.merge(meta_df, on=\"Sample_ID\", how=\"inner\")\n",
    "# Create 'group' column: fast vs slow\n",
    "clonotype_df_test[\"group\"] = clonotype_df_test[\"extraction_time\"].apply(lambda x: \"fast\" if x <= time_threshold else \"slow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e0ac49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCRs from the removed sample that are now missing in the new graph:\n",
      "set()\n",
      "Count: 0\n"
     ]
    }
   ],
   "source": [
    "removed_sample_id = 'P3-S11'  # change as appropriate\n",
    "tcrs_in_removed_sample = set(clonotype_df_test[clonotype_df_test['Sample_ID'] == removed_sample_id]['aaSeqCDR3'])\n",
    "\n",
    "# Which of these TCRs are among the lost nodes?\n",
    "lost_nodes = set(G_old.nodes()) - set(G_new.nodes())\n",
    "unique_to_removed = tcrs_in_removed_sample & lost_nodes\n",
    "\n",
    "print(\"TCRs from the removed sample that are now missing in the new graph:\")\n",
    "print(unique_to_removed)\n",
    "print(f\"Count: {len(unique_to_removed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78968fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lost nodes and the number of samples they appeared in:\n",
      "{'CASSLGRGYNEQFF': 27, 'CASSGTNEKLFF': 11, 'CASSLAPGRNTEAFF': 13, 'CASSGQSSYNEQFF': 16, 'CASSLRGPYEQYF': 47, 'CASSPEGNYGYTF': 24, 'CASSLVPGNTEAFF': 30, 'CASSPWTGGSYEQYF': 14, 'CASSYGGSYGYTF': 9, 'CASSLLQGAYEQYF': 20, 'CASSLGQVSYEQYF': 30, 'CASSWGQGTGELFF': 26}\n"
     ]
    }
   ],
   "source": [
    "# For each lost node, count the number of samples it appeared in (before removal)\n",
    "lost_node_counts = {}\n",
    "for node in lost_nodes:\n",
    "    count = clonotype_df_test[clonotype_df_test['aaSeqCDR3'] == node]['Sample_ID'].nunique()\n",
    "    lost_node_counts[node] = count\n",
    "\n",
    "print(\"Lost nodes and the number of samples they appeared in:\")\n",
    "print(lost_node_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b52a70",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e9463a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1' # This will force CUDA to report errors immediately when they happen, giving you the correct line number.\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import dgl\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59397b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/dsi/efroni-lab/sbm/OrrBavly/colon_data/graphs/G_overlap_lcc_246_gnn.pkl\", 'rb') as f:\n",
    "    G_overlap = pickle.load(f)\n",
    "node_meta = pd.read_csv(\"/home/dsi/orrbavly/GNN_project/outputs/colon/g_overlap_lcc_246_node_meta_gnn.csv\")\n",
    "node_meta = node_meta.loc[:, ~node_meta.columns.str.contains('^Unnamed')]\n",
    "colon_meta = pd.read_csv(\"/home/dsi/orrbavly/GNN_project/data/metadata/colon_meta.csv\")\n",
    "colon_meta = colon_meta.loc[:, ~colon_meta.columns.str.contains('^Unnamed')]\n",
    "valid_samples = colon_meta[colon_meta['N'].isin(['0', '1', '1a', '1b', '2'])]['sample_id'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bfeefff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sum_fast_weight",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_fast_weight",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sum_slow_weight",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_slow_weight",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "V_gene_fast",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "J_gene_fast",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "codon_diversity_fast",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sample_prevalence_fast",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "V_gene_slow",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "J_gene_slow",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "codon_diversity_slow",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sample_prevalence_slow",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "degree",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "betweenness",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "closeness",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "eigenvector",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "V_gene_fast_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "V_gene_slow_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "J_gene_fast_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "J_gene_slow_id",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "213f130f-1c6b-498d-b4e7-8900004279ee",
       "rows": [
        [
         "0",
         "CASSLLAGTYNEQFF",
         "0.6334496843762154",
         "0.6334496843762154",
         "0.6213563836446092",
         "0.6213563836446092",
         "TRBV11-1",
         "TRBJ2-1",
         "38",
         "46",
         "TRBV11-1",
         "TRBJ2-1",
         "11",
         "11",
         "1",
         "0.0",
         "0.1111615245009074",
         "1.603854845398193e-05",
         "3",
         "3",
         "6",
         "6"
        ],
        [
         "1",
         "CASSSVNQPQHF",
         "1.3709060543952245",
         "0.6854530271976123",
         "1.5906512665334347",
         "0.7953256332667173",
         "TRBV6-2",
         "TRBJ1-5",
         "9",
         "15",
         "TRBV12-3",
         "TRBJ1-5",
         "5",
         "5",
         "2",
         "0.0081632653061224",
         "0.1107094441934026",
         "0.0001389660565441",
         "25",
         "6",
         "4",
         "4"
        ],
        [
         "2",
         "CASSLGDRDTGELFF",
         "9.108662002885096",
         "0.7590551669070913",
         "9.535913189226168",
         "0.794659432435514",
         "TRBV12-3",
         "TRBJ2-2",
         "20",
         "29",
         "TRBV11-2",
         "TRBJ2-2",
         "6",
         "7",
         "12",
         "0.0035685898800653",
         "0.0957405236420476",
         "7.238639353277491e-05",
         "6",
         "4",
         "7",
         "7"
        ],
        [
         "3",
         "CASSFGGNEKLFF",
         "0.6174242798708044",
         "0.6174242798708044",
         "0.6648428658809944",
         "0.6648428658809944",
         "TRBV12-3",
         "TRBJ1-4",
         "20",
         "24",
         "TRBV12-3",
         "TRBJ1-4",
         "6",
         "6",
         "1",
         "0.0",
         "0.0898423175650898",
         "4.963791784794374e-07",
         "6",
         "6",
         "3",
         "3"
        ],
        [
         "4",
         "CASSLAGAEAFF",
         "6.572670909414647",
         "0.7302967677127385",
         "5.968545648662321",
         "0.6631717387402579",
         "TRBV11-2",
         "TRBJ1-1",
         "25",
         "29",
         "TRBV5-1",
         "TRBJ1-1",
         "5",
         "5",
         "9",
         "0.1218065001462414",
         "0.1248089658685685",
         "0.1275792617358839",
         "4",
         "22",
         "0",
         "0"
        ],
        [
         "5",
         "CASSRQGAGELFF",
         "4.838494145155567",
         "0.691213449307938",
         "4.589537231735042",
         "0.6556481759621489",
         "TRBV11-2",
         "TRBJ2-2",
         "19",
         "40",
         "TRBV11-2",
         "TRBJ2-2",
         "9",
         "12",
         "7",
         "0.0037429813534561",
         "0.1286764705882352",
         "0.1880383976829683",
         "4",
         "4",
         "7",
         "7"
        ],
        [
         "6",
         "CASSLGGGDYGYTF",
         "0.6171589599725572",
         "0.6171589599725572",
         "0.676078775180638",
         "0.676078775180638",
         "TRBV12-3",
         "TRBJ1-2",
         "24",
         "25",
         "TRBV12-3",
         "TRBJ1-2",
         "7",
         "6",
         "1",
         "0.0",
         "0.1046114432109308",
         "1.8684697784208866e-07",
         "6",
         "6",
         "1",
         "1"
        ],
        [
         "7",
         "CASSLGGLGQPQHF",
         "2.7877396826410137",
         "0.6969349206602534",
         "2.836345880952792",
         "0.7090864702381979",
         "TRBV12-3",
         "TRBJ1-5",
         "21",
         "20",
         "TRBV12-4",
         "TRBJ1-5",
         "6",
         "6",
         "4",
         "0.0523210210843815",
         "0.1003276003276003",
         "3.738761238201222e-08",
         "6",
         "7",
         "4",
         "4"
        ],
        [
         "8",
         "CASSLLQGAYEQYF",
         "0.6076283600906569",
         "0.6076283600906569",
         "0.6524930715242807",
         "0.6524930715242807",
         "TRBV11-2",
         "TRBJ2-7",
         "24",
         "23",
         "TRBV14",
         "TRBJ2-7",
         "6",
         "5",
         "1",
         "0.0",
         "0.1186440677966101",
         "4.852672117894975e-06",
         "4",
         "9",
         "11",
         "11"
        ],
        [
         "9",
         "CASSLGQDYEQYF",
         "0.6526863473799972",
         "0.6526863473799972",
         "0.6632346565089322",
         "0.6632346565089322",
         "TRBV10-1",
         "TRBJ2-7",
         "45",
         "53",
         "TRBV10-1",
         "TRBJ2-7",
         "9",
         "10",
         "1",
         "0.0",
         "0.096078431372549",
         "3.341063607453647e-05",
         "0",
         "0",
         "11",
         "11"
        ],
        [
         "10",
         "CASSLGQGAGEQYF",
         "1.458679757391504",
         "0.729339878695752",
         "1.2359427249641368",
         "0.6179713624820684",
         "TRBV11-2",
         "TRBJ2-7",
         "54",
         "46",
         "TRBV12-3",
         "TRBJ2-7",
         "12",
         "12",
         "2",
         "0.0",
         "0.0898752751283932",
         "6.064441326287675e-07",
         "4",
         "6",
         "11",
         "11"
        ],
        [
         "11",
         "CASSLGQGFYEQYF",
         "0.6272015435985039",
         "0.6272015435985039",
         "0.6851159682972462",
         "0.6851159682972462",
         "TRBV11-1",
         "TRBJ2-7",
         "41",
         "43",
         "TRBV11-3",
         "TRBJ2-7",
         "12",
         "9",
         "1",
         "0.0",
         "0.0917259453388244",
         "0.0003921998109915",
         "3",
         "5",
         "11",
         "11"
        ],
        [
         "12",
         "CASSLRGPYEQYF",
         "0.6576913939393041",
         "0.6576913939393041",
         "0.7245603282515702",
         "0.7245603282515702",
         "TRBV11-2",
         "TRBJ2-7",
         "54",
         "54",
         "TRBV12-3",
         "TRBJ2-7",
         "10",
         "8",
         "1",
         "0.0",
         "0.1004510045100451",
         "0.001301419485992",
         "4",
         "6",
         "11",
         "11"
        ],
        [
         "13",
         "CASSLGENTEAFF",
         "1.908320897671945",
         "0.6361069658906483",
         "2.064870143517612",
         "0.688290047839204",
         "TRBV12-3",
         "TRBJ1-1",
         "61",
         "113",
         "TRBV11-2",
         "TRBJ1-1",
         "34",
         "25",
         "3",
         "0.0",
         "0.0973767885532591",
         "8.639816918932e-05",
         "6",
         "4",
         "0",
         "0"
        ],
        [
         "14",
         "CASSYSSDTQYF",
         "3.4714584152798227",
         "0.6942916830559646",
         "3.3864864002717607",
         "0.6772972800543522",
         "TRBV2",
         "TRBJ2-3",
         "16",
         "30",
         "TRBV6-2",
         "TRBJ2-3",
         "5",
         "6",
         "5",
         "0.1000847944256995",
         "0.1243023845763571",
         "0.0013972855521585",
         "13",
         "25",
         "8",
         "8"
        ],
        [
         "15",
         "CASSSGLAADTQYF",
         "1.234288933987531",
         "0.6171444669937655",
         "1.5485407670952505",
         "0.7742703835476252",
         "TRBV5-1",
         "TRBJ2-3",
         "29",
         "44",
         "TRBV11-2",
         "TRBJ2-3",
         "9",
         "8",
         "2",
         "0.0162596186015389",
         "0.1104100946372239",
         "1.7473581132410435e-06",
         "22",
         "4",
         "8",
         "8"
        ],
        [
         "16",
         "CASSPGQNTEAFF",
         "1.252528151664874",
         "0.626264075832437",
         "1.4585585167450803",
         "0.7292792583725403",
         "TRBV18",
         "TRBJ1-1",
         "54",
         "103",
         "TRBV12-4",
         "TRBJ1-1",
         "19",
         "21",
         "2",
         "0.0081632653061224",
         "0.1041666666666666",
         "3.759973197709288e-05",
         "11",
         "7",
         "0",
         "0"
        ],
        [
         "17",
         "CASSPWTGGSYEQYF",
         "1.816635577084948",
         "0.6055451923616493",
         "2.273491696969674",
         "0.7578305656565579",
         "TRBV12-3",
         "TRBJ2-7",
         "15",
         "15",
         "TRBV19",
         "TRBJ2-7",
         "5",
         "5",
         "3",
         "0.0081632653061224",
         "0.1225612806403201",
         "0.0022097109816225",
         "6",
         "12",
         "11",
         "11"
        ],
        [
         "18",
         "CASSLYYGYTF",
         "0.6449898490677634",
         "0.6449898490677634",
         "0.941954231999726",
         "0.941954231999726",
         "TRBV11-2",
         "TRBJ1-2",
         "11",
         "28",
         "TRBV12-3",
         "TRBJ1-2",
         "5",
         "7",
         "1",
         "0.0",
         "0.0997150997150997",
         "1.3686707689108489e-05",
         "4",
         "6",
         "1",
         "1"
        ],
        [
         "19",
         "CASSLAPSSYNEQFF",
         "1.9713790535391404",
         "0.6571263511797135",
         "1.9488300426570928",
         "0.6496100142190309",
         "TRBV11-2",
         "TRBJ2-1",
         "36",
         "37",
         "TRBV27",
         "TRBJ2-1",
         "7",
         "8",
         "3",
         "0.0",
         "0.1100134710372698",
         "0.0001147651267234",
         "4",
         "16",
         "6",
         "6"
        ],
        [
         "20",
         "CASSLGGASTDTQYF",
         "0.6070351358559573",
         "0.6070351358559573",
         "0.6475862254631068",
         "0.6475862254631068",
         "TRBV7-2",
         "TRBJ2-3",
         "75",
         "65",
         "TRBV12-3",
         "TRBJ2-3",
         "20",
         "17",
         "1",
         "0.0",
         "0.08816120906801",
         "1.2269724217110186e-08",
         "26",
         "6",
         "8",
         "8"
        ],
        [
         "21",
         "CASSSGFYEQYF",
         "1.1734593685718844",
         "0.5867296842859422",
         "1.8159942617900475",
         "0.9079971308950237",
         "TRBV11-2",
         "TRBJ2-7",
         "28",
         "31",
         "TRBV12-3",
         "TRBJ2-7",
         "7",
         "7",
         "2",
         "0.0",
         "0.0874687611567297",
         "1.3250230168428562e-05",
         "4",
         "6",
         "11",
         "11"
        ],
        [
         "22",
         "CASSSTGNTEAFF",
         "3.3967926530439323",
         "0.8491981632609831",
         "2.518962258427996",
         "0.6297405646069989",
         "TRBV12-3",
         "TRBJ1-1",
         "60",
         "89",
         "TRBV10-1",
         "TRBJ1-1",
         "21",
         "22",
         "4",
         "0.0",
         "0.1120768526989935",
         "0.0020686371494005",
         "6",
         "0",
         "0",
         "0"
        ],
        [
         "23",
         "CASSLGGSGYTF",
         "3.917199056050876",
         "0.7834398112101753",
         "4.09160801385991",
         "0.818321602771982",
         "TRBV11-2",
         "TRBJ1-2",
         "25",
         "24",
         "TRBV12-3",
         "TRBJ1-2",
         "8",
         "8",
         "5",
         "0.0019356347423546",
         "0.1015754560530679",
         "2.9540864429768896e-08",
         "4",
         "6",
         "1",
         "1"
        ],
        [
         "24",
         "CASSGNEQFF",
         "8.38304521551501",
         "0.6985871012929175",
         "9.276705055068684",
         "0.7730587545890569",
         "TRBV10-2",
         "TRBJ2-1",
         "18",
         "39",
         "TRBV28",
         "TRBJ2-1",
         "4",
         "7",
         "12",
         "0.099950239201039",
         "0.1205708661417322",
         "0.0003408809317314",
         "1",
         "17",
         "6",
         "6"
        ],
        [
         "25",
         "CASSPRGGNSPLHF",
         "4.598908915965016",
         "0.6569869879950022",
         "4.87367798855953",
         "0.6962397126513614",
         "TRBV12-3",
         "TRBJ1-6",
         "13",
         "13",
         "TRBV12-3",
         "TRBJ1-6",
         "7",
         "7",
         "7",
         "0.0142331139157045",
         "0.1289473684210526",
         "0.172810533361512",
         "6",
         "6",
         "5",
         "5"
        ],
        [
         "26",
         "CASSPTGGGQPQHF",
         "10.422030630641116",
         "0.6948020420427412",
         "11.381399903120611",
         "0.7587599935413741",
         "TRBV12-3",
         "TRBJ1-5",
         "43",
         "41",
         "TRBV12-3",
         "TRBJ1-5",
         "9",
         "7",
         "15",
         "0.1238507422265376",
         "0.1231774761186526",
         "0.0003990827806874",
         "6",
         "6",
         "4",
         "4"
        ],
        [
         "27",
         "CASSPKGNTEAFF",
         "0.5948685284584858",
         "0.5948685284584858",
         "0.6596265275370917",
         "0.6596265275370917",
         "TRBV11-2",
         "TRBJ1-1",
         "12",
         "17",
         "TRBV12-3",
         "TRBJ1-1",
         "6",
         "7",
         "1",
         "0.0",
         "0.1003687013519049",
         "0.0012886718068997",
         "4",
         "6",
         "0",
         "0"
        ],
        [
         "28",
         "CASSLGGPGNTIYF",
         "1.4037381733356842",
         "0.7018690866678421",
         "1.279864194524505",
         "0.6399320972622525",
         "TRBV11-2",
         "TRBJ1-3",
         "40",
         "45",
         "TRBV12-3",
         "TRBJ1-3",
         "7",
         "7",
         "2",
         "0.0235735217844619",
         "0.1229302558956347",
         "0.0001638533567383",
         "4",
         "6",
         "2",
         "2"
        ],
        [
         "29",
         "CASSLGQVSYEQYF",
         "2.7646711772262904",
         "0.6911677943065726",
         "2.65916042965367",
         "0.6647901074134175",
         "TRBV12-4",
         "TRBJ2-7",
         "32",
         "34",
         "TRBV12-3",
         "TRBJ2-7",
         "5",
         "5",
         "4",
         "0.0268205642912901",
         "0.1116173120728929",
         "0.0132137793363298",
         "7",
         "6",
         "11",
         "11"
        ],
        [
         "30",
         "CASSSQGYNEQFF",
         "1.2424226300285222",
         "0.6212113150142611",
         "1.2257345885068287",
         "0.6128672942534144",
         "TRBV11-2",
         "TRBJ2-1",
         "34",
         "48",
         "TRBV12-3",
         "TRBJ2-1",
         "10",
         "7",
         "2",
         "0.0461003767598023",
         "0.1143790849673202",
         "0.0126895895431015",
         "4",
         "6",
         "6",
         "6"
        ],
        [
         "31",
         "CASSLGRGSGANVLTF",
         "12.00605261801988",
         "0.7503782886262425",
         "12.281702499142789",
         "0.7676064061964243",
         "TRBV11-3",
         "TRBJ2-6",
         "15",
         "15",
         "TRBV13",
         "TRBJ2-6",
         "7",
         "7",
         "16",
         "0.0489313824238791",
         "0.1364902506963788",
         "0.3437245420891207",
         "5",
         "8",
         "10",
         "10"
        ],
        [
         "32",
         "CASSLEDEQFF",
         "2.097208139884014",
         "0.699069379961338",
         "1.918458724950856",
         "0.6394862416502853",
         "TRBV11-2",
         "TRBJ2-1",
         "14",
         "31",
         "TRBV11-2",
         "TRBJ2-1",
         "7",
         "9",
         "3",
         "0.0451132119796169",
         "0.0989898989898989",
         "3.390590924713764e-07",
         "4",
         "4",
         "6",
         "6"
        ],
        [
         "33",
         "CASSLVGGMNTEAFF",
         "1.9653484556349008",
         "0.6551161518783002",
         "2.550680908207776",
         "0.850226969402592",
         "TRBV11-2",
         "TRBJ1-1",
         "29",
         "29",
         "TRBV11-2",
         "TRBJ1-1",
         "11",
         "10",
         "3",
         "0.0162930746068919",
         "0.0849220103986135",
         "3.0047664229824353e-10",
         "4",
         "4",
         "0",
         "0"
        ],
        [
         "34",
         "CASSLASYNEQFF",
         "1.5285451357827802",
         "0.7642725678913901",
         "1.3858677731991738",
         "0.6929338865995869",
         "TRBV7-2",
         "TRBJ2-1",
         "76",
         "80",
         "TRBV11-2",
         "TRBJ2-1",
         "18",
         "18",
         "2",
         "0.0081632653061224",
         "0.0993511759935117",
         "8.114218176171068e-06",
         "26",
         "4",
         "6",
         "6"
        ],
        [
         "35",
         "CASSLGAGMNTEAFF",
         "8.930691136655753",
         "0.8118810124232503",
         "8.911338992313853",
         "0.8101217265739866",
         "TRBV11-2",
         "TRBJ1-1",
         "28",
         "31",
         "TRBV12-4",
         "TRBJ1-1",
         "6",
         "6",
         "11",
         "9.03312144529943e-05",
         "0.0727218759275749",
         "3.050890110436821e-07",
         "4",
         "7",
         "0",
         "0"
        ],
        [
         "36",
         "CASSLTGTGGYEQYF",
         "3.497624664113129",
         "0.6995249328226258",
         "3.404491905044199",
         "0.6808983810088398",
         "TRBV12-3",
         "TRBJ2-7",
         "32",
         "40",
         "TRBV11-2",
         "TRBJ2-7",
         "5",
         "6",
         "5",
         "0.1786013137589145",
         "0.1382618510158013",
         "0.0025572254965328",
         "6",
         "4",
         "11",
         "11"
        ],
        [
         "37",
         "CASSYNTDTQYF",
         "2.613765340992628",
         "0.653441335248157",
         "2.6001008426269223",
         "0.6500252106567306",
         "TRBV11-2",
         "TRBJ2-3",
         "10",
         "36",
         "TRBV11-2",
         "TRBJ2-3",
         "3",
         "8",
         "4",
         "0.0037518520288677",
         "0.1298357180710121",
         "0.0026821059998387",
         "4",
         "4",
         "8",
         "8"
        ],
        [
         "38",
         "CASSLTSGYEQYF",
         "2.442326892343127",
         "0.6105817230857817",
         "2.893416389327274",
         "0.7233540973318187",
         "TRBV11-2",
         "TRBJ2-7",
         "42",
         "49",
         "TRBV12-3",
         "TRBJ2-7",
         "9",
         "10",
         "4",
         "0.1711431249199708",
         "0.1267459906880496",
         "1.8687714894231912e-05",
         "4",
         "6",
         "11",
         "11"
        ],
        [
         "39",
         "CASSLDYYNEQFF",
         "3.378473341971332",
         "0.6756946683942664",
         "3.644471827905626",
         "0.7288943655811252",
         "TRBV12-3",
         "TRBJ2-1",
         "16",
         "23",
         "TRBV5-1",
         "TRBJ2-1",
         "4",
         "5",
         "5",
         "0.0001003680160588",
         "0.0732655502392344",
         "9.034401262878426e-09",
         "6",
         "22",
         "6",
         "6"
        ],
        [
         "40",
         "CASSLGQPYGYTF",
         "1.4286829934220466",
         "0.7143414967110233",
         "1.3022811429668983",
         "0.6511405714834492",
         "TRBV11-2",
         "TRBJ1-2",
         "22",
         "26",
         "TRBV12-3",
         "TRBJ1-2",
         "10",
         "8",
         "2",
         "0.0",
         "0.0898752751283932",
         "6.064441326287675e-07",
         "4",
         "6",
         "1",
         "1"
        ],
        [
         "41",
         "CASSLTDNQPQHF",
         "2.7110982944791004",
         "0.6777745736197751",
         "2.5158875832385936",
         "0.6289718958096484",
         "TRBV11-2",
         "TRBJ1-5",
         "24",
         "40",
         "TRBV11-2",
         "TRBJ1-5",
         "11",
         "12",
         "4",
         "0.0",
         "0.0875312611647016",
         "2.803303423051186e-05",
         "4",
         "4",
         "4",
         "4"
        ],
        [
         "42",
         "CASSLSGRSSYEQYF",
         "6.341601328796736",
         "0.792700166099592",
         "6.5001671737914055",
         "0.8125208967239257",
         "TRBV12-3",
         "TRBJ2-7",
         "18",
         "19",
         "TRBV12-4",
         "TRBJ2-7",
         "7",
         "7",
         "8",
         "0.1071535638787057",
         "0.1259640102827763",
         "0.0053614558786946",
         "6",
         "7",
         "11",
         "11"
        ],
        [
         "43",
         "CASSPAGGQETQYF",
         "2.2630746084562445",
         "0.7543582028187482",
         "2.202881839389548",
         "0.7342939464631827",
         "TRBV12-3",
         "TRBJ2-5",
         "36",
         "36",
         "TRBV12-3",
         "TRBJ2-5",
         "8",
         "7",
         "3",
         "0.1296754767480762",
         "0.0858745180511742",
         "5.526969379888009e-07",
         "6",
         "6",
         "9",
         "9"
        ],
        [
         "44",
         "CASSLDRSYNEQFF",
         "3.3643417348118736",
         "0.6728683469623747",
         "3.3153595811431007",
         "0.6630719162286202",
         "TRBV11-2",
         "TRBJ2-1",
         "33",
         "33",
         "TRBV12-4",
         "TRBJ2-1",
         "5",
         "5",
         "5",
         "0.0492027290320028",
         "0.1071272409269785",
         "0.0001310294257852",
         "4",
         "7",
         "6",
         "6"
        ],
        [
         "45",
         "CASSLGRGYNEQFF",
         "5.5330730882439525",
         "0.6916341360304941",
         "5.812853036163373",
         "0.7266066295204217",
         "TRBV11-2",
         "TRBJ2-1",
         "35",
         "32",
         "TRBV12-3",
         "TRBJ2-1",
         "6",
         "5",
         "8",
         "0.101015740336836",
         "0.1345414607358594",
         "4.916481882506122e-05",
         "4",
         "6",
         "6",
         "6"
        ],
        [
         "46",
         "CASSLSGSGNTIYF",
         "2.381321931682265",
         "0.7937739772274216",
         "2.0830378642952594",
         "0.6943459547650864",
         "TRBV12-4",
         "TRBJ1-3",
         "49",
         "56",
         "TRBV12-3",
         "TRBJ1-3",
         "16",
         "13",
         "3",
         "0.1469029540223468",
         "0.1265495867768595",
         "0.000152911760169",
         "7",
         "6",
         "2",
         "2"
        ],
        [
         "47",
         "CASSLVGTEAFF",
         "6.195223772987552",
         "0.774402971623444",
         "6.481466969227411",
         "0.8101833711534264",
         "TRBV12-3",
         "TRBJ1-1",
         "63",
         "83",
         "TRBV11-2",
         "TRBJ1-1",
         "18",
         "19",
         "8",
         "0.1071535638787057",
         "0.1259640102827763",
         "0.0053614558786946",
         "6",
         "4",
         "0",
         "0"
        ],
        [
         "48",
         "CASSPGQTNQPQHF",
         "6.1995490755074965",
         "0.7749436344384371",
         "6.350549939663104",
         "0.793818742457888",
         "TRBV18",
         "TRBJ1-5",
         "17",
         "17",
         "TRBV12-3",
         "TRBJ1-5",
         "8",
         "6",
         "8",
         "0.1071535638787057",
         "0.1259640102827763",
         "0.0053614558786946",
         "11",
         "6",
         "4",
         "4"
        ],
        [
         "49",
         "CASSLGQVTDTQYF",
         "0.6334779698098364",
         "0.6334779698098364",
         "0.6184637840248173",
         "0.6184637840248173",
         "TRBV12-3",
         "TRBJ2-3",
         "28",
         "33",
         "TRBV5-1",
         "TRBJ2-3",
         "4",
         "5",
         "1",
         "0.0",
         "0.0912137006701414",
         "3.682291229591713e-09",
         "6",
         "22",
         "8",
         "8"
        ]
       ],
       "shape": {
        "columns": 21,
        "rows": 246
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sum_fast_weight</th>\n",
       "      <th>avg_fast_weight</th>\n",
       "      <th>sum_slow_weight</th>\n",
       "      <th>avg_slow_weight</th>\n",
       "      <th>V_gene_fast</th>\n",
       "      <th>J_gene_fast</th>\n",
       "      <th>codon_diversity_fast</th>\n",
       "      <th>sample_prevalence_fast</th>\n",
       "      <th>V_gene_slow</th>\n",
       "      <th>...</th>\n",
       "      <th>codon_diversity_slow</th>\n",
       "      <th>sample_prevalence_slow</th>\n",
       "      <th>degree</th>\n",
       "      <th>betweenness</th>\n",
       "      <th>closeness</th>\n",
       "      <th>eigenvector</th>\n",
       "      <th>V_gene_fast_id</th>\n",
       "      <th>V_gene_slow_id</th>\n",
       "      <th>J_gene_fast_id</th>\n",
       "      <th>J_gene_slow_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASSLLAGTYNEQFF</td>\n",
       "      <td>0.633450</td>\n",
       "      <td>0.633450</td>\n",
       "      <td>0.621356</td>\n",
       "      <td>0.621356</td>\n",
       "      <td>TRBV11-1</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "      <td>38</td>\n",
       "      <td>46</td>\n",
       "      <td>TRBV11-1</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111162</td>\n",
       "      <td>1.603855e-05</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CASSSVNQPQHF</td>\n",
       "      <td>1.370906</td>\n",
       "      <td>0.685453</td>\n",
       "      <td>1.590651</td>\n",
       "      <td>0.795326</td>\n",
       "      <td>TRBV6-2</td>\n",
       "      <td>TRBJ1-5</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>TRBV12-3</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.008163</td>\n",
       "      <td>0.110709</td>\n",
       "      <td>1.389661e-04</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CASSLGDRDTGELFF</td>\n",
       "      <td>9.108662</td>\n",
       "      <td>0.759055</td>\n",
       "      <td>9.535913</td>\n",
       "      <td>0.794659</td>\n",
       "      <td>TRBV12-3</td>\n",
       "      <td>TRBJ2-2</td>\n",
       "      <td>20</td>\n",
       "      <td>29</td>\n",
       "      <td>TRBV11-2</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>0.003569</td>\n",
       "      <td>0.095741</td>\n",
       "      <td>7.238639e-05</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CASSFGGNEKLFF</td>\n",
       "      <td>0.617424</td>\n",
       "      <td>0.617424</td>\n",
       "      <td>0.664843</td>\n",
       "      <td>0.664843</td>\n",
       "      <td>TRBV12-3</td>\n",
       "      <td>TRBJ1-4</td>\n",
       "      <td>20</td>\n",
       "      <td>24</td>\n",
       "      <td>TRBV12-3</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089842</td>\n",
       "      <td>4.963792e-07</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CASSLAGAEAFF</td>\n",
       "      <td>6.572671</td>\n",
       "      <td>0.730297</td>\n",
       "      <td>5.968546</td>\n",
       "      <td>0.663172</td>\n",
       "      <td>TRBV11-2</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "      <td>25</td>\n",
       "      <td>29</td>\n",
       "      <td>TRBV5-1</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.121807</td>\n",
       "      <td>0.124809</td>\n",
       "      <td>1.275793e-01</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>CASSYGSNQPQHF</td>\n",
       "      <td>0.673082</td>\n",
       "      <td>0.673082</td>\n",
       "      <td>0.695346</td>\n",
       "      <td>0.695346</td>\n",
       "      <td>TRBV12-3</td>\n",
       "      <td>TRBJ1-5</td>\n",
       "      <td>20</td>\n",
       "      <td>42</td>\n",
       "      <td>TRBV12-3</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089123</td>\n",
       "      <td>4.596769e-07</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>CASSPSDTQYF</td>\n",
       "      <td>3.974290</td>\n",
       "      <td>0.662382</td>\n",
       "      <td>5.055111</td>\n",
       "      <td>0.842518</td>\n",
       "      <td>TRBV18</td>\n",
       "      <td>TRBJ2-3</td>\n",
       "      <td>39</td>\n",
       "      <td>64</td>\n",
       "      <td>TRBV11-2</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>0.002750</td>\n",
       "      <td>0.106153</td>\n",
       "      <td>3.085686e-04</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>CASSPGLDYEQYF</td>\n",
       "      <td>3.207921</td>\n",
       "      <td>0.641584</td>\n",
       "      <td>3.726921</td>\n",
       "      <td>0.745384</td>\n",
       "      <td>TRBV11-2</td>\n",
       "      <td>TRBJ2-7</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>TRBV18</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.085812</td>\n",
       "      <td>0.108025</td>\n",
       "      <td>2.801964e-05</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>CASSLNQGNTEAFF</td>\n",
       "      <td>1.364046</td>\n",
       "      <td>0.682023</td>\n",
       "      <td>1.347463</td>\n",
       "      <td>0.673732</td>\n",
       "      <td>TRBV11-1</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>TRBV11-2</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073200</td>\n",
       "      <td>7.119434e-09</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>CASSLGRYEQYF</td>\n",
       "      <td>1.210300</td>\n",
       "      <td>0.605150</td>\n",
       "      <td>1.421355</td>\n",
       "      <td>0.710678</td>\n",
       "      <td>TRBV12-4</td>\n",
       "      <td>TRBJ2-7</td>\n",
       "      <td>79</td>\n",
       "      <td>80</td>\n",
       "      <td>TRBV12-3</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>0.008536</td>\n",
       "      <td>0.123426</td>\n",
       "      <td>4.657293e-06</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>246 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id  sum_fast_weight  avg_fast_weight  sum_slow_weight  \\\n",
       "0    CASSLLAGTYNEQFF         0.633450         0.633450         0.621356   \n",
       "1       CASSSVNQPQHF         1.370906         0.685453         1.590651   \n",
       "2    CASSLGDRDTGELFF         9.108662         0.759055         9.535913   \n",
       "3      CASSFGGNEKLFF         0.617424         0.617424         0.664843   \n",
       "4       CASSLAGAEAFF         6.572671         0.730297         5.968546   \n",
       "..               ...              ...              ...              ...   \n",
       "241    CASSYGSNQPQHF         0.673082         0.673082         0.695346   \n",
       "242      CASSPSDTQYF         3.974290         0.662382         5.055111   \n",
       "243    CASSPGLDYEQYF         3.207921         0.641584         3.726921   \n",
       "244   CASSLNQGNTEAFF         1.364046         0.682023         1.347463   \n",
       "245     CASSLGRYEQYF         1.210300         0.605150         1.421355   \n",
       "\n",
       "     avg_slow_weight V_gene_fast J_gene_fast  codon_diversity_fast  \\\n",
       "0           0.621356    TRBV11-1     TRBJ2-1                    38   \n",
       "1           0.795326     TRBV6-2     TRBJ1-5                     9   \n",
       "2           0.794659    TRBV12-3     TRBJ2-2                    20   \n",
       "3           0.664843    TRBV12-3     TRBJ1-4                    20   \n",
       "4           0.663172    TRBV11-2     TRBJ1-1                    25   \n",
       "..               ...         ...         ...                   ...   \n",
       "241         0.695346    TRBV12-3     TRBJ1-5                    20   \n",
       "242         0.842518      TRBV18     TRBJ2-3                    39   \n",
       "243         0.745384    TRBV11-2     TRBJ2-7                    27   \n",
       "244         0.673732    TRBV11-1     TRBJ1-1                    10   \n",
       "245         0.710678    TRBV12-4     TRBJ2-7                    79   \n",
       "\n",
       "     sample_prevalence_fast V_gene_slow  ... codon_diversity_slow  \\\n",
       "0                        46    TRBV11-1  ...                   11   \n",
       "1                        15    TRBV12-3  ...                    5   \n",
       "2                        29    TRBV11-2  ...                    6   \n",
       "3                        24    TRBV12-3  ...                    6   \n",
       "4                        29     TRBV5-1  ...                    5   \n",
       "..                      ...         ...  ...                  ...   \n",
       "241                      42    TRBV12-3  ...                    8   \n",
       "242                      64    TRBV11-2  ...                   14   \n",
       "243                      28      TRBV18  ...                    4   \n",
       "244                      16    TRBV11-2  ...                    5   \n",
       "245                      80    TRBV12-3  ...                   24   \n",
       "\n",
       "     sample_prevalence_slow  degree  betweenness  closeness   eigenvector  \\\n",
       "0                        11       1     0.000000   0.111162  1.603855e-05   \n",
       "1                         5       2     0.008163   0.110709  1.389661e-04   \n",
       "2                         7      12     0.003569   0.095741  7.238639e-05   \n",
       "3                         6       1     0.000000   0.089842  4.963792e-07   \n",
       "4                         5       9     0.121807   0.124809  1.275793e-01   \n",
       "..                      ...     ...          ...        ...           ...   \n",
       "241                       7       1     0.000000   0.089123  4.596769e-07   \n",
       "242                      16       6     0.002750   0.106153  3.085686e-04   \n",
       "243                       5       5     0.085812   0.108025  2.801964e-05   \n",
       "244                       5       2     0.000000   0.073200  7.119434e-09   \n",
       "245                      17       2     0.008536   0.123426  4.657293e-06   \n",
       "\n",
       "     V_gene_fast_id  V_gene_slow_id  J_gene_fast_id  J_gene_slow_id  \n",
       "0                 3               3               6               6  \n",
       "1                25               6               4               4  \n",
       "2                 6               4               7               7  \n",
       "3                 6               6               3               3  \n",
       "4                 4              22               0               0  \n",
       "..              ...             ...             ...             ...  \n",
       "241               6               6               4               4  \n",
       "242              11               4               8               8  \n",
       "243               4              11              11              11  \n",
       "244               3               4               0               0  \n",
       "245               7               6              11              11  \n",
       "\n",
       "[246 rows x 21 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c532cc7",
   "metadata": {},
   "source": [
    "The global correlation graph was built from all 210 repertoires, but supervised training and evaluation used only the 159?160? samples with clinical phenotype labels.  Unlabeled repertoires contributed to graph topology and node statistics but were not included in the loss function or reported metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e5af622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "filename",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sum value",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "T",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "N",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "M",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "sample_id",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "5d11f325-a4ff-4abc-b7c8-1c4bb328afd3",
       "rows": [
        [
         "0",
         "pool1_S1_TRB_mig_cdr3_clones_all",
         "33296",
         "T3",
         "1b",
         "Mx",
         "P1-S1"
        ],
        [
         "2",
         "pool1_S11_TRB_mig_cdr3_clones_all",
         "78560",
         "T2",
         "0",
         "Mx",
         "P1-S11"
        ],
        [
         "3",
         "pool1_S12_TRB_mig_cdr3_clones_all",
         "190517",
         "T3",
         "0",
         "Mx",
         "P1-S12"
        ],
        [
         "4",
         "pool1_S13_TRB_mig_cdr3_clones_all",
         "72593",
         "T3",
         "0",
         "0",
         "P1-S13"
        ],
        [
         "7",
         "pool1_S16_TRB_mig_cdr3_clones_all",
         "270087",
         "T4",
         "1",
         "0",
         "P1-S16"
        ],
        [
         "9",
         "pool1_S18_TRB_mig_cdr3_clones_all",
         "68608",
         "T3",
         "2",
         "0",
         "P1-S18"
        ],
        [
         "10",
         "pool1_S19_TRB_mig_cdr3_clones_all",
         "228531",
         "T3",
         "0",
         "Mx",
         "P1-S19"
        ],
        [
         "11",
         "pool1_S2_TRB_mig_cdr3_clones_all",
         "182461",
         "T3",
         "2",
         "Mx",
         "P1-S2"
        ],
        [
         "13",
         "pool1_S21_TRB_mig_cdr3_clones_all",
         "534445",
         "T2",
         "0",
         "0",
         "P1-S21"
        ],
        [
         "14",
         "pool1_S22_TRB_mig_cdr3_clones_all",
         "215319",
         "T2",
         "1",
         "0",
         "P1-S22"
        ],
        [
         "15",
         "pool1_S23_TRB_mig_cdr3_clones_all",
         "281314",
         "T3",
         "1",
         null,
         "P1-S23"
        ],
        [
         "16",
         "pool1_S24_TRB_mig_cdr3_clones_all",
         "540687",
         "T1",
         "1",
         "M0",
         "P1-S24"
        ],
        [
         "17",
         "pool1_S3_TRB_mig_cdr3_clones_all",
         "167545",
         "T2",
         "0",
         "Mx",
         "P1-S3"
        ],
        [
         "19",
         "pool1_S5_TRB_mig_cdr3_clones_all",
         "231509",
         "T3",
         "0",
         "Mx",
         "P1-S5"
        ],
        [
         "20",
         "pool1_S6_TRB_mig_cdr3_clones_all",
         "15164",
         "T3",
         "0",
         "M0",
         "P1-S6"
        ],
        [
         "21",
         "pool1_S7_TRB_mig_cdr3_clones_all",
         "200085",
         "T3",
         "2",
         "Mx",
         "P1-S7"
        ],
        [
         "22",
         "pool1_S8_TRB_mig_cdr3_clones_all",
         "240661",
         "T3",
         "0",
         "Mx",
         "P1-S8"
        ],
        [
         "23",
         "pool1_S9_TRB_mig_cdr3_clones_all",
         "307486",
         "T2",
         "0",
         "Mx",
         "P1-S9"
        ],
        [
         "25",
         "pool2_S10_TRB_mig_cdr3_clones_all",
         "29816",
         "T3",
         "0",
         "Mx",
         "P2-S10"
        ],
        [
         "28",
         "pool2_S13_TRB_mig_cdr3_clones_all",
         "148607",
         "T2",
         "0",
         "0",
         "P2-S13"
        ],
        [
         "30",
         "pool2_S15_TRB_mig_cdr3_clones_all",
         "255309",
         "T2",
         "1",
         null,
         "P2-S15"
        ],
        [
         "31",
         "pool2_S16_TRB_mig_cdr3_clones_all",
         "242373",
         "T3",
         "2",
         null,
         "P2-S16"
        ],
        [
         "33",
         "pool2_S18_TRB_mig_cdr3_clones_all",
         "19711",
         "T3",
         "0",
         null,
         "P2-S18"
        ],
        [
         "35",
         "pool2_S2_TRB_mig_cdr3_clones_all",
         "280184",
         "T3",
         "0",
         null,
         "P2-S2"
        ],
        [
         "36",
         "pool2_S20_TRB_mig_cdr3_clones_all",
         "174031",
         "T2",
         "0",
         "0",
         "P2-S20"
        ],
        [
         "38",
         "pool2_S22_TRB_mig_cdr3_clones_all",
         "310220",
         "T2",
         "0",
         null,
         "P2-S22"
        ],
        [
         "39",
         "pool2_S23_TRB_mig_cdr3_clones_all",
         "62110",
         "T3",
         "1",
         "Mx",
         "P2-S23"
        ],
        [
         "40",
         "pool2_S24_TRB_mig_cdr3_clones_all",
         "235024",
         "T3",
         "0",
         "0",
         "P2-S24"
        ],
        [
         "42",
         "pool2_S4_TRB_mig_cdr3_clones_all",
         "145584",
         "T3",
         "2",
         "0",
         "P2-S4"
        ],
        [
         "43",
         "pool2_S5_TRB_mig_cdr3_clones_all",
         "210578",
         "T3",
         "0",
         "Mx",
         "P2-S5"
        ],
        [
         "45",
         "pool2_S7_TRB_mig_cdr3_clones_all",
         "188052",
         "T3",
         "0",
         "Mx",
         "P2-S7"
        ],
        [
         "48",
         "pool3_S1_TRB_mig_cdr3_clones_all",
         "26081",
         "T2",
         "0",
         "Mx",
         "P3-S1"
        ],
        [
         "49",
         "pool3_S10_TRB_mig_cdr3_clones_all",
         "281105",
         "T4b",
         "0",
         null,
         "P3-S10"
        ],
        [
         "51",
         "pool3_S12_TRB_mig_cdr3_clones_all",
         "236346",
         "T2",
         "2",
         null,
         "P3-S12"
        ],
        [
         "52",
         "pool3_S13_TRB_mig_cdr3_clones_all",
         "191990",
         "T3",
         "1",
         "0",
         "P3-S13"
        ],
        [
         "53",
         "pool3_S14_TRB_mig_cdr3_clones_all",
         "136202",
         "T3",
         "0",
         "Mx",
         "P3-S14"
        ],
        [
         "54",
         "pool3_S15_TRB_mig_cdr3_clones_all",
         "66067",
         "T2",
         "0",
         "Mx",
         "P3-S15"
        ],
        [
         "56",
         "pool3_S17_TRB_mig_cdr3_clones_all",
         "208554",
         "T1",
         "0",
         null,
         "P3-S17"
        ],
        [
         "57",
         "pool3_S18_TRB_mig_cdr3_clones_all",
         "163267",
         "T2",
         "0",
         "Mx",
         "P3-S18"
        ],
        [
         "58",
         "pool3_S19_TRB_mig_cdr3_clones_all",
         "20493",
         "T3",
         "0",
         "0",
         "P3-S19"
        ],
        [
         "60",
         "pool3_S20_TRB_mig_cdr3_clones_all",
         "184820",
         "T3",
         "0",
         null,
         "P3-S20"
        ],
        [
         "61",
         "pool3_S21_TRB_mig_cdr3_clones_all",
         "57299",
         "T2",
         "0",
         "0",
         "P3-S21"
        ],
        [
         "62",
         "pool3_S22_TRB_mig_cdr3_clones_all",
         "8738",
         "T3",
         "0",
         null,
         "P3-S22"
        ],
        [
         "63",
         "pool3_S23_TRB_mig_cdr3_clones_all",
         "28454",
         "T3",
         "0",
         "Mx",
         "P3-S23"
        ],
        [
         "64",
         "pool3_S24_TRB_mig_cdr3_clones_all",
         "208675",
         "T2",
         "0",
         "0",
         "P3-S24"
        ],
        [
         "65",
         "pool3_S3_TRB_mig_cdr3_clones_all",
         "107900",
         "T1",
         "0",
         null,
         "P3-S3"
        ],
        [
         "66",
         "pool3_S4_TRB_mig_cdr3_clones_all",
         "177380",
         "T3",
         "0",
         null,
         "P3-S4"
        ],
        [
         "67",
         "pool3_S5_TRB_mig_cdr3_clones_all",
         "210406",
         "T2",
         "0",
         "0",
         "P3-S5"
        ],
        [
         "68",
         "pool3_S6_TRB_mig_cdr3_clones_all",
         "262784",
         "T3",
         "0",
         "M0",
         "P3-S6"
        ],
        [
         "69",
         "pool3_S7_TRB_mig_cdr3_clones_all",
         "250849",
         "T3",
         "0",
         "0",
         "P3-S7"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 160
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>sum value</th>\n",
       "      <th>T</th>\n",
       "      <th>N</th>\n",
       "      <th>M</th>\n",
       "      <th>sample_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pool1_S1_TRB_mig_cdr3_clones_all</td>\n",
       "      <td>33296</td>\n",
       "      <td>T3</td>\n",
       "      <td>1b</td>\n",
       "      <td>Mx</td>\n",
       "      <td>P1-S1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pool1_S11_TRB_mig_cdr3_clones_all</td>\n",
       "      <td>78560</td>\n",
       "      <td>T2</td>\n",
       "      <td>0</td>\n",
       "      <td>Mx</td>\n",
       "      <td>P1-S11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pool1_S12_TRB_mig_cdr3_clones_all</td>\n",
       "      <td>190517</td>\n",
       "      <td>T3</td>\n",
       "      <td>0</td>\n",
       "      <td>Mx</td>\n",
       "      <td>P1-S12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pool1_S13_TRB_mig_cdr3_clones_all</td>\n",
       "      <td>72593</td>\n",
       "      <td>T3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>P1-S13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pool1_S16_TRB_mig_cdr3_clones_all</td>\n",
       "      <td>270087</td>\n",
       "      <td>T4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>P1-S16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>pool9_S3_TRB_mig_cdr3_clones_all</td>\n",
       "      <td>190092</td>\n",
       "      <td>T1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>P9-S3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>pool9_S4_TRB_mig_cdr3_clones_all</td>\n",
       "      <td>198511</td>\n",
       "      <td>T2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>P9-S4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>pool9_S5_TRB_mig_cdr3_clones_all</td>\n",
       "      <td>62094</td>\n",
       "      <td>T3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>P9-S5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>pool9_S7_TRB_mig_cdr3_clones_all</td>\n",
       "      <td>170894</td>\n",
       "      <td>T2</td>\n",
       "      <td>0</td>\n",
       "      <td>X</td>\n",
       "      <td>P9-S7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>pool9_S8_TRB_mig_cdr3_clones_all</td>\n",
       "      <td>184919</td>\n",
       "      <td>T3</td>\n",
       "      <td>1</td>\n",
       "      <td>Mx</td>\n",
       "      <td>P9-S8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              filename  sum value   T   N   M sample_id\n",
       "0     pool1_S1_TRB_mig_cdr3_clones_all      33296  T3  1b  Mx     P1-S1\n",
       "2    pool1_S11_TRB_mig_cdr3_clones_all      78560  T2   0  Mx    P1-S11\n",
       "3    pool1_S12_TRB_mig_cdr3_clones_all     190517  T3   0  Mx    P1-S12\n",
       "4    pool1_S13_TRB_mig_cdr3_clones_all      72593  T3   0   0    P1-S13\n",
       "7    pool1_S16_TRB_mig_cdr3_clones_all     270087  T4   1   0    P1-S16\n",
       "..                                 ...        ...  ..  ..  ..       ...\n",
       "209   pool9_S3_TRB_mig_cdr3_clones_all     190092  T1   0   0     P9-S3\n",
       "210   pool9_S4_TRB_mig_cdr3_clones_all     198511  T2   1   0     P9-S4\n",
       "211   pool9_S5_TRB_mig_cdr3_clones_all      62094  T3   1   0     P9-S5\n",
       "213   pool9_S7_TRB_mig_cdr3_clones_all     170894  T2   0   X     P9-S7\n",
       "214   pool9_S8_TRB_mig_cdr3_clones_all     184919  T3   1  Mx     P9-S8\n",
       "\n",
       "[160 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_values = ['0', '1', '1a', '1b', '2']\n",
    "# List of samples to exclude - did not undergo mixcr due to missing lanes.\n",
    "exclude_samples = ['P2-S8', 'P2-S9', 'P2-S19', 'P8-S18'] # 'P4-S1'\n",
    "colon_meta = colon_meta[colon_meta['N'].isin(relevant_values)]\n",
    "colon_meta = colon_meta[~colon_meta['sample_id'].isin(exclude_samples)].copy()\n",
    "colon_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6abe878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n"
     ]
    }
   ],
   "source": [
    "ih_df = pd.read_csv(\"/home/dsi/orrbavly/GNN_project/outputs/colon_hardness_rf_cos_every5_new_mixcr.csv\")\n",
    "ih_df['sample_id'] = ih_df['sample_name'].str.extract(r'^(P\\d+-S\\d+)')\n",
    "ih_df\n",
    "print(ih_df['sample_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd141031",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixcr_dir = \"/dsi/efroni-lab/sbm/OrrBavly/colon_data/new_mixcr/TRB/\"\n",
    "meta_file = \"/home/dsi/orrbavly/GNN_project/data/colon_meta_time.csv\"\n",
    "\n",
    "time_threshold = 750\n",
    "meta_df = pd.read_csv(meta_file)\n",
    "meta_df = meta_df[[\"Sample_ID\", \"extraction_time\"]]\n",
    "\n",
    "# Columns to keep from MiXCR files\n",
    "mixcr_cols = [\n",
    "    \"aaSeqCDR3\", \"nSeqCDR3\", \"readCount\", 'readFraction',\n",
    "    \"allVHitsWithScore\", \"allDHitsWithScore\", \"allJHitsWithScore\"\n",
    "]\n",
    "clonotype_dfs = []\n",
    "\n",
    "for fname in os.listdir(mixcr_dir):\n",
    "    if fname.endswith(\".tsv\") and os.path.isfile(os.path.join(mixcr_dir, fname)):\n",
    "        sample_prefix = fname.split(\"_\")[0]\n",
    "        file_path = os.path.join(mixcr_dir, fname)\n",
    "        if sample_prefix in valid_samples:\n",
    "            df = pd.read_csv(file_path, sep=\"\\t\", usecols=mixcr_cols)\n",
    "            df[\"Sample_ID\"] = sample_prefix\n",
    "            clonotype_dfs.append(df)\n",
    "\n",
    "clonotype_df = pd.concat(clonotype_dfs, ignore_index=True)\n",
    "\n",
    "# Merge with metadata to get extraction time\n",
    "clonotype_df = clonotype_df.merge(meta_df, on=\"Sample_ID\", how=\"inner\")\n",
    "# Create 'group' column: fast vs slow\n",
    "clonotype_df[\"group\"] = clonotype_df[\"extraction_time\"].apply(lambda x: \"fast\" if x <= time_threshold else \"slow\")\n",
    "\n",
    "# Remove samples with no metadata (e.g., extraction time or label)\n",
    "valid_meta_ids = set(colon_meta['sample_id'].unique())\n",
    "clonotype_df = clonotype_df[clonotype_df['Sample_ID'].isin(valid_meta_ids)].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ccd45f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clonotype_df['Sample_ID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd989bee",
   "metadata": {},
   "source": [
    "Connect to GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d85b6d",
   "metadata": {},
   "source": [
    "# GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a7d929",
   "metadata": {},
   "source": [
    "## Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5910bed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "def connect_gpu():\n",
    "\n",
    "    GPU_DEVICE = 0\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if device.type == \"cpu\":\n",
    "        logging.warning(\"CUDA is not available, using CPU for training.\")\n",
    "    else:\n",
    "        # Print available devices\n",
    "        num_devices = torch.cuda.device_count()\n",
    "        logging.info(f\"CUDA is available. Number of devices: {num_devices}\")\n",
    "        print(f\"CUDA is available. Number of devices: {num_devices}\")\n",
    "\n",
    "        # Try connecting to the specific device\n",
    "        try:\n",
    "            torch.cuda.set_device(GPU_DEVICE)  # SET GPU INDEX HERE:\n",
    "            current_device = torch.cuda.current_device()\n",
    "            device_name = torch.cuda.get_device_name(current_device)\n",
    "            logging.info(f\"Using GPU device {current_device}: {device_name}\")\n",
    "            print(f\"Using GPU device {current_device}: {device_name}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to connect to GPU: {e}\")\n",
    "            device = torch.device(\"cpu\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e67ba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import dgl\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class SingleGraphSampleDataset(Dataset):\n",
    "    \"\"\"\n",
    "    One item = (graph, dyn_features, presence_mask, label)\n",
    "    dyn_features  : FloatTensor [num_nodes, D_dyn]\n",
    "    presence_mask : BoolTensor  [num_nodes]  (True for nodes that appear in sample)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        g_overlap: dgl.DGLGraph,\n",
    "        node_static_df: pd.DataFrame,\n",
    "        dynamic_tensor_dict: dict,   # sample_id -> FloatTensor[num_nodes, D_dyn]\n",
    "        presence_mask_dict: dict,    # sample_id -> BoolTensor[num_nodes]\n",
    "        label_series: pd.Series,     # index=sample_id, value=label\n",
    "    ):\n",
    "        self.g = g_overlap\n",
    "\n",
    "        # # store static features inside the graph once\n",
    "        # self.g.ndata[\"static\"] = torch.tensor(\n",
    "        #     node_static_df.values, dtype=torch.float32\n",
    "        # )\n",
    "        \n",
    "        # 1. Define which columns are integer IDs for the embedding layers.\n",
    "        id_cols = [\n",
    "            \"V_gene_fast_id\", \"J_gene_fast_id\",\n",
    "            \"V_gene_slow_id\", \"J_gene_slow_id\"\n",
    "        ]\n",
    "        \n",
    "        # 2. Define any other string-based columns that are not features.\n",
    "        string_cols = [\"id\", \"V_gene_fast\", \"J_gene_fast\", \"V_gene_slow\", \"J_gene_slow\"]\n",
    "        \n",
    "        # 3. Create the list of all columns to exclude from the main numeric tensor.\n",
    "        cols_to_exclude = id_cols + string_cols\n",
    "        \n",
    "        # 4. Get the final list of purely numeric feature columns.\n",
    "        numeric_feature_cols = [c for c in node_static_df.columns if c not in cols_to_exclude]\n",
    "        # 5. Store the NUMERICAL static features as torch.float32.\n",
    "        self.g.ndata[\"static\"] = torch.tensor(\n",
    "            node_static_df[numeric_feature_cols].values, dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        # 6. Store the pre-computed CATEGORICAL ID features as torch.long.\n",
    "        for col_name in id_cols:\n",
    "            self.g.ndata[col_name] = torch.tensor(\n",
    "                node_static_df[col_name].values, dtype=torch.long\n",
    "            )\n",
    "\n",
    "        # -------------------------\n",
    "        self.samples = list(label_series.index)\n",
    "        # Convert to string (in case some are int/float), then map '0' -> 0, else 1\n",
    "        labels = [0 if str(v) == '0' else 1 for v in label_series.values]\n",
    "        self.y = torch.tensor(labels, dtype=torch.long)\n",
    "        self.dyn = dynamic_tensor_dict\n",
    "        self.mask = presence_mask_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sid = self.samples[idx]\n",
    "        return (\n",
    "            self.g,                      # shared pointer, no copy\n",
    "            self.dyn[sid],               # [N,D_dyn]\n",
    "            self.mask[sid],              # [N]\n",
    "            self.y[idx]                  # scalar\n",
    "        )\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Batch size = number of samples in batch (NOT graph batching).\"\"\"\n",
    "    g_list, dyn_list, mask_list, label_list = zip(*batch)\n",
    "    # All g_list items are the same pointer; keep one\n",
    "    g = g_list[0]\n",
    "    dyn = torch.stack(dyn_list, dim=0)      # [B,N,D_dyn]\n",
    "    mask = torch.stack(mask_list, dim=0)    # [B,N]\n",
    "    labels = torch.tensor(label_list)       # [B]\n",
    "    return g, dyn, mask, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "666e311e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import dgl.nn as dglnn\n",
    "\n",
    "# older version - before removing _id columns from node_meta, and scaling them insided maskedGIN.\n",
    "# class MaskedGIN(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         static_dim: int,\n",
    "#         dyn_dim: int,\n",
    "#         hidden_dim: int = 128,\n",
    "#         num_layers: int = 3,\n",
    "#         num_classes: int = 2,\n",
    "#         dropout: float = 0.3,\n",
    "#         readout: str = \"mean\"   # \"mean\" (default), \"att\", or \"s2s\"\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         in_dim = static_dim + dyn_dim\n",
    "\n",
    "#         self.convs = nn.ModuleList()\n",
    "#         mlp = nn.Sequential(nn.Linear(in_dim, hidden_dim),\n",
    "#                             nn.ReLU(),\n",
    "#                             nn.Linear(hidden_dim, hidden_dim))\n",
    "#         self.convs.append(dglnn.GINConv(mlp, \"sum\"))\n",
    "\n",
    "#         for _ in range(num_layers - 1):\n",
    "#             mlp = nn.Sequential(nn.Linear(hidden_dim, hidden_dim),\n",
    "#                                 nn.ReLU(),\n",
    "#                                 nn.Linear(hidden_dim, hidden_dim))\n",
    "#             self.convs.append(dglnn.GINConv(mlp, \"sum\"))\n",
    "\n",
    "#         # Readout layer selection\n",
    "#         if readout == \"att\":\n",
    "#             self.pool = dglnn.GlobalAttentionPooling(nn.Linear(hidden_dim, 1)) # type: ignore\n",
    "#             self.readout_dim = hidden_dim\n",
    "#             self.readout_type = \"att\"\n",
    "#         elif readout == \"s2s\":\n",
    "#             self.pool = dglnn.Set2Set(hidden_dim, n_iters=3, n_layers=1) # type: ignore\n",
    "#             self.readout_dim = 2 * hidden_dim\n",
    "#             self.readout_type = \"s2s\"\n",
    "#         else: # mean\n",
    "#             self.pool = None\n",
    "#             self.readout_dim = hidden_dim\n",
    "#             self.readout_type = \"mean\"\n",
    "#         self.readout = nn.Linear(self.readout_dim, num_classes) # output layer - by defualt, a perceptron; linear transformation.\n",
    "#         # self.readout = nn.Sequential(nn.Linear(self.readout_dim, 64), nn.ReLU(), nn.Linear(64, num_classes))\n",
    "\n",
    "#     def forward(self, g, dyn_feat, presence_mask):\n",
    "#         static = g.ndata[\"static\"]                    # [N, static_dim]\n",
    "#         B, N, D_dyn = dyn_feat.shape                  # Batch size, num_nodes, dyn_dim\n",
    "#         static_expand = static.unsqueeze(0).expand(B, -1, -1)\n",
    "#         h = torch.cat([static_expand, dyn_feat], dim=-1)\n",
    "#         mask_expand = presence_mask.unsqueeze(-1).float()\n",
    "#         h = h * mask_expand\n",
    "#         h = h.view(-1, h.shape[-1])\n",
    "\n",
    "#         batched_g = dgl.batch([g] * B).to(h.device)\n",
    "#         batched_g.ndata[\"h\"] = h\n",
    "\n",
    "#         x = batched_g.ndata[\"h\"]\n",
    "\n",
    "#         for conv in self.convs:\n",
    "#             x = conv(batched_g, x)\n",
    "#             x = torch.relu(x)\n",
    "#             x = self.dropout(x)\n",
    "#         batched_g.ndata[\"x\"] = x\n",
    "\n",
    "#         # Unbatch to get per-graph node features\n",
    "#         x = x.view(B, N, -1)\n",
    "#         mask = presence_mask.unsqueeze(-1)        # [B, N, 1]\n",
    "#         x_masked = x * mask.float()\n",
    "\n",
    "#         if self.readout_type == \"att\" or self.readout_type == \"s2s\":\n",
    "#             graph_emb = self.pool(batched_g, batched_g.ndata[\"x\"])\n",
    "#         else: # mean pooling (masked)\n",
    "#             denom = mask.sum(dim=1).clamp(min=1)  # Avoid div by zero\n",
    "#             graph_emb = x_masked.sum(dim=1) / denom\n",
    "\n",
    "#         return self.readout(graph_emb)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "\n",
    "# New model - uses embedding to encode genes data, instead of simple categorial data.\n",
    "class MaskedGIN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        static_dim: int,                # numeric part only (IDs removed)\n",
    "        dyn_dim: int,\n",
    "        hidden_dim: int = 128,\n",
    "        num_layers: int = 3,\n",
    "        num_classes: int = 2,\n",
    "        dropout: float = 0.3,\n",
    "        readout: str = \"mean\",              # \"mean\", \"att\", or \"s2s\"\n",
    "        v_vocab=29, j_vocab=12, v_slow_vocab=30, j_slow_vocab=12    # based on max values +1 in node_meta _id columns.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        #  1.  Gene-ID embeddings (4  16 d) \n",
    "        emb_dim = 16\n",
    "        self.v_fast_emb = nn.Embedding(v_vocab, 16)\n",
    "        self.j_fast_emb = nn.Embedding(j_vocab, 16)\n",
    "        self.v_slow_emb = nn.Embedding(v_slow_vocab, 16)\n",
    "        self.j_slow_emb = nn.Embedding(j_slow_vocab, 16)\n",
    "\n",
    "        in_dim = static_dim + 4 * emb_dim + dyn_dim\n",
    "\n",
    "        #  2.  GIN layers \n",
    "        self.convs = nn.ModuleList()\n",
    "        mlp0 = nn.Sequential(nn.Linear(in_dim, hidden_dim), \n",
    "                             nn.BatchNorm1d(hidden_dim), \n",
    "                             nn.ReLU(), nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.convs.append(dglnn.GINConv(mlp0, \"sum\"))\n",
    "\n",
    "        for _ in range(num_layers - 1):\n",
    "            mlp = nn.Sequential(nn.Linear(hidden_dim, hidden_dim),\n",
    "                                nn.BatchNorm1d(hidden_dim),\n",
    "                                nn.ReLU(), nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.convs.append(dglnn.GINConv(mlp, \"sum\"))\n",
    "\n",
    "        #  3.  Read-out choice \n",
    "        if readout == \"att\":\n",
    "            self.pool = dglnn.GlobalAttentionPooling(nn.Linear(hidden_dim, 1))\n",
    "            self.readout_dim = hidden_dim\n",
    "        elif readout == \"s2s\":\n",
    "            self.pool = dglnn.Set2Set(hidden_dim, n_iters=3, n_layers=1)\n",
    "            self.readout_dim = 2 * hidden_dim\n",
    "        else:                       # masked mean\n",
    "            self.pool = None\n",
    "            self.readout_dim = hidden_dim\n",
    "\n",
    "        #  4.  2-layer MLP head \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.readout_dim, 2 * self.readout_dim),\n",
    "            nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(2 * self.readout_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # def forward(self, g, dyn_feat, presence_mask):\n",
    "    #     # numeric static part\n",
    "    #     static_num = g.ndata[\"static\"]                    # [N, static_num_dim]\n",
    "\n",
    "    #     # gene-ID look-ups\n",
    "    #     v_fast = self.v_fast_emb(g.ndata[\"V_gene_fast_id\"])\n",
    "    #     j_fast = self.j_fast_emb(g.ndata[\"J_gene_fast_id\"])\n",
    "    #     v_slow = self.v_slow_emb(g.ndata[\"V_gene_slow_id\"])\n",
    "    #     j_slow = self.j_slow_emb(g.ndata[\"J_gene_slow_id\"])\n",
    "\n",
    "    #     static_cat = torch.cat([static_num, v_fast, j_fast, v_slow, j_slow], -1)\n",
    "\n",
    "    #     B, N, _ = dyn_feat.shape\n",
    "    #     static_expand = static_cat.unsqueeze(0).expand(B, -1, -1)     # [B,N,F]\n",
    "    #     h = torch.cat([static_expand, dyn_feat], -1)                  # [B,N,F+]\n",
    "    #     h = h * presence_mask.unsqueeze(-1).float()                   # early mask\n",
    "    #     h = h.view(-1, h.shape[-1])                                   # [BN,F]\n",
    "\n",
    "    #     # g = g.int() # added for standerized 32-bit integer indcies, for hyper tune in new\n",
    "    #     bg = dgl.batch([g] * B).to(h.device)\n",
    "    #     x = h\n",
    "    #     for conv in self.convs:\n",
    "    #         x = self.dropout(F.relu(conv(bg, x)))\n",
    "\n",
    "    #     # reshape back, re-mask\n",
    "    #     x = x.view(B, N, -1) * presence_mask.unsqueeze(-1).float()\n",
    "\n",
    "    #     if self.pool is None:            # masked mean\n",
    "    #         denom = presence_mask.sum(1).clamp(min=1).unsqueeze(-1)\n",
    "    #         graph_emb = x.sum(1) / denom\n",
    "    #     else:\n",
    "    #         bg.ndata[\"x\"] = x.view(-1, x.shape[-1])\n",
    "    #         graph_emb = self.pool(bg, bg.ndata[\"x\"])\n",
    "\n",
    "    #     return self.head(graph_emb)\n",
    "    \n",
    "    def forward(self, g, dyn_feat, presence_mask):\n",
    "        # --- 1. Feature Preparation (Your existing code is correct) ---\n",
    "        # This section correctly creates your batched feature tensor 'h'.\n",
    "        static_num = g.ndata[\"static\"]\n",
    "        v_fast = self.v_fast_emb(g.ndata[\"V_gene_fast_id\"])\n",
    "        j_fast = self.j_fast_emb(g.ndata[\"J_gene_fast_id\"])\n",
    "        v_slow = self.v_slow_emb(g.ndata[\"V_gene_slow_id\"])\n",
    "        j_slow = self.j_slow_emb(g.ndata[\"J_gene_slow_id\"])\n",
    "        static_cat = torch.cat([static_num, v_fast, j_fast, v_slow, j_slow], -1)\n",
    "\n",
    "        B, N, _ = dyn_feat.shape\n",
    "        static_expand = static_cat.unsqueeze(0).expand(B, -1, -1)\n",
    "        h = torch.cat([static_expand, dyn_feat], -1)\n",
    "        h = h * presence_mask.unsqueeze(-1).float()\n",
    "        \n",
    "        # Flatten features into shape [B*N, F_total]\n",
    "        x = h.view(-1, h.shape[-1])\n",
    "\n",
    "        # --- 2. GIN Message Passing (The Definitive Fix) ---\n",
    "\n",
    "        # To avoid the underlying CUDA bug, we create a batched graph from a\n",
    "        # \"clean\" version of 'g' that has no node data attached yet. This is more stable.\n",
    "        clean_g = dgl.graph(g.edges(), num_nodes=g.number_of_nodes()).to(x.device)\n",
    "        \n",
    "        # Now create the batched graph. It will have B*N nodes.\n",
    "        bg = dgl.batch([clean_g] * B)\n",
    "        \n",
    "        # Assign our batched features 'x' to the new, correctly-sized batched graph.\n",
    "        # The number of nodes in `bg` now matches the number of rows in `x`.\n",
    "        bg.ndata['h'] = x\n",
    "\n",
    "        # The convolution layer now receives a consistent graph and its own features.\n",
    "        h_conv = bg.ndata['h']\n",
    "        for conv in self.convs:\n",
    "            h_conv = self.dropout(F.relu(conv(bg, h_conv)))\n",
    "\n",
    "        # --- 3. Readout and Pooling ---\n",
    "        \n",
    "        x_final = h_conv.view(B, N, -1) * presence_mask.unsqueeze(-1).float()\n",
    "\n",
    "        if self.pool is None:  # Masked mean pooling\n",
    "            denom = presence_mask.sum(1).clamp(min=1).unsqueeze(-1)\n",
    "            graph_emb = x_final.sum(1) / denom\n",
    "        else:\n",
    "            # The pooling layers can now use the final convolved features.\n",
    "            bg.ndata[\"x\"] = h_conv\n",
    "            graph_emb = self.pool(bg, bg.ndata[\"x\"])\n",
    "\n",
    "        return self.head(graph_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d1940bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "import copy\n",
    "\n",
    "class MaskedGAT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Node feature dimensions\n",
    "        static_dim: int,\n",
    "        dyn_dim: int,\n",
    "        \n",
    "        # Vocab sizes for embeddings\n",
    "        v_vocab: int, \n",
    "        j_vocab: int,\n",
    "        v_slow_vocab: int, \n",
    "        j_slow_vocab: int,\n",
    "        \n",
    "        # Model architecture hyperparameters\n",
    "        hidden_dim: int = 128,\n",
    "        num_layers: int = 3,\n",
    "        num_classes: int = 2,\n",
    "        dropout: float = 0.3,\n",
    "        readout: str = \"mean\",\n",
    "        \n",
    "        # GAT-specific hyperparameters\n",
    "        edge_feat_dim: int = 2,  # (fast_weight, slow_weight)\n",
    "        num_heads: int = 4       # Number of attention heads\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.num_heads = num_heads\n",
    "        emb_dim = 16  # The fixed embedding size from your GIN model\n",
    "\n",
    "        # --- 1. Gene-ID Embeddings (Same as your GIN model) ---\n",
    "        self.v_fast_emb = nn.Embedding(v_vocab, emb_dim)\n",
    "        self.j_fast_emb = nn.Embedding(j_vocab, emb_dim)\n",
    "        self.v_slow_emb = nn.Embedding(v_slow_vocab, emb_dim)\n",
    "        self.j_slow_emb = nn.Embedding(j_slow_vocab, emb_dim)\n",
    "\n",
    "        # Calculate the total input dimension for each node\n",
    "        in_dim = static_dim + (4 * emb_dim) + dyn_dim\n",
    "\n",
    "        # --- 2. EdgeGATConv Convolution Layers (The Correction) ---\n",
    "        self.convs = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        # Using the correct layer and argument names from the documentation\n",
    "        self.convs.append(dglnn.EdgeGATConv(\n",
    "            in_feats=in_dim,\n",
    "            edge_feats=edge_feat_dim, # <-- Correct argument name\n",
    "            out_feats=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            feat_drop=dropout,\n",
    "            attn_drop=dropout,\n",
    "            allow_zero_in_degree=True\n",
    "            # Note: 'residual' is not a param for this layer\n",
    "        ))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(dglnn.EdgeGATConv(\n",
    "                in_feats=hidden_dim * num_heads, # Input from all heads\n",
    "                edge_feats=edge_feat_dim,\n",
    "                out_feats=hidden_dim,\n",
    "                num_heads=num_heads,\n",
    "                feat_drop=dropout,\n",
    "                attn_drop=dropout,\n",
    "                allow_zero_in_degree=True\n",
    "            ))\n",
    "\n",
    "        # --- 3. Read-out choice ---\n",
    "        self.readout_dim = hidden_dim * num_heads\n",
    "        if readout == \"att\":\n",
    "            self.pool = dglnn.GlobalAttentionPooling(nn.Linear(self.readout_dim, 1))\n",
    "        elif readout == \"s2s\":\n",
    "            self.pool = dglnn.Set2Set(self.readout_dim, n_iters=3, n_layers=1)\n",
    "            self.readout_dim = self.readout_dim * 2\n",
    "        else:  # masked mean\n",
    "            self.pool = None\n",
    "\n",
    "        # --- 4. 2-layer MLP head ---\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.readout_dim, 2 * self.readout_dim),\n",
    "            nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(2 * self.readout_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, g, dyn_feat, presence_mask):\n",
    "            # --- 1. Node Feature Preparation (Same as before) ---\n",
    "            static_num = g.ndata[\"static\"]\n",
    "            v_fast = self.v_fast_emb(g.ndata[\"V_gene_fast_id\"])\n",
    "            j_fast = self.j_fast_emb(g.ndata[\"J_gene_fast_id\"])\n",
    "            v_slow = self.v_slow_emb(g.ndata[\"V_gene_slow_id\"])\n",
    "            j_slow = self.j_slow_emb(g.ndata[\"J_gene_slow_id\"])\n",
    "            static_cat = torch.cat([static_num, v_fast, j_fast, v_slow, j_slow], -1)\n",
    "\n",
    "            B, N, _ = dyn_feat.shape\n",
    "            static_expand = static_cat.unsqueeze(0).expand(B, -1, -1)\n",
    "            h = torch.cat([static_expand, dyn_feat], -1)\n",
    "            h = h * presence_mask.unsqueeze(-1).float()\n",
    "            x = h.view(-1, h.shape[-1])\n",
    "\n",
    "            # --- 2. Graph & Edge Feature Preparation (Same as before) ---\n",
    "            clean_g = dgl.graph(g.edges(), num_nodes=g.number_of_nodes()).to(x.device)\n",
    "            bg = dgl.batch([clean_g] * B)\n",
    "            bg.ndata['h'] = x\n",
    "            edge_features = g.edata['feat'].to(x.device)\n",
    "            bg.edata['feat'] = edge_features.repeat(B, 1)\n",
    "\n",
    "            # --- 3. GATv2 Message Passing (THE FIX) ---\n",
    "            h_conv = bg.ndata['h']\n",
    "            e_feat = bg.edata['feat']\n",
    "            \n",
    "            for conv in self.convs:\n",
    "                # GATv2Conv returns ONE tensor: the new node features.\n",
    "                # We no longer expect a 2-tuple.\n",
    "                h_conv = conv(bg, h_conv, e_feat) # <-- THE FIX IS HERE\n",
    "                \n",
    "                # The rest of the loop is the same\n",
    "                h_conv = h_conv.flatten(1) # Flatten heads\n",
    "                h_conv = self.dropout(F.relu(h_conv))\n",
    "\n",
    "            # --- 4. Readout and Pooling (Same as before) ---\n",
    "            x_final = h_conv.view(B, N, -1) * presence_mask.unsqueeze(-1).float()\n",
    "            if self.pool is None:\n",
    "                denom = presence_mask.sum(1).clamp(min=1).unsqueeze(-1)\n",
    "                graph_emb = x_final.sum(1) / denom\n",
    "            else:\n",
    "                bg.ndata[\"x\"] = h_conv\n",
    "                graph_emb = self.pool(bg, bg.ndata[\"x\"])\n",
    "\n",
    "            return self.head(graph_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "310d1bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scale_dyn_dict(train_sample_ids, all_sample_ids, orig_dyn_feat_dict):\n",
    "    \"\"\"Fit scaler on train samples, apply to all (returns new dict).\"\"\"\n",
    "    # Stack all train features: shape [N_train_samples * num_nodes, D_dyn]\n",
    "    train_dyn = torch.cat([orig_dyn_feat_dict[s] for s in train_sample_ids], dim=0).numpy()\n",
    "    scaler = StandardScaler().fit(train_dyn)\n",
    "\n",
    "    new_dyn = {}\n",
    "    for s in all_sample_ids:\n",
    "        arr = orig_dyn_feat_dict[s].numpy()\n",
    "        arr_scaled = scaler.transform(arr)\n",
    "        new_dyn[s] = torch.from_numpy(arr_scaled).float()\n",
    "    return new_dyn\n",
    "\n",
    "\n",
    "def build_orig_dyn_feat_dict(clonotype_df, node_idx, sample_ids, if_presence=False):\n",
    "    \"\"\"\n",
    "    Builds the [log(readFraction)] or [log_rf, presence] dynamic feature tensor for each sample.\n",
    "    Also creates a presence mask dict: torch.bool [num_nodes].\n",
    "    Log-transform ReadFraction values for normalization (as original values are right-skewed, most TCRs are rare)\n",
    "    and makes features more comparable. Also, changes in clonotype abundance often reflect fold-changes.\n",
    "    Returns: orig_dyn_feat_dict, presence_mask_dict\n",
    "    \"\"\"\n",
    "    orig_dyn_feat_dict = {}\n",
    "    presence_mask_dict = {}\n",
    "    num_nodes = len(node_idx)\n",
    "    for sid in sample_ids:\n",
    "        df_sample = clonotype_df[clonotype_df['Sample_ID'] == sid]\n",
    "        agg = df_sample.groupby('aaSeqCDR3').agg({'readFraction': 'sum'}).reset_index()\n",
    "        tcr_to_rf = dict(zip(agg['aaSeqCDR3'], agg['readFraction']))\n",
    "\n",
    "        log_rf   = np.zeros(num_nodes, dtype=np.float32)\n",
    "        presence = np.zeros(num_nodes, dtype=np.float32)\n",
    "        for tcr, idx in node_idx.items():\n",
    "            rf = tcr_to_rf.get(tcr, 0.0)\n",
    "            log_rf[idx] = np.log1p(rf)\n",
    "            presence[idx] = 1.0 if rf > 0 else 0.0\n",
    "\n",
    "        if if_presence:\n",
    "            # Stack into [num_nodes, 2]: [log_rf, presence]\n",
    "            dyn = np.stack([log_rf, presence], axis=1)\n",
    "        else:\n",
    "            # Only [log_rf], shape [num_nodes, 1]\n",
    "            dyn = log_rf[:, None]\n",
    "\n",
    "        orig_dyn_feat_dict[sid] = torch.from_numpy(dyn).float()\n",
    "        # Presence mask: torch.bool, True where presence > 0\n",
    "        presence_mask_dict[sid] = torch.from_numpy(presence > 0).bool()\n",
    "\n",
    "    return orig_dyn_feat_dict, presence_mask_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b567e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_one_epoch(model, loader, device, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for g, dyn, mask, y in loader:\n",
    "        dyn, mask, y = dyn.to(device), mask.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(g.to(device), dyn, mask) # calls forward function\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        # # Gradient clipping!\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * y.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    all_pred, all_true = [], []\n",
    "    for g,dyn,mask,y in loader:\n",
    "        probs = model(g.to(device), dyn.to(device), mask.to(device))\n",
    "        preds = probs.argmax(dim=1).cpu()\n",
    "        all_pred.extend(preds)\n",
    "        all_true.extend(y)\n",
    "    bal_acc = balanced_accuracy_score(all_true, all_pred)\n",
    "    f1 = f1_score(all_true, all_pred, average=\"binary\")\n",
    "    return bal_acc, f1\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "def sample_id_subset(dataset, subset_ids):\n",
    "    # Get indices of these IDs in dataset.samples\n",
    "    id_to_idx = {sid: i for i, sid in enumerate(dataset.samples)}\n",
    "    subset_indices = [id_to_idx[sid] for sid in subset_ids]\n",
    "    return Subset(dataset, subset_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f045008",
   "metadata": {},
   "source": [
    "## General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f319391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def intersection_graph_with_weights(G1, G2, attr1='weight', attr2='weight', new_attr1='fast_weight', new_attr2='slow_weight'):\n",
    "    \"\"\"\n",
    "    Returns a new graph with only edges present in both G1 and G2.\n",
    "    Edge attributes from G1 and G2 are preserved as new_attr1 and new_attr2.\n",
    "    Only nodes with at least one shared edge will be included.\n",
    "    \"\"\"\n",
    "    G_inter = nx.Graph()\n",
    "    # Create sets of edges as (nodeA, nodeB) sorted tuples for undirected comparison\n",
    "    edges1 = set(tuple(sorted(e)) for e in G1.edges())\n",
    "    edges2 = set(tuple(sorted(e)) for e in G2.edges())\n",
    "    shared_edges = edges1 & edges2\n",
    "    \n",
    "    for u, v in shared_edges:\n",
    "        # Copy correlation weights from both graphs\n",
    "        w1 = G1[u][v][attr1]\n",
    "        w2 = G2[u][v][attr2]\n",
    "        G_inter.add_edge(u, v, **{new_attr1: w1, new_attr2: w2})\n",
    "    return G_inter\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "def largest_component_size(G):\n",
    "    if G.number_of_nodes() == 0:\n",
    "        return 0\n",
    "    return max(len(c) for c in nx.connected_components(G))\n",
    "\n",
    "def tune_overlap_graph(\n",
    "    fast_corr, slow_corr, \n",
    "    target_size=80, \n",
    "    init_min_r_fast=None, \n",
    "    init_min_r_slow=None,\n",
    "    step=0.01, \n",
    "    max_iter=30, \n",
    "    tol=5, \n",
    "    verbose=True\n",
    "):\n",
    "    # Estimate good starting thresholds if not given\n",
    "    if init_min_r_fast is None:\n",
    "        fast_vals = fast_corr.values[np.triu_indices_from(fast_corr, k=1)]\n",
    "        init_min_r_fast = np.percentile(fast_vals, 99)  # or adjust\n",
    "    if init_min_r_slow is None:\n",
    "        slow_vals = slow_corr.values[np.triu_indices_from(slow_corr, k=1)]\n",
    "        init_min_r_slow = np.percentile(slow_vals, 99)\n",
    "        \n",
    "    min_r_fast = init_min_r_fast\n",
    "    min_r_slow = init_min_r_slow\n",
    "    best_diff = float('inf')\n",
    "    best_result = None\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        # Build graphs\n",
    "        G_fast = build_pos_corr_graph_faster(fast_corr, min_r_fast)\n",
    "        G_slow = build_pos_corr_graph_faster(slow_corr, min_r_slow)\n",
    "        # Intersect\n",
    "        G_overlap = intersection_graph_with_weights(G_fast, G_slow)\n",
    "        # Largest component size\n",
    "        lcc_size = largest_component_size(G_overlap)\n",
    "        diff = abs(lcc_size - target_size)\n",
    "        if verbose:\n",
    "            print(f\"Iter {i}: min_r_fast={min_r_fast:.4f}, min_r_slow={min_r_slow:.4f}, \"\n",
    "                  f\"LCC size={lcc_size}, nodes={G_overlap.number_of_nodes()}, edges={G_overlap.number_of_edges()}\")\n",
    "        # Save best result so far\n",
    "        if diff < best_diff:\n",
    "            best_diff = diff\n",
    "            best_result = (min_r_fast, min_r_slow, G_fast, G_slow, G_overlap, lcc_size)\n",
    "        # Converged?\n",
    "        if diff <= tol:\n",
    "            break\n",
    "        # Adjust thresholds:\n",
    "        if lcc_size > target_size:\n",
    "            # Too big: increase thresholds\n",
    "            min_r_fast += step\n",
    "            min_r_slow += step\n",
    "        else:\n",
    "            # Too small: decrease thresholds\n",
    "            min_r_fast -= step\n",
    "            min_r_slow -= step\n",
    "    # Return best result\n",
    "    return {\n",
    "        \"min_r_fast\": best_result[0],\n",
    "        \"min_r_slow\": best_result[1],\n",
    "        \"G_fast\": best_result[2],\n",
    "        \"G_slow\": best_result[3],\n",
    "        \"G_overlap\": best_result[4],\n",
    "        \"lcc_size\": best_result[5]\n",
    "    }\n",
    "\n",
    "def build_pos_corr_graph_faster(corr_matrix, min_r=None):\n",
    "    \"\"\"\n",
    "    Efficiently build a NetworkX graph from a correlation matrix using a threshold.\n",
    "    Precompute the edge list directly from the correlation matrix (using NumPy or pandas).\n",
    "    Only adds edges where r >= min_r (positive correlation).\n",
    "    \"\"\"\n",
    "    tcrs = np.array(corr_matrix.index)\n",
    "    # Get upper triangle indices (excluding diagonal)\n",
    "    mask = np.triu(np.ones(corr_matrix.shape, dtype=bool), k=1)\n",
    "    # Get pairs and correlations\n",
    "    i_idx, j_idx = np.where(mask)\n",
    "    corrs = corr_matrix.values[mask]\n",
    "    if min_r is not None:\n",
    "        select = np.where(corrs >= min_r)[0]\n",
    "        i_idx = i_idx[select]\n",
    "        j_idx = j_idx[select]\n",
    "        corrs = corrs[select]\n",
    "    # Build edge list\n",
    "    edge_list = [\n",
    "        (tcrs[i], tcrs[j], {'weight': float(corr)})\n",
    "        for i, j, corr in zip(i_idx, j_idx, corrs)\n",
    "    ]\n",
    "    # Create graph\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(edge_list)\n",
    "    return G\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save function\n",
    "def save_graph(graph, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(graph, f)\n",
    "\n",
    "# Load function\n",
    "def load_graph(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96dcf384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def create_node_meta_df_for_gnn(G_overlap):\n",
    "    # This function creates static (predetermined) information about nodes (TCRs) that will be later used\n",
    "    # as initial node embedding input in GNN algorithm.\n",
    "    \n",
    "    def aggregate_edge_weights(G, weight_key):\n",
    "        \"\"\"Return a dict mapping node -> sum of edge weights for that key.\"\"\"\n",
    "        return {\n",
    "            node: sum(G[node][nbr].get(weight_key, 0) for nbr in G.neighbors(node))\n",
    "            for node in G.nodes\n",
    "        }\n",
    "\n",
    "    sum_fast = aggregate_edge_weights(G_overlap, 'fast_weight')\n",
    "    avg_fast = {k: v / G_overlap.degree[k] if G_overlap.degree[k] else 0 for k, v in sum_fast.items()}\n",
    "    sum_slow = aggregate_edge_weights(G_overlap, 'slow_weight')\n",
    "    avg_slow = {k: v / G_overlap.degree[k] if G_overlap.degree[k] else 0 for k, v in sum_slow.items()}\n",
    "\n",
    "    overlap_node_meta_df = pd.DataFrame({'id': list(G_overlap.nodes())})\n",
    "    overlap_node_meta_df['sum_fast_weight'] = overlap_node_meta_df['id'].map(sum_fast)\n",
    "    overlap_node_meta_df['avg_fast_weight'] = overlap_node_meta_df['id'].map(avg_fast)\n",
    "    overlap_node_meta_df['sum_slow_weight'] = overlap_node_meta_df['id'].map(sum_slow)\n",
    "    overlap_node_meta_df['avg_slow_weight'] = overlap_node_meta_df['id'].map(avg_slow)\n",
    "\n",
    "    # Define function to clean allele info from V/J genes\n",
    "    def clean_vj(gene_str):\n",
    "        return re.sub(r\"\\*.*\", \"\", gene_str) if isinstance(gene_str, str) else gene_str\n",
    "\n",
    "    # Generate per-group metadata from fast_df\n",
    "    vj_map_fast = (\n",
    "        fast_df.groupby(\"aaSeqCDR3\")\n",
    "        .agg({\n",
    "            \"allVHitsWithScore\": lambda x: x.mode()[0] if not x.mode().empty else \"\",\n",
    "            \"allJHitsWithScore\": lambda x: x.mode()[0] if not x.mode().empty else \"\",\n",
    "            \"nSeqCDR3\": pd.Series.nunique,\n",
    "            \"Sample_ID\": pd.Series.nunique\n",
    "        })\n",
    "        .rename(columns={\n",
    "            \"allVHitsWithScore\": \"V_gene_fast\",\n",
    "            \"allJHitsWithScore\": \"J_gene_fast\",\n",
    "            \"nSeqCDR3\": \"codon_diversity_fast\",\n",
    "            \"Sample_ID\": \"sample_prevalence_fast\"\n",
    "        })\n",
    "    )\n",
    "\n",
    "    # Generate per-group metadata from slow_df\n",
    "    vj_map_slow = (\n",
    "        slow_df.groupby(\"aaSeqCDR3\")\n",
    "        .agg({\n",
    "            \"allVHitsWithScore\": lambda x: x.mode()[0] if not x.mode().empty else \"\",\n",
    "            \"allJHitsWithScore\": lambda x: x.mode()[0] if not x.mode().empty else \"\",\n",
    "            \"nSeqCDR3\": pd.Series.nunique,\n",
    "            \"Sample_ID\": pd.Series.nunique\n",
    "        })\n",
    "        .rename(columns={\n",
    "            \"allVHitsWithScore\": \"V_gene_slow\",\n",
    "            \"allJHitsWithScore\": \"J_gene_slow\",\n",
    "            \"nSeqCDR3\": \"codon_diversity_slow\",\n",
    "            \"Sample_ID\": \"sample_prevalence_slow\"\n",
    "        })\n",
    "    )\n",
    "\n",
    "    # Clean allele info\n",
    "    vj_map_fast[\"V_gene_fast\"] = vj_map_fast[\"V_gene_fast\"].apply(clean_vj)\n",
    "    vj_map_fast[\"J_gene_fast\"] = vj_map_fast[\"J_gene_fast\"].apply(clean_vj)\n",
    "    vj_map_slow[\"V_gene_slow\"] = vj_map_slow[\"V_gene_slow\"].apply(clean_vj)\n",
    "    vj_map_slow[\"J_gene_slow\"] = vj_map_slow[\"J_gene_slow\"].apply(clean_vj)\n",
    "\n",
    "    # Merge both metadata sets into it\n",
    "    overlap_node_meta_df = (\n",
    "        overlap_node_meta_df\n",
    "        .merge(vj_map_fast, left_on='id', right_index=True, how='left')\n",
    "        .merge(vj_map_slow, left_on='id', right_index=True, how='left')\n",
    "    )\n",
    "\n",
    "    # graph level stats\n",
    "    degree_dict = dict(G_overlap.degree())\n",
    "    betweenness = nx.betweenness_centrality(G_overlap)\n",
    "    closeness = nx.closeness_centrality(G_overlap)\n",
    "    eigenvector = nx.eigenvector_centrality(G_overlap, max_iter=500)\n",
    "\n",
    "    # Convert metrics to Series with index as node IDs\n",
    "    degree_series = pd.Series(degree_dict, name='degree')\n",
    "    betweenness_series = pd.Series(betweenness, name='betweenness')\n",
    "    closeness_series = pd.Series(closeness, name='closeness')\n",
    "    eigenvector_series = pd.Series(eigenvector, name='eigenvector')\n",
    "\n",
    "    # Merge all into overlap_node_meta_df\n",
    "    overlap_node_meta_df = (\n",
    "        overlap_node_meta_df\n",
    "        .merge(degree_series, left_on='id', right_index=True, how='left')\n",
    "        .merge(betweenness_series, left_on='id', right_index=True, how='left')\n",
    "        .merge(closeness_series, left_on='id', right_index=True, how='left')\n",
    "        .merge(eigenvector_series, left_on='id', right_index=True, how='left')\n",
    "    )\n",
    "\n",
    "    # Step 1: Combine all V genes to create a unified vocabulary\n",
    "    all_v_genes = pd.concat([\n",
    "        overlap_node_meta_df['V_gene_fast'],\n",
    "        overlap_node_meta_df['V_gene_slow']\n",
    "    ]).dropna().unique()\n",
    "    # Step 2: Create mapping from V gene name to integer ID\n",
    "    unique_v_genes = sorted(set(all_v_genes))\n",
    "    v_gene_to_id = {v: i for i, v in enumerate(unique_v_genes)}\n",
    "    # Step 3: Map V_gene columns to integer IDs\n",
    "    overlap_node_meta_df['V_gene_fast_id'] = overlap_node_meta_df['V_gene_fast'].map(v_gene_to_id)\n",
    "    overlap_node_meta_df['V_gene_slow_id'] = overlap_node_meta_df['V_gene_slow'].map(v_gene_to_id)\n",
    "    # Step 1: Combine all J genes to create a unified vocabulary\n",
    "    all_j_genes = pd.concat([\n",
    "        overlap_node_meta_df['J_gene_fast'],\n",
    "        overlap_node_meta_df['J_gene_slow']\n",
    "    ]).dropna().unique()\n",
    "    unique_j_genes = sorted(set(all_j_genes))\n",
    "    j_gene_to_id = {j: i for i, j in enumerate(unique_j_genes)}\n",
    "    overlap_node_meta_df['J_gene_fast_id'] = overlap_node_meta_df['J_gene_fast'].map(j_gene_to_id)\n",
    "    overlap_node_meta_df['J_gene_slow_id'] = overlap_node_meta_df['J_gene_slow'].map(j_gene_to_id)\n",
    "\n",
    "    return overlap_node_meta_df\n",
    "\t\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "\n",
    "def add_tcr_biochem_features(df: pd.DataFrame, seq_col: str = \"id\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add biochemical features for AA sequences in `seq_col` to the same DataFrame.\n",
    "    Returns a copy with new columns:\n",
    "      ['length','aromaticity','instability_index','isoelectric_point','gravy',\n",
    "       'charge_7.0','molecular_weight','helix_frac','turn_frac','sheet_frac']\n",
    "    \"\"\"\n",
    "    if seq_col not in df.columns:\n",
    "        raise KeyError(f\"Column '{seq_col}' not found in df\")\n",
    "\n",
    "    # Define which columns well produce\n",
    "    biochem_cols = [\n",
    "        \"length\", \"aromaticity\", \"instability_index\", \"isoelectric_point\",\n",
    "        \"gravy\", \"charge_7.0\", \"molecular_weight\", \"helix_frac\", \"turn_frac\", \"sheet_frac\"\n",
    "    ]\n",
    "\n",
    "    # Helper for one sequence\n",
    "    def analyze_sequence(seq: str):\n",
    "        try:\n",
    "            if not isinstance(seq, str) or not seq:\n",
    "                raise ValueError(\"empty\")\n",
    "            s = seq.upper()\n",
    "            # Optionally reject non-standard residues to avoid ProtParam errors\n",
    "            if any(c not in \"ACDEFGHIKLMNPQRSTVWY\" for c in s):\n",
    "                raise ValueError(\"non-standard AA\")\n",
    "            prot = ProteinAnalysis(s)\n",
    "            ss_h, ss_t, ss_s = prot.secondary_structure_fraction()  # helix, turn, sheet\n",
    "            return {\n",
    "                \"length\": len(s),\n",
    "                \"aromaticity\": prot.aromaticity(),\n",
    "                \"instability_index\": prot.instability_index(),\n",
    "                \"isoelectric_point\": prot.isoelectric_point(),\n",
    "                \"gravy\": prot.gravy(),\n",
    "                \"charge_7.0\": prot.charge_at_pH(7.0),\n",
    "                \"molecular_weight\": prot.molecular_weight(),\n",
    "                \"helix_frac\": ss_h,\n",
    "                \"turn_frac\": ss_t,\n",
    "                \"sheet_frac\": ss_s,\n",
    "            }\n",
    "        except Exception:\n",
    "            # Fill NaNs on any issue; keeps shapes consistent\n",
    "            return {k: np.nan for k in biochem_cols}\n",
    "\n",
    "    # Compute once per unique sequence\n",
    "    uniq = df[seq_col].astype(str).unique()\n",
    "    feat_map = {seq: analyze_sequence(seq) for seq in uniq}\n",
    "\n",
    "    # Build features frame and merge back\n",
    "    feats_df = pd.DataFrame.from_dict(feat_map, orient=\"index\")\n",
    "    feats_df.index.name = seq_col\n",
    "    out = df.merge(feats_df, left_on=seq_col, right_index=True, how=\"left\")\n",
    "\n",
    "    return out\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21dae49",
   "metadata": {},
   "source": [
    "## Load Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31faf4e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'overlap_node_meta_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m overlap_node_meta_df \u001b[38;5;241m=\u001b[39m add_tcr_biochem_features(\u001b[43moverlap_node_meta_df\u001b[49m, seq_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'overlap_node_meta_df' is not defined"
     ]
    }
   ],
   "source": [
    "overlap_node_meta_df = add_tcr_biochem_features(overlap_node_meta_df, seq_col=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18edc42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "device = connect_gpu()\n",
    "\n",
    "# If G_overlap is a NetworkX graph:\n",
    "G_overlap_dgl = dgl.from_networkx(G_overlap)\n",
    "\n",
    "node_list = list(G_overlap.nodes)\n",
    "node_idx = {tcr: i for i, tcr in enumerate(node_list)}\n",
    "\n",
    "# Drop string columns (keep *_id)\n",
    "string_cols = [\n",
    "    \"id\", \"V_gene_fast\", \"J_gene_fast\", \"V_gene_slow\", \"J_gene_slow\"\n",
    "]\n",
    "node_meta_numeric = node_meta.drop(columns=string_cols)\n",
    "\n",
    "# Addition - to prevent _id columns from interfering scaling other columns\n",
    "# node_meta_numeric already has the four *_id columns as integers\n",
    "for col in [\"V_gene_fast_id\", \"J_gene_fast_id\",\n",
    "            \"V_gene_slow_id\", \"J_gene_slow_id\"]:\n",
    "    G_overlap_dgl.ndata[col] = torch.tensor(node_meta_numeric[col].values, dtype=torch.long)\n",
    "\n",
    "node_meta_numeric = node_meta_numeric.drop(columns=[\"V_gene_fast_id\",\"J_gene_fast_id\",\"V_gene_slow_id\",\"J_gene_slow_id\"])\n",
    "\n",
    "# List of columns to normalize\n",
    "cols_to_normalize = [\n",
    "    'sum_fast_weight', 'avg_fast_weight', 'sum_slow_weight',\n",
    "    'avg_slow_weight', 'codon_diversity_fast', 'sample_prevalence_fast',\n",
    "    'codon_diversity_slow', 'sample_prevalence_slow',\n",
    "    'degree', 'betweenness', 'closeness', 'eigenvector'\n",
    "]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "node_meta_numeric[cols_to_normalize] = scaler.fit_transform(node_meta_numeric[cols_to_normalize])\n",
    "\n",
    "# Check all remaining columns are numeric\n",
    "assert all([np.issubdtype(dtype, np.number) for dtype in node_meta_numeric.dtypes])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5d869c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "all_samples = colon_meta['sample_id'].tolist()\n",
    "# colon_meta = colon_meta.set_index('sample_id')\n",
    "labels = [0 if str(v) == \"0\" else 1 for v in colon_meta.set_index('sample_id').loc[all_samples, 'N']]\n",
    "# r_state=2\n",
    "\n",
    "trainval_ids, test_ids, trainval_labels, test_labels = train_test_split(\n",
    "    all_samples, labels, test_size=0.15, stratify=labels)\n",
    "\n",
    "train_ids, val_ids, train_labels, val_labels = train_test_split(\n",
    "    trainval_ids, trainval_labels, test_size=0.1, stratify=trainval_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec8f3694",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_dyn_feat_dict, presence_mask_dict = build_orig_dyn_feat_dict(\n",
    "    clonotype_df, node_idx, all_samples, if_presence=False)\n",
    "# You must provide both the train IDs (to fit), and all IDs (to transform)\n",
    "dyn_feat_dict_scaled = scale_dyn_dict(train_ids, all_samples, orig_dyn_feat_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e82a8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = SingleGraphSampleDataset(\n",
    "    G_overlap_dgl,\n",
    "    node_meta_numeric,\n",
    "    dyn_feat_dict_scaled,\n",
    "    presence_mask_dict,\n",
    "    colon_meta.set_index(\"sample_id\")[\"N\"]  # labels\n",
    ")\n",
    "\n",
    "train_set = sample_id_subset(dataset, train_ids)\n",
    "val_set = sample_id_subset(dataset, val_ids)\n",
    "test_set  = sample_id_subset(dataset, test_ids)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743466a8",
   "metadata": {},
   "source": [
    "## GIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd443002",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc6dee5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | Loss 0.7232 | Val bal-acc 0.489 | Val F1 0.250\n",
      "Epoch 05 | Loss 0.6936 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 10 | Loss 0.6917 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 15 | Loss 0.6942 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 20 | Loss 0.6936 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 25 | Loss 0.6938 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 30 | Loss 0.6936 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 35 | Loss 0.6935 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 40 | Loss 0.6951 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 45 | Loss 0.6937 | Val bal-acc 0.600 | Val F1 0.333\n",
      "Epoch 50 | Loss 0.6930 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Test baln-acc:0.5, test F1:0.0\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "dyn_dim = next(iter(dyn_feat_dict_scaled.values())).shape[-1]\n",
    "\n",
    "model = MaskedGIN(\n",
    "    static_dim=node_meta_numeric.shape[1],\n",
    "    dyn_dim=dyn_dim,          # len(dynamic feature columns we've added [if only readFraction, than 1])\n",
    "    hidden_dim=256,\n",
    "    num_layers=4,\n",
    "    num_classes=2,\n",
    "    dropout=0.4,\n",
    "    readout='att'\n",
    ").to(device)\n",
    "\n",
    "# loss function\n",
    "class_counts = np.bincount(labels)   # [101,  58]\n",
    "\n",
    "weights = 1.0 / torch.tensor(class_counts, dtype=torch.float32)\n",
    "criterion  = nn.CrossEntropyLoss(weight=weights.to(device))\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0013170, weight_decay=0.0001561646)\n",
    "\n",
    "best_ba = 0; best_f1 = 0\n",
    "patience = 50\n",
    "epochs_since_best = 0\n",
    "max_epochs = 150\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    train_loss = train_one_epoch(model, train_loader, device, criterion, optimizer)\n",
    "    ba, f1 = evaluate(model, val_loader, device)\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch:02d} | Loss {train_loss:.4f} | \"\n",
    "            f\"Val bal-acc {ba:.3f} | Val F1 {f1:.3f}\")\n",
    "    if ba > best_ba:\n",
    "        best_ba = ba; best_f1 = f1\n",
    "        epochs_since_best = 0\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "    else:\n",
    "        epochs_since_best += 1\n",
    "    if epochs_since_best >= patience:\n",
    "        break\n",
    "\n",
    "test_balacc, test_f1 = evaluate(model, test_loader, device)\n",
    "print(f\"Test baln-acc:{test_balacc}, test F1:{test_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ae1e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.4661) 1.0\n"
     ]
    }
   ],
   "source": [
    "dyn_var = dyn_feat_dict_scaled[train_ids[0]].std()\n",
    "stat_var = node_meta_numeric.values.std()\n",
    "print(dyn_var, stat_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc23616",
   "metadata": {},
   "source": [
    "### Hyper Parameter Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff97d0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "# ----- HYPERPARAMETER SEARCH SPACE -----\n",
    "np.random.seed(7)\n",
    "random.seed(7)\n",
    "\n",
    "HIDDEN_OPTIONS = [64, 128, 256, 384]\n",
    "LAYER_OPTIONS = [3, 4, 5]\n",
    "DROPOUT_RANGE = (0.1, 0.4)\n",
    "LR_RANGE = (1e-3, 5e-3)   # log scale\n",
    "WD_RANGE = (1e-5, 1e-3)   # log scale\n",
    "READOUT_OPTIONS = ['mean', 'att', 's2s']\n",
    "BATCH_SIZE = 16\n",
    "N_TRIALS = 50  # Can reduce for quick testing\n",
    "\n",
    "def sample_config():\n",
    "    cfg = {\n",
    "        'hidden': random.choice(HIDDEN_OPTIONS),\n",
    "        'layers': random.choice(LAYER_OPTIONS),\n",
    "        'dropout': round(random.uniform(*DROPOUT_RANGE), 3),\n",
    "        'lr': 10**random.uniform(np.log10(LR_RANGE[0]), np.log10(LR_RANGE[1])),\n",
    "        'wd': 10**random.uniform(np.log10(WD_RANGE[0]), np.log10(WD_RANGE[1])),\n",
    "        'readout': random.choice(READOUT_OPTIONS)\n",
    "        # Extend with other params if needed\n",
    "    }\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19963134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trial 1/50 ---\n",
      "Config: {'hidden': 64, 'layers': 4, 'dropout': 0.305, 'lr': np.float64(0.0023494261734691587), 'wd': np.float64(9.225162557357044e-05), 'readout': 'mean'}\n",
      "fold 0: val_bal_acc=0.567, val_F1=0.400\n",
      "fold 1: val_bal_acc=0.667, val_F1=0.625\n",
      "fold 2: val_bal_acc=0.600, val_F1=0.500\n",
      "fold 3: val_bal_acc=0.606, val_F1=0.500\n",
      "fold 4: val_bal_acc=0.677, val_F1=0.571\n",
      "TRIAL 1: mean bal-acc=0.6230.042, mean F1=0.5190.076\n",
      "\n",
      "--- Trial 2/50 ---\n",
      "Config: {'hidden': 64, 'layers': 3, 'dropout': 0.157, 'lr': np.float64(0.0010702759377890722), 'wd': np.float64(1.568521755033666e-05), 'readout': 'att'}\n",
      "fold 0: val_bal_acc=0.542, val_F1=0.154\n",
      "fold 1: val_bal_acc=0.542, val_F1=0.483\n",
      "fold 2: val_bal_acc=0.642, val_F1=0.560\n",
      "fold 3: val_bal_acc=0.632, val_F1=0.500\n",
      "fold 4: val_bal_acc=0.677, val_F1=0.571\n",
      "TRIAL 2: mean bal-acc=0.6070.055, mean F1=0.4540.154\n",
      "\n",
      "--- Trial 3/50 ---\n",
      "Config: {'hidden': 64, 'layers': 3, 'dropout': 0.233, 'lr': np.float64(0.0026800235974081), 'wd': np.float64(0.00010259029099934905), 'readout': 's2s'}\n",
      "fold 0: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 1: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 2: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 3: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 4: val_bal_acc=0.658, val_F1=0.500\n",
      "TRIAL 3: mean bal-acc=0.5320.063, mean F1=0.1000.200\n",
      "\n",
      "--- Trial 4/50 ---\n",
      "Config: {'hidden': 128, 'layers': 5, 'dropout': 0.183, 'lr': np.float64(0.0022656139472048455), 'wd': np.float64(0.00041183597957282096), 'readout': 's2s'}\n",
      "fold 0: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 1: val_bal_acc=0.625, val_F1=0.400\n",
      "fold 2: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 3: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 4: val_bal_acc=0.500, val_F1=0.000\n",
      "TRIAL 4: mean bal-acc=0.5250.050, mean F1=0.0800.160\n",
      "\n",
      "--- Trial 5/50 ---\n",
      "Config: {'hidden': 128, 'layers': 5, 'dropout': 0.257, 'lr': np.float64(0.004095259238071209), 'wd': np.float64(0.0007171641966770469), 'readout': 's2s'}\n",
      "fold 0: val_bal_acc=0.542, val_F1=0.154\n",
      "fold 1: val_bal_acc=0.542, val_F1=0.154\n",
      "fold 2: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 3: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 4: val_bal_acc=0.500, val_F1=0.000\n",
      "TRIAL 5: mean bal-acc=0.5170.020, mean F1=0.0620.075\n",
      "\n",
      "--- Trial 6/50 ---\n",
      "Config: {'hidden': 128, 'layers': 4, 'dropout': 0.141, 'lr': np.float64(0.0012162142790822488), 'wd': np.float64(7.660130642856665e-05), 'readout': 'mean'}\n",
      "fold 0: val_bal_acc=0.583, val_F1=0.533\n",
      "fold 1: val_bal_acc=0.600, val_F1=0.375\n",
      "fold 2: val_bal_acc=0.617, val_F1=0.538\n",
      "fold 3: val_bal_acc=0.602, val_F1=0.533\n",
      "fold 4: val_bal_acc=0.595, val_F1=0.564\n",
      "TRIAL 6: mean bal-acc=0.5990.011, mean F1=0.5090.068\n",
      "\n",
      "--- Trial 7/50 ---\n",
      "Config: {'hidden': 128, 'layers': 4, 'dropout': 0.122, 'lr': np.float64(0.0029372502423665836), 'wd': np.float64(0.00036971922529156857), 'readout': 'mean'}\n",
      "fold 0: val_bal_acc=0.683, val_F1=0.556\n",
      "fold 1: val_bal_acc=0.567, val_F1=0.400\n",
      "fold 2: val_bal_acc=0.558, val_F1=0.267\n",
      "fold 3: val_bal_acc=0.610, val_F1=0.444\n",
      "fold 4: val_bal_acc=0.654, val_F1=0.545\n",
      "TRIAL 7: mean bal-acc=0.6140.048, mean F1=0.4420.106\n",
      "\n",
      "--- Trial 8/50 ---\n",
      "Config: {'hidden': 256, 'layers': 3, 'dropout': 0.176, 'lr': np.float64(0.0012472021488114811), 'wd': np.float64(8.619293259102051e-05), 'readout': 's2s'}\n",
      "fold 0: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 1: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 2: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 3: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 4: val_bal_acc=0.500, val_F1=0.000\n",
      "TRIAL 8: mean bal-acc=0.5000.000, mean F1=0.0000.000\n",
      "\n",
      "--- Trial 9/50 ---\n",
      "Config: {'hidden': 64, 'layers': 4, 'dropout': 0.365, 'lr': np.float64(0.0012995378747077074), 'wd': np.float64(0.00021660373382490305), 'readout': 'mean'}\n",
      "fold 0: val_bal_acc=0.567, val_F1=0.556\n",
      "fold 1: val_bal_acc=0.550, val_F1=0.529\n",
      "fold 2: val_bal_acc=0.625, val_F1=0.400\n",
      "fold 3: val_bal_acc=0.656, val_F1=0.526\n",
      "fold 4: val_bal_acc=0.656, val_F1=0.526\n",
      "TRIAL 9: mean bal-acc=0.6110.045, mean F1=0.5080.055\n",
      "\n",
      "--- Trial 10/50 ---\n",
      "Config: {'hidden': 128, 'layers': 5, 'dropout': 0.229, 'lr': np.float64(0.002292938832741763), 'wd': np.float64(4.766858809622575e-05), 'readout': 'mean'}\n",
      "fold 0: val_bal_acc=0.633, val_F1=0.611\n",
      "fold 1: val_bal_acc=0.617, val_F1=0.538\n",
      "fold 2: val_bal_acc=0.675, val_F1=0.621\n",
      "fold 3: val_bal_acc=0.610, val_F1=0.444\n",
      "fold 4: val_bal_acc=0.721, val_F1=0.640\n",
      "TRIAL 10: mean bal-acc=0.6510.041, mean F1=0.5710.072\n",
      "\n",
      "--- Trial 11/50 ---\n",
      "Config: {'hidden': 256, 'layers': 4, 'dropout': 0.128, 'lr': np.float64(0.0018021454117873009), 'wd': np.float64(4.741976220899371e-05), 'readout': 'att'}\n",
      "fold 0: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 1: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 2: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 3: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 4: val_bal_acc=0.500, val_F1=0.000\n",
      "TRIAL 11: mean bal-acc=0.5000.000, mean F1=0.0000.000\n",
      "\n",
      "--- Trial 12/50 ---\n",
      "Config: {'hidden': 384, 'layers': 5, 'dropout': 0.105, 'lr': np.float64(0.00170493207405338), 'wd': np.float64(0.00017695145898002945), 'readout': 's2s'}\n",
      "fold 0: val_bal_acc=0.542, val_F1=0.154\n",
      "fold 1: val_bal_acc=0.542, val_F1=0.154\n",
      "fold 2: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 3: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 4: val_bal_acc=0.500, val_F1=0.000\n",
      "TRIAL 12: mean bal-acc=0.5170.020, mean F1=0.0620.075\n",
      "\n",
      "--- Trial 13/50 ---\n",
      "Config: {'hidden': 64, 'layers': 3, 'dropout': 0.396, 'lr': np.float64(0.0035566581826052047), 'wd': np.float64(0.0008777926039925468), 'readout': 'mean'}\n",
      "fold 0: val_bal_acc=0.608, val_F1=0.552\n",
      "fold 1: val_bal_acc=0.583, val_F1=0.286\n",
      "fold 2: val_bal_acc=0.675, val_F1=0.571\n",
      "fold 3: val_bal_acc=0.541, val_F1=0.333\n",
      "fold 4: val_bal_acc=0.701, val_F1=0.600\n",
      "TRIAL 13: mean bal-acc=0.6220.059, mean F1=0.4680.132\n",
      "\n",
      "--- Trial 14/50 ---\n",
      "Config: {'hidden': 64, 'layers': 4, 'dropout': 0.182, 'lr': np.float64(0.004297303333475706), 'wd': np.float64(2.3072931824055642e-05), 'readout': 'mean'}\n",
      "fold 0: val_bal_acc=0.692, val_F1=0.609\n",
      "fold 1: val_bal_acc=0.592, val_F1=0.421\n",
      "fold 2: val_bal_acc=0.658, val_F1=0.526\n",
      "fold 3: val_bal_acc=0.652, val_F1=0.560\n",
      "fold 4: val_bal_acc=0.723, val_F1=0.636\n",
      "TRIAL 14: mean bal-acc=0.6630.044, mean F1=0.5500.075\n",
      "\n",
      "--- Trial 15/50 ---\n",
      "Config: {'hidden': 384, 'layers': 5, 'dropout': 0.346, 'lr': np.float64(0.0015162120512093678), 'wd': np.float64(1.989463129420913e-05), 'readout': 's2s'}\n",
      "fold 0: val_bal_acc=0.542, val_F1=0.154\n",
      "fold 1: val_bal_acc=0.617, val_F1=0.538\n",
      "fold 2: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 3: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 4: val_bal_acc=0.500, val_F1=0.000\n",
      "TRIAL 15: mean bal-acc=0.5320.045, mean F1=0.1380.209\n",
      "\n",
      "--- Trial 16/50 ---\n",
      "Config: {'hidden': 384, 'layers': 5, 'dropout': 0.198, 'lr': np.float64(0.0015669537496565866), 'wd': np.float64(0.0003973517278168833), 'readout': 'mean'}\n",
      "fold 0: val_bal_acc=0.617, val_F1=0.444\n",
      "fold 1: val_bal_acc=0.767, val_F1=0.700\n",
      "fold 2: val_bal_acc=0.700, val_F1=0.643\n",
      "fold 3: val_bal_acc=0.623, val_F1=0.562\n",
      "fold 4: val_bal_acc=0.680, val_F1=0.556\n",
      "TRIAL 16: mean bal-acc=0.6770.055, mean F1=0.5810.087\n",
      "\n",
      "--- Trial 17/50 ---\n",
      "Config: {'hidden': 384, 'layers': 3, 'dropout': 0.181, 'lr': np.float64(0.0010274598727164279), 'wd': np.float64(1.5035983608358327e-05), 'readout': 'att'}\n",
      "fold 0: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 1: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 2: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 3: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 4: val_bal_acc=0.500, val_F1=0.000\n",
      "TRIAL 17: mean bal-acc=0.5000.000, mean F1=0.0000.000\n",
      "\n",
      "--- Trial 18/50 ---\n",
      "Config: {'hidden': 64, 'layers': 5, 'dropout': 0.357, 'lr': np.float64(0.0011131845074583053), 'wd': np.float64(0.0005315571180140422), 'readout': 'att'}\n",
      "fold 0: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 1: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 2: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 3: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 4: val_bal_acc=0.500, val_F1=0.000\n",
      "TRIAL 18: mean bal-acc=0.5000.000, mean F1=0.0000.000\n",
      "\n",
      "--- Trial 19/50 ---\n",
      "Config: {'hidden': 64, 'layers': 4, 'dropout': 0.398, 'lr': np.float64(0.0019588534359864115), 'wd': np.float64(0.0006774128125452102), 'readout': 's2s'}\n",
      "fold 0: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 1: val_bal_acc=0.583, val_F1=0.286\n",
      "fold 2: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 3: val_bal_acc=0.623, val_F1=0.562\n",
      "fold 4: val_bal_acc=0.500, val_F1=0.000\n",
      "TRIAL 19: mean bal-acc=0.5410.052, mean F1=0.1700.225\n",
      "\n",
      "--- Trial 20/50 ---\n",
      "Config: {'hidden': 128, 'layers': 3, 'dropout': 0.258, 'lr': np.float64(0.0014677758250895736), 'wd': np.float64(1.6553999210815826e-05), 'readout': 'mean'}\n",
      "fold 0: val_bal_acc=0.633, val_F1=0.500\n",
      "fold 1: val_bal_acc=0.767, val_F1=0.714\n",
      "fold 2: val_bal_acc=0.675, val_F1=0.571\n",
      "fold 3: val_bal_acc=0.543, val_F1=0.267\n",
      "fold 4: val_bal_acc=0.647, val_F1=0.581\n",
      "TRIAL 20: mean bal-acc=0.6530.072, mean F1=0.5270.147\n",
      "\n",
      "--- Trial 21/50 ---\n",
      "Config: {'hidden': 256, 'layers': 3, 'dropout': 0.154, 'lr': np.float64(0.004483452899824858), 'wd': np.float64(0.00018085986156945696), 'readout': 's2s'}\n",
      "fold 0: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 1: val_bal_acc=0.500, val_F1=0.545\n",
      "fold 2: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 3: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 4: val_bal_acc=0.500, val_F1=0.000\n",
      "TRIAL 21: mean bal-acc=0.5000.000, mean F1=0.1090.218\n",
      "\n",
      "--- Trial 22/50 ---\n",
      "Config: {'hidden': 128, 'layers': 4, 'dropout': 0.234, 'lr': np.float64(0.0029499708236562943), 'wd': np.float64(3.475719593568576e-05), 'readout': 'mean'}\n",
      "fold 0: val_bal_acc=0.508, val_F1=0.235\n",
      "fold 1: val_bal_acc=0.625, val_F1=0.522\n",
      "fold 2: val_bal_acc=0.625, val_F1=0.522\n",
      "fold 3: val_bal_acc=0.628, val_F1=0.538\n",
      "fold 4: val_bal_acc=0.716, val_F1=0.645\n",
      "TRIAL 22: mean bal-acc=0.6200.066, mean F1=0.4920.137\n",
      "\n",
      "--- Trial 23/50 ---\n",
      "Config: {'hidden': 256, 'layers': 3, 'dropout': 0.105, 'lr': np.float64(0.0032538774994912882), 'wd': np.float64(0.00012650225174010737), 'readout': 'mean'}\n",
      "fold 0: val_bal_acc=0.667, val_F1=0.500\n",
      "fold 1: val_bal_acc=0.558, val_F1=0.516\n",
      "fold 2: val_bal_acc=0.592, val_F1=0.421\n",
      "fold 3: val_bal_acc=0.654, val_F1=0.545\n",
      "fold 4: val_bal_acc=0.658, val_F1=0.500\n",
      "TRIAL 23: mean bal-acc=0.6260.043, mean F1=0.4970.041\n",
      "\n",
      "--- Trial 24/50 ---\n",
      "Config: {'hidden': 384, 'layers': 3, 'dropout': 0.38, 'lr': np.float64(0.00118655390294337), 'wd': np.float64(0.0004343504548928044), 'readout': 'att'}\n",
      "fold 0: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 1: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 2: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 3: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 4: val_bal_acc=0.500, val_F1=0.512\n",
      "TRIAL 24: mean bal-acc=0.5000.000, mean F1=0.1020.205\n",
      "\n",
      "--- Trial 25/50 ---\n",
      "Config: {'hidden': 384, 'layers': 5, 'dropout': 0.35, 'lr': np.float64(0.0018825883895291005), 'wd': np.float64(0.00010312688605626004), 'readout': 's2s'}\n",
      "fold 0: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 1: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 2: val_bal_acc=0.542, val_F1=0.154\n",
      "fold 3: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 4: val_bal_acc=0.610, val_F1=0.444\n",
      "TRIAL 25: mean bal-acc=0.5300.043, mean F1=0.1200.173\n",
      "\n",
      "--- Trial 26/50 ---\n",
      "Config: {'hidden': 128, 'layers': 3, 'dropout': 0.203, 'lr': np.float64(0.0038171860564716765), 'wd': np.float64(0.000259090090849788), 'readout': 's2s'}\n",
      "fold 0: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 1: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 2: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 3: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 4: val_bal_acc=0.500, val_F1=0.000\n",
      "TRIAL 26: mean bal-acc=0.5000.000, mean F1=0.0000.000\n",
      "\n",
      "--- Trial 27/50 ---\n",
      "Config: {'hidden': 128, 'layers': 4, 'dropout': 0.397, 'lr': np.float64(0.004856305708725347), 'wd': np.float64(0.00047203769022863024), 'readout': 'mean'}\n",
      "fold 0: val_bal_acc=0.600, val_F1=0.375\n",
      "fold 1: val_bal_acc=0.700, val_F1=0.600\n",
      "fold 2: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 3: val_bal_acc=0.587, val_F1=0.421\n",
      "fold 4: val_bal_acc=0.680, val_F1=0.556\n",
      "TRIAL 27: mean bal-acc=0.6130.072, mean F1=0.3900.212\n",
      "\n",
      "--- Trial 28/50 ---\n",
      "Config: {'hidden': 64, 'layers': 5, 'dropout': 0.322, 'lr': np.float64(0.0015088721865844836), 'wd': np.float64(2.1207674081149757e-05), 'readout': 'mean'}\n",
      "fold 0: val_bal_acc=0.575, val_F1=0.480\n",
      "fold 1: val_bal_acc=0.642, val_F1=0.560\n",
      "fold 2: val_bal_acc=0.558, val_F1=0.435\n",
      "fold 3: val_bal_acc=0.654, val_F1=0.545\n",
      "fold 4: val_bal_acc=0.604, val_F1=0.519\n",
      "TRIAL 28: mean bal-acc=0.6070.037, mean F1=0.5080.046\n",
      "\n",
      "--- Trial 29/50 ---\n",
      "Config: {'hidden': 384, 'layers': 5, 'dropout': 0.301, 'lr': np.float64(0.0015742108723994944), 'wd': np.float64(3.0508852130084227e-05), 'readout': 'att'}\n",
      "fold 0: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 1: val_bal_acc=0.600, val_F1=0.375\n",
      "fold 2: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 3: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 4: val_bal_acc=0.500, val_F1=0.000\n",
      "TRIAL 29: mean bal-acc=0.5200.040, mean F1=0.0750.150\n",
      "\n",
      "--- Trial 30/50 ---\n",
      "Config: {'hidden': 64, 'layers': 4, 'dropout': 0.156, 'lr': np.float64(0.0015418729437352134), 'wd': np.float64(1.0168231502263165e-05), 'readout': 'att'}\n",
      "fold 0: val_bal_acc=0.542, val_F1=0.154\n",
      "fold 1: val_bal_acc=0.542, val_F1=0.154\n",
      "fold 2: val_bal_acc=0.517, val_F1=0.143\n",
      "fold 3: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 4: val_bal_acc=0.703, val_F1=0.588\n",
      "TRIAL 30: mean bal-acc=0.5610.073, mean F1=0.2080.199\n",
      "\n",
      "--- Trial 31/50 ---\n",
      "Config: {'hidden': 256, 'layers': 5, 'dropout': 0.197, 'lr': np.float64(0.001057005449776419), 'wd': np.float64(0.0005818045903495788), 'readout': 'mean'}\n",
      "fold 0: val_bal_acc=0.650, val_F1=0.545\n",
      "fold 1: val_bal_acc=0.667, val_F1=0.583\n",
      "fold 2: val_bal_acc=0.550, val_F1=0.571\n",
      "fold 3: val_bal_acc=0.584, val_F1=0.455\n",
      "fold 4: val_bal_acc=0.587, val_F1=0.421\n",
      "TRIAL 31: mean bal-acc=0.6080.044, mean F1=0.5150.065\n",
      "\n",
      "--- Trial 32/50 ---\n",
      "Config: {'hidden': 256, 'layers': 3, 'dropout': 0.1, 'lr': np.float64(0.0018481854669287648), 'wd': np.float64(8.897894554652559e-05), 'readout': 's2s'}\n",
      "fold 0: val_bal_acc=0.542, val_F1=0.154\n",
      "fold 1: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 2: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 3: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 4: val_bal_acc=0.519, val_F1=0.250\n",
      "TRIAL 32: mean bal-acc=0.5120.017, mean F1=0.0810.103\n",
      "\n",
      "--- Trial 33/50 ---\n",
      "Config: {'hidden': 128, 'layers': 3, 'dropout': 0.251, 'lr': np.float64(0.0010079993986676678), 'wd': np.float64(3.375494242838719e-05), 'readout': 'mean'}\n",
      "fold 0: val_bal_acc=0.542, val_F1=0.381\n",
      "fold 1: val_bal_acc=0.625, val_F1=0.522\n",
      "fold 2: val_bal_acc=0.600, val_F1=0.375\n",
      "fold 3: val_bal_acc=0.589, val_F1=0.375\n",
      "fold 4: val_bal_acc=0.610, val_F1=0.444\n",
      "TRIAL 33: mean bal-acc=0.5930.028, mean F1=0.4190.057\n",
      "\n",
      "--- Trial 34/50 ---\n",
      "Config: {'hidden': 128, 'layers': 4, 'dropout': 0.276, 'lr': np.float64(0.0018852947238974592), 'wd': np.float64(3.9745880175543886e-05), 'readout': 's2s'}\n",
      "fold 0: val_bal_acc=0.542, val_F1=0.154\n",
      "fold 1: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 2: val_bal_acc=0.550, val_F1=0.571\n",
      "fold 3: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 4: val_bal_acc=0.524, val_F1=0.524\n",
      "TRIAL 34: mean bal-acc=0.5230.021, mean F1=0.2500.250\n",
      "\n",
      "--- Trial 35/50 ---\n",
      "Config: {'hidden': 128, 'layers': 3, 'dropout': 0.276, 'lr': np.float64(0.0023436222952376105), 'wd': np.float64(0.00031701605728784606), 'readout': 's2s'}\n",
      "fold 0: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 1: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 2: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 3: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 4: val_bal_acc=0.500, val_F1=0.000\n",
      "TRIAL 35: mean bal-acc=0.5000.000, mean F1=0.0000.000\n",
      "\n",
      "--- Trial 36/50 ---\n",
      "Config: {'hidden': 384, 'layers': 4, 'dropout': 0.316, 'lr': np.float64(0.002215259018090394), 'wd': np.float64(3.7012903685446716e-05), 'readout': 's2s'}\n",
      "fold 0: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 1: val_bal_acc=0.542, val_F1=0.154\n",
      "fold 2: val_bal_acc=0.525, val_F1=0.558\n",
      "fold 3: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 4: val_bal_acc=0.500, val_F1=0.000\n",
      "TRIAL 36: mean bal-acc=0.5130.017, mean F1=0.1420.216\n",
      "\n",
      "--- Trial 37/50 ---\n",
      "Config: {'hidden': 128, 'layers': 3, 'dropout': 0.347, 'lr': np.float64(0.0031606124146651197), 'wd': np.float64(0.00010616036956067433), 'readout': 'att'}\n",
      "fold 0: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 1: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 2: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 3: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 4: val_bal_acc=0.500, val_F1=0.000\n",
      "TRIAL 37: mean bal-acc=0.5000.000, mean F1=0.0000.000\n",
      "\n",
      "--- Trial 38/50 ---\n",
      "Config: {'hidden': 128, 'layers': 5, 'dropout': 0.326, 'lr': np.float64(0.0024966066226689895), 'wd': np.float64(0.0004224845037693429), 'readout': 'mean'}\n",
      "fold 0: val_bal_acc=0.608, val_F1=0.595\n",
      "fold 1: val_bal_acc=0.617, val_F1=0.538\n",
      "fold 2: val_bal_acc=0.683, val_F1=0.556\n",
      "fold 3: val_bal_acc=0.647, val_F1=0.581\n",
      "fold 4: val_bal_acc=0.630, val_F1=0.522\n",
      "TRIAL 38: mean bal-acc=0.6370.027, mean F1=0.5580.027\n",
      "\n",
      "--- Trial 39/50 ---\n",
      "Config: {'hidden': 128, 'layers': 3, 'dropout': 0.109, 'lr': np.float64(0.0012388768940868665), 'wd': np.float64(5.265200972826751e-05), 'readout': 'mean'}\n",
      "fold 0: val_bal_acc=0.608, val_F1=0.476\n",
      "fold 1: val_bal_acc=0.542, val_F1=0.541\n",
      "fold 2: val_bal_acc=0.592, val_F1=0.421\n",
      "fold 3: val_bal_acc=0.537, val_F1=0.417\n",
      "fold 4: val_bal_acc=0.697, val_F1=0.615\n",
      "TRIAL 39: mean bal-acc=0.5950.058, mean F1=0.4940.076\n",
      "\n",
      "--- Trial 40/50 ---\n",
      "Config: {'hidden': 384, 'layers': 4, 'dropout': 0.268, 'lr': np.float64(0.0027465681437969804), 'wd': np.float64(0.0001788351642286373), 'readout': 's2s'}\n",
      "fold 0: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 1: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 2: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 3: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 4: val_bal_acc=0.522, val_F1=0.154\n",
      "TRIAL 40: mean bal-acc=0.5040.009, mean F1=0.0310.062\n",
      "\n",
      "--- Trial 41/50 ---\n",
      "Config: {'hidden': 128, 'layers': 4, 'dropout': 0.179, 'lr': np.float64(0.00208637952610486), 'wd': np.float64(1.3810934552468163e-05), 'readout': 's2s'}\n",
      "fold 0: val_bal_acc=0.542, val_F1=0.154\n",
      "fold 1: val_bal_acc=0.542, val_F1=0.154\n",
      "fold 2: val_bal_acc=0.542, val_F1=0.154\n",
      "fold 3: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 4: val_bal_acc=0.500, val_F1=0.000\n",
      "TRIAL 41: mean bal-acc=0.5250.020, mean F1=0.0920.075\n",
      "\n",
      "--- Trial 42/50 ---\n",
      "Config: {'hidden': 64, 'layers': 5, 'dropout': 0.258, 'lr': np.float64(0.0033207902115007636), 'wd': np.float64(8.865777969367654e-05), 'readout': 'mean'}\n",
      "fold 0: val_bal_acc=0.642, val_F1=0.560\n",
      "fold 1: val_bal_acc=0.592, val_F1=0.519\n",
      "fold 2: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 3: val_bal_acc=0.565, val_F1=0.353\n",
      "fold 4: val_bal_acc=0.656, val_F1=0.526\n",
      "TRIAL 42: mean bal-acc=0.5910.056, mean F1=0.3920.209\n",
      "\n",
      "--- Trial 43/50 ---\n",
      "Config: {'hidden': 256, 'layers': 3, 'dropout': 0.319, 'lr': np.float64(0.0013913644277476723), 'wd': np.float64(0.0003017568814777921), 'readout': 'att'}\n",
      "fold 0: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 1: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 2: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 3: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 4: val_bal_acc=0.500, val_F1=0.000\n",
      "TRIAL 43: mean bal-acc=0.5000.000, mean F1=0.0000.000\n",
      "\n",
      "--- Trial 44/50 ---\n",
      "Config: {'hidden': 384, 'layers': 4, 'dropout': 0.123, 'lr': np.float64(0.00432901296128346), 'wd': np.float64(3.755245491863097e-05), 'readout': 'mean'}\n",
      "fold 0: val_bal_acc=0.708, val_F1=0.588\n",
      "fold 1: val_bal_acc=0.750, val_F1=0.667\n",
      "fold 2: val_bal_acc=0.550, val_F1=0.333\n",
      "fold 3: val_bal_acc=0.654, val_F1=0.545\n",
      "fold 4: val_bal_acc=0.699, val_F1=0.609\n",
      "TRIAL 44: mean bal-acc=0.6720.068, mean F1=0.5480.114\n",
      "\n",
      "--- Trial 45/50 ---\n",
      "Config: {'hidden': 128, 'layers': 3, 'dropout': 0.28, 'lr': np.float64(0.0017056869765421235), 'wd': np.float64(0.0002009410759709008), 'readout': 's2s'}\n",
      "fold 0: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 1: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 2: val_bal_acc=0.575, val_F1=0.480\n",
      "fold 3: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 4: val_bal_acc=0.561, val_F1=0.435\n",
      "TRIAL 45: mean bal-acc=0.5270.034, mean F1=0.1830.225\n",
      "\n",
      "--- Trial 46/50 ---\n",
      "Config: {'hidden': 256, 'layers': 5, 'dropout': 0.27, 'lr': np.float64(0.001020271149348345), 'wd': np.float64(1.3222757344798792e-05), 'readout': 'att'}\n",
      "fold 0: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 1: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 2: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 3: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 4: val_bal_acc=0.500, val_F1=0.000\n",
      "TRIAL 46: mean bal-acc=0.5000.000, mean F1=0.0000.000\n",
      "\n",
      "--- Trial 47/50 ---\n",
      "Config: {'hidden': 64, 'layers': 5, 'dropout': 0.165, 'lr': np.float64(0.002199002466153512), 'wd': np.float64(0.00026166271470766384), 'readout': 'att'}\n",
      "fold 0: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 1: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 2: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 3: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 4: val_bal_acc=0.500, val_F1=0.512\n",
      "TRIAL 47: mean bal-acc=0.5000.000, mean F1=0.1020.205\n",
      "\n",
      "--- Trial 48/50 ---\n",
      "Config: {'hidden': 384, 'layers': 4, 'dropout': 0.24, 'lr': np.float64(0.0012101242012037428), 'wd': np.float64(0.0006128100123224952), 'readout': 'mean'}\n",
      "fold 0: val_bal_acc=0.642, val_F1=0.471\n",
      "fold 1: val_bal_acc=0.633, val_F1=0.500\n",
      "fold 2: val_bal_acc=0.658, val_F1=0.526\n",
      "fold 3: val_bal_acc=0.626, val_F1=0.552\n",
      "fold 4: val_bal_acc=0.558, val_F1=0.462\n",
      "TRIAL 48: mean bal-acc=0.6230.034, mean F1=0.5020.034\n",
      "\n",
      "--- Trial 49/50 ---\n",
      "Config: {'hidden': 256, 'layers': 3, 'dropout': 0.381, 'lr': np.float64(0.0010285729280868664), 'wd': np.float64(8.27830924556389e-05), 'readout': 's2s'}\n",
      "fold 0: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 1: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 2: val_bal_acc=0.533, val_F1=0.250\n",
      "fold 3: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 4: val_bal_acc=0.500, val_F1=0.000\n",
      "TRIAL 49: mean bal-acc=0.5070.013, mean F1=0.0500.100\n",
      "\n",
      "--- Trial 50/50 ---\n",
      "Config: {'hidden': 384, 'layers': 4, 'dropout': 0.216, 'lr': np.float64(0.004371639057681903), 'wd': np.float64(0.0007262265318063068), 'readout': 'mean'}\n",
      "fold 0: val_bal_acc=0.542, val_F1=0.154\n",
      "fold 1: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 2: val_bal_acc=0.500, val_F1=0.000\n",
      "fold 3: val_bal_acc=0.589, val_F1=0.375\n",
      "fold 4: val_bal_acc=0.677, val_F1=0.571\n",
      "TRIAL 50: mean bal-acc=0.5620.067, mean F1=0.2200.223\n"
     ]
    }
   ],
   "source": [
    "# ----- MAIN RANDOM SEARCH LOOP -----\n",
    "results = []\n",
    "dyn_dim = next(iter(dyn_feat_dict_scaled.values())).shape[-1]\n",
    "r_state = 7\n",
    "for trial in range(N_TRIALS):\n",
    "    cfg = sample_config()\n",
    "    print(f\"\\n--- Trial {trial+1}/{N_TRIALS} ---\\nConfig: {cfg}\")\n",
    "\n",
    "    all_balacc, all_f1 = [], []\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=r_state)\n",
    "    G_overlap_dgl_self_loops = dgl.add_self_loop(dgl.remove_self_loop(G_overlap_dgl))\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(all_samples, dataset.y)):\n",
    "        train_sample_ids = [all_samples[i] for i in train_idx]\n",
    "        val_sample_ids   = [all_samples[i] for i in val_idx]\n",
    "\n",
    "        # Dynamic features (fit scaler only on train)\n",
    "        dyn_feat_dict_scaled = scale_dyn_dict(train_sample_ids, all_samples, orig_dyn_feat_dict)\n",
    "\n",
    "        # Build dataset for this fold\n",
    "        dataset_fold = SingleGraphSampleDataset(\n",
    "            G_overlap_dgl_self_loops, node_meta_numeric,\n",
    "            dyn_feat_dict_scaled, presence_mask_dict,\n",
    "            colon_meta.set_index(\"sample_id\")[\"N\"]\n",
    "        )\n",
    "\n",
    "        train_ds = Subset(dataset_fold, train_idx)\n",
    "        val_ds   = Subset(dataset_fold, val_idx)\n",
    "        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "        val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "        # ----- MODEL -----\n",
    "        model = MaskedGIN(\n",
    "            static_dim=node_meta_numeric.shape[1], dyn_dim=dyn_dim,  \n",
    "            hidden_dim=cfg['hidden'], num_layers=cfg['layers'],\n",
    "            dropout=cfg['dropout'], readout=cfg['readout']\n",
    "        ).to(device)\n",
    "\n",
    "        # ----- OPTIMIZER & LOSS -----\n",
    "        opt = torch.optim.AdamW(model.parameters(), lr=cfg['lr'], weight_decay=cfg['wd'])\n",
    "        criterion = torch.nn.CrossEntropyLoss()  # <--- USE YOUR LOSS HERE\n",
    "\n",
    "        best_ba = 0; best_f1 = 0\n",
    "        patience = 50\n",
    "        epochs_since_best = 0\n",
    "        max_epochs = 120\n",
    "\n",
    "        for epoch in range(max_epochs):\n",
    "            train_one_epoch(model, train_loader, device, criterion, opt)\n",
    "            tr_ba, tr_f1 = evaluate(model, train_loader, device)\n",
    "            ba, f1 = evaluate(model, val_loader, device)\n",
    "            if ba > best_ba:\n",
    "                best_ba = ba; best_f1 = f1\n",
    "                epochs_since_best = 0\n",
    "            else:\n",
    "                epochs_since_best += 1\n",
    "            if epochs_since_best >= patience:\n",
    "                break\n",
    "\n",
    "        all_balacc.append(best_ba)\n",
    "        all_f1.append(best_f1)\n",
    "        print(f\"fold {fold}: val_bal_acc={best_ba:.3f}, val_F1={best_f1:.3f}\")\n",
    "\n",
    "    mean_ba = np.mean(all_balacc)\n",
    "    mean_f1 = np.mean(all_f1)\n",
    "    std_ba = np.std(all_balacc)\n",
    "    std_f1 = np.std(all_f1)\n",
    "    print(f\"TRIAL {trial+1}: mean bal-acc={mean_ba:.3f}{std_ba:.3f}, mean F1={mean_f1:.3f}{std_f1:.3f}\")\n",
    "\n",
    "    results.append({'cfg': cfg, 'mean_balacc': mean_ba, 'std_balacc': std_ba, 'mean_f1': mean_f1, 'std_f1': std_f1})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e7735c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best config by mean balanced accuracy:\n",
      " {'cfg': {'hidden': 384, 'layers': 5, 'dropout': 0.198, 'lr': np.float64(0.0015669537496565866), 'wd': np.float64(0.0003973517278168833), 'readout': 'mean'}, 'mean_balacc': np.float64(0.6772727272727272), 'std_balacc': np.float64(0.054938800237341566), 'mean_f1': np.float64(0.5810714285714285), 'std_f1': np.float64(0.08674603174603175)}\n",
      "\n",
      "Top 3 configs:\n",
      "Rank 1: {'cfg': {'hidden': 384, 'layers': 5, 'dropout': 0.198, 'lr': np.float64(0.0015669537496565866), 'wd': np.float64(0.0003973517278168833), 'readout': 'mean'}, 'mean_balacc': np.float64(0.6772727272727272), 'std_balacc': np.float64(0.054938800237341566), 'mean_f1': np.float64(0.5810714285714285), 'std_f1': np.float64(0.08674603174603175)}\n",
      "Rank 2: {'cfg': {'hidden': 384, 'layers': 4, 'dropout': 0.123, 'lr': np.float64(0.00432901296128346), 'wd': np.float64(3.755245491863097e-05), 'readout': 'mean'}, 'mean_balacc': np.float64(0.6722294372294373), 'std_balacc': np.float64(0.0683516609247378), 'mean_f1': np.float64(0.5484770983492211), 'std_f1': np.float64(0.11443053601707422)}\n",
      "Rank 3: {'cfg': {'hidden': 64, 'layers': 4, 'dropout': 0.182, 'lr': np.float64(0.004297303333475706), 'wd': np.float64(2.3072931824055642e-05), 'readout': 'mean'}, 'mean_balacc': np.float64(0.6632251082251082), 'std_balacc': np.float64(0.04394746476535066), 'mean_f1': np.float64(0.5504855419180361), 'std_f1': np.float64(0.07508800725318271)}\n"
     ]
    }
   ],
   "source": [
    "# ----- BEST CONFIG SELECTION -----\n",
    "results_sorted = sorted(results, key=lambda x: x['mean_balacc'], reverse=True)\n",
    "best_result = results_sorted[0]\n",
    "print(\"\\nBest config by mean balanced accuracy:\\n\", best_result)\n",
    "print(\"\\nTop 3 configs:\")\n",
    "for i, res in enumerate(results_sorted[:3]):\n",
    "    print(f\"Rank {i+1}: {res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6376b123",
   "metadata": {},
   "source": [
    "### K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "48500a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Seed 0 ====\n",
      "Epoch 000 | Loss 0.9057 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 005 | Loss 0.6923 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 010 | Loss 0.6889 | Val bal-acc 0.589 | Val F1 0.444\n",
      "Epoch 015 | Loss 0.6872 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 020 | Loss 0.6387 | Val bal-acc 0.444 | Val F1 0.000\n",
      "Epoch 025 | Loss 0.6812 | Val bal-acc 0.433 | Val F1 0.222\n",
      "Epoch 030 | Loss 0.6971 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 035 | Loss 0.6919 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 040 | Loss 0.6919 | Val bal-acc 0.400 | Val F1 0.444\n",
      "Epoch 045 | Loss 0.6918 | Val bal-acc 0.444 | Val F1 0.000\n",
      "Epoch 050 | Loss 0.6932 | Val bal-acc 0.444 | Val F1 0.000\n",
      "Epoch 055 | Loss 0.6920 | Val bal-acc 0.444 | Val F1 0.000\n",
      "Epoch 060 | Loss 0.6898 | Val bal-acc 0.400 | Val F1 0.444\n",
      "Epoch 065 | Loss 0.6888 | Val bal-acc 0.567 | Val F1 0.533\n",
      "Epoch 070 | Loss 0.6927 | Val bal-acc 0.400 | Val F1 0.444\n",
      "Epoch 075 | Loss 0.6873 | Val bal-acc 0.400 | Val F1 0.444\n",
      "Epoch 080 | Loss 0.6854 | Val bal-acc 0.400 | Val F1 0.444\n",
      "Epoch 085 | Loss 0.6888 | Val bal-acc 0.456 | Val F1 0.471\n",
      "Epoch 090 | Loss 0.6906 | Val bal-acc 0.378 | Val F1 0.200\n",
      "Epoch 095 | Loss 0.6860 | Val bal-acc 0.400 | Val F1 0.444\n",
      "Epoch 100 | Loss 0.6877 | Val bal-acc 0.444 | Val F1 0.000\n",
      "Epoch 105 | Loss 0.6845 | Val bal-acc 0.444 | Val F1 0.000\n",
      "Early stopping at epoch 106\n",
      "SEED 0: Test bal-acc: 0.522, Test F1: 0.421\n",
      "\n",
      "==== Seed 2 ====\n",
      "Epoch 000 | Loss 0.8350 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 005 | Loss 0.6921 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 010 | Loss 0.6914 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 015 | Loss 0.6931 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 020 | Loss 0.6940 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 025 | Loss 0.6939 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 030 | Loss 0.6934 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 035 | Loss 0.6912 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 040 | Loss 0.6924 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 045 | Loss 0.6945 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 050 | Loss 0.6921 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 055 | Loss 0.6957 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Early stopping at epoch 57\n",
      "SEED 2: Test bal-acc: 0.467, Test F1: 0.333\n",
      "\n",
      "==== Seed 7 ====\n",
      "Epoch 000 | Loss 1.1330 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 005 | Loss 0.6938 | Val bal-acc 0.467 | Val F1 0.429\n",
      "Epoch 010 | Loss 0.6881 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 015 | Loss 0.6950 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 020 | Loss 0.6919 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 025 | Loss 0.6915 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 030 | Loss 0.6410 | Val bal-acc 0.389 | Val F1 0.000\n",
      "Early stopping at epoch 30\n",
      "SEED 7: Test bal-acc: 0.500, Test F1: 0.545\n",
      "\n",
      "==== Seed 13 ====\n",
      "Epoch 000 | Loss 0.8554 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 005 | Loss 0.7086 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 010 | Loss 0.6960 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 015 | Loss 0.6929 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 020 | Loss 0.6951 | Val bal-acc 0.622 | Val F1 0.571\n",
      "Epoch 025 | Loss 0.6946 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 030 | Loss 0.6916 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Early stopping at epoch 31\n",
      "SEED 13: Test bal-acc: 0.489, Test F1: 0.400\n",
      "\n",
      "==== Seed 42 ====\n",
      "Epoch 000 | Loss 0.8257 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 005 | Loss 0.6935 | Val bal-acc 0.389 | Val F1 0.000\n",
      "Epoch 010 | Loss 0.6897 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 015 | Loss 0.7031 | Val bal-acc 0.456 | Val F1 0.471\n",
      "Epoch 020 | Loss 0.6924 | Val bal-acc 0.556 | Val F1 0.556\n",
      "Epoch 025 | Loss 0.6891 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 030 | Loss 0.6919 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Early stopping at epoch 31\n",
      "SEED 42: Test bal-acc: 0.444, Test F1: 0.417\n",
      "\n",
      "===== Final 5-seed Results =====\n",
      "Test bal-acc: 0.484  0.027\n",
      "Test F1:      0.423  0.069\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "seeds = [0, 2, 7, 13, 42]\n",
    "# seeds = np.random.randint(0,10000, size=5).tolist()\n",
    "test_balaccs = []\n",
    "test_f1s = []\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f\"\\n==== Seed {seed} ====\")\n",
    "    # 1. Stratified split: 85% dev, 15% test\n",
    "    trainval_ids, test_ids, trainval_labels, test_labels = train_test_split(\n",
    "        all_samples, labels, test_size=0.15, stratify=labels, random_state=seed\n",
    "    )\n",
    "    # 2. Within dev, 10% val\n",
    "    train_ids, val_ids, train_labels, val_labels = train_test_split(\n",
    "        trainval_ids, trainval_labels, test_size=0.10, stratify=trainval_labels, random_state=seed\n",
    "    )\n",
    "\n",
    "    train_set = sample_id_subset(dataset, train_ids)\n",
    "    val_set   = sample_id_subset(dataset, val_ids)\n",
    "    test_set  = sample_id_subset(dataset, test_ids)\n",
    "\n",
    "    BATCH_SIZE = 16\n",
    "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_fn)\n",
    "    val_loader   = DataLoader(val_set,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader  = DataLoader(test_set,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    dyn_dim = next(iter(dyn_feat_dict_scaled.values())).shape[-1]\n",
    "    model = MaskedGIN(\n",
    "        static_dim=node_meta_numeric.shape[1],\n",
    "        dyn_dim=dyn_dim,\n",
    "        hidden_dim=128,\n",
    "        num_layers=5,\n",
    "        num_classes=2,\n",
    "        dropout=0.294,\n",
    "        readout=\"mean\"\n",
    "    ).to(device)\n",
    "\n",
    "    class_counts = np.bincount(labels)\n",
    "    weights = 1.0 / torch.tensor(class_counts, dtype=torch.float32)\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=weights.to(device))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.002087306672736472, weight_decay=0.0007211683580957234)\n",
    "\n",
    "    best_ba = 0\n",
    "    best_f1 = 0\n",
    "    patience = 30\n",
    "    epochs_since_best = 0\n",
    "    max_epochs = 120\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        train_loss = train_one_epoch(model, train_loader, device, criterion, optimizer)\n",
    "        ba, f1 = evaluate(model, val_loader, device)\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch:03d} | Loss {train_loss:.4f} | \"\n",
    "                  f\"Val bal-acc {ba:.3f} | Val F1 {f1:.3f}\")\n",
    "        if ba > best_ba:\n",
    "            best_ba = ba\n",
    "            best_f1 = f1\n",
    "            epochs_since_best = 0\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            epochs_since_best += 1\n",
    "        if epochs_since_best >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    # Restore best weights and evaluate on test set\n",
    "    model.load_state_dict(best_state)\n",
    "    test_balacc, test_f1 = evaluate(model, test_loader, device)\n",
    "    print(f\"SEED {seed}: Test bal-acc: {test_balacc:.3f}, Test F1: {test_f1:.3f}\")\n",
    "    test_balaccs.append(test_balacc)\n",
    "    test_f1s.append(test_f1)\n",
    "\n",
    "# Summary: report mean  std across all seeds\n",
    "mean_balacc = np.mean(test_balaccs)\n",
    "std_balacc = np.std(test_balaccs)\n",
    "mean_f1 = np.mean(test_f1s)\n",
    "std_f1 = np.std(test_f1s)\n",
    "print(\"\\n===== Final 5-seed Results =====\")\n",
    "print(f\"Test bal-acc: {mean_balacc:.3f}  {std_balacc:.3f}\")\n",
    "print(f\"Test F1:      {mean_f1:.3f}  {std_f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec276e40",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dgl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m all_balacc, all_f1 \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m      4\u001b[0m kfold \u001b[38;5;241m=\u001b[39m StratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m G_overlap_dgl_self_loops \u001b[38;5;241m=\u001b[39m \u001b[43mdgl\u001b[49m\u001b[38;5;241m.\u001b[39madd_self_loop(dgl\u001b[38;5;241m.\u001b[39mremove_self_loop(G_overlap_dgl))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# orig_dyn_feat_dict, presence_mask_dict = build_orig_dyn_feat_dict(\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#     clonotype_df, node_idx, all_samples, if_presence=False)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# You must provide both the train IDs (to fit), and all IDs (to transform)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold, (train_idx, val_idx) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(kfold\u001b[38;5;241m.\u001b[39msplit(all_samples, dataset\u001b[38;5;241m.\u001b[39my)):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dgl' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "all_balacc, all_f1 = [], []\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "G_overlap_dgl_self_loops = dgl.add_self_loop(dgl.remove_self_loop(G_overlap_dgl))\n",
    "\n",
    "# orig_dyn_feat_dict, presence_mask_dict = build_orig_dyn_feat_dict(\n",
    "#     clonotype_df, node_idx, all_samples, if_presence=False)\n",
    "# You must provide both the train IDs (to fit), and all IDs (to transform)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(all_samples, dataset.y)):\n",
    "    train_sample_ids = [all_samples[i] for i in train_idx]\n",
    "    val_sample_ids   = [all_samples[i] for i in val_idx]\n",
    "\n",
    "    # 2. Scale dynamic features (fit only on train, apply to all)\n",
    "    dyn_feat_dict_scaled = scale_dyn_dict(train_sample_ids, all_samples, orig_dyn_feat_dict)\n",
    "\n",
    "    # 3. Rebuild dataset for this fold (now using scaled dynamic features!)\n",
    "    dataset_fold = SingleGraphSampleDataset(\n",
    "        G_overlap_dgl_self_loops, node_meta_numeric,\n",
    "        dyn_feat_dict_scaled, presence_mask_dict,\n",
    "        colon_meta.set_index(\"sample_id\")[\"N\"]\n",
    "    )\n",
    "\n",
    "    train_ds = Subset(dataset_fold, train_idx)\n",
    "    val_ds   = Subset(dataset_fold, val_idx)\n",
    "    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Model\n",
    "    model = MaskedGIN(\n",
    "        static_dim=node_meta_numeric.shape[1], dyn_dim=1,  # 1 features: logRF\n",
    "        hidden_dim=256, num_layers=5, dropout=0.4, attention=False\n",
    "    ).to(device)\n",
    "    \n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=2e-3, weight_decay=5e-5)\n",
    "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, patience=5, factor=0.5)\n",
    "    criterion = torch.nn.CrossEntropyLoss() \n",
    "\n",
    "    best_ba = 0; best_f1 = 0; best_ba_tr = 0; best_f1_tr = 0\n",
    "    for epoch in range(150):\n",
    "        train_one_epoch(model, train_loader, device, criterion, opt)\n",
    "        tr_ba, tr_f1 = evaluate(model, train_loader, device)\n",
    "        ba, f1 = evaluate(model, val_loader, device)\n",
    "        sched.step(ba)\n",
    "        if ba > best_ba: best_ba = ba; best_f1 = f1\n",
    "        if tr_ba > best_ba_tr: best_ba_tr = tr_ba; best_f1_tr = tr_f1\n",
    "    \n",
    "    all_balacc.append(best_ba); all_f1.append(best_f1)\n",
    "    print(f\"train: fold {fold}: bal-acc={best_ba_tr:.3f}, F1={best_f1_tr:.3f}\")\n",
    "    print(f\"validation: fold {fold}: bal-acc={best_ba:.3f}, F1={best_f1:.3f}\")\n",
    "\n",
    "print(f\"\\nCV mean bal-acc {np.mean(all_balacc):.3f}  {np.std(all_balacc):.3f}\")\n",
    "print(f\"CV mean F1      {np.mean(all_f1):.3f}  {np.std(all_f1):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b500aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, random\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1.  Global deterministic settings\n",
    "# ------------------------------------------------------------------\n",
    "def set_all_seeds(seed: int = 0):\n",
    "    # 1) Python / NumPy / Torch / DGL RNGs\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    dgl.seed(seed); dgl.random.seed(seed)\n",
    "\n",
    "    # 2) cuDNN settings: deterministic kernels OFF, benchmarking OFF\n",
    "    torch.backends.cudnn.deterministic = False   # allow fastest kernels\n",
    "    torch.backends.cudnn.benchmark     = False   # keep reproducible speed\n",
    "\n",
    "GLOBAL_SEED = 2\n",
    "set_all_seeds(GLOBAL_SEED)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def make_loader(dataset, batch_size, shuffle, seed):\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "    return DataLoader(dataset,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=shuffle,\n",
    "                      generator=g,\n",
    "                      num_workers=0,\n",
    "                      collate_fn=collate_fn)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  Utility: fit scaler once per trial\n",
    "# ------------------------------------------------------------------\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def fit_scaler(train_ids, dyn_dict):\n",
    "    arr = np.concatenate([dyn_dict[sid] for sid in train_ids], axis=0)\n",
    "    return StandardScaler().fit(arr)\n",
    "\n",
    "def transform_dyn_dict(scaler, all_ids, dyn_dict):\n",
    "    return {sid: torch.tensor(scaler.transform(dyn_dict[sid]),\n",
    "                              dtype=torch.float32)\n",
    "            for sid in all_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d426bb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold0: val BA=0.500, F1=0.000\n",
      "fold1: val BA=0.567, F1=0.500\n",
      "fold2: val BA=0.558, F1=0.516\n",
      "fold3: val BA=0.515, F1=0.364\n",
      "fold4: val BA=0.654, F1=0.545\n",
      "\n",
      "CV mean BA = 0.559  0.054\n",
      "CV mean F1 = 0.385  0.202\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3.  Prepare labels & sample ordering\n",
    "# ------------------------------------------------------------------\n",
    "all_samples = colon_meta[\"sample_id\"].tolist()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4.  Fixed hyper-parameters (best from previous search)\n",
    "# ------------------------------------------------------------------\n",
    "BEST_CFG = dict(hidden=256, layers=5, dropout=0.4,\n",
    "                lr=2e-3, wd=2e-5, readout=\"mean\")\n",
    "BATCH_SIZE   = 16\n",
    "MAX_EPOCHS   = 200\n",
    "PATIENCE     = 50\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5.  5-fold CV loop\n",
    "# ------------------------------------------------------------------\n",
    "fold_balacc, fold_f1 = [], []\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=GLOBAL_SEED)\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(kfold.split(all_samples, dataset.y)):\n",
    "    train_ids = [all_samples[i] for i in tr_idx]\n",
    "    val_ids   = [all_samples[i] for i in va_idx]\n",
    "\n",
    "    # --- fit scaler ON TRAIN ONLY, then transform all ---\n",
    "    scaler = fit_scaler(train_ids, orig_dyn_feat_dict)\n",
    "    dyn_scaled = transform_dyn_dict(scaler, all_samples, orig_dyn_feat_dict)\n",
    "    dyn_dim = next(iter(dyn_scaled.values())).shape[1]\n",
    "\n",
    "    # --- clone graph (self-loops) per fold ---\n",
    "    g_fold = dgl.add_self_loop(dgl.remove_self_loop(G_overlap_dgl.clone()))\n",
    "\n",
    "    dataset_fold = SingleGraphSampleDataset(\n",
    "        g_fold, node_meta_numeric,\n",
    "        dyn_scaled, presence_mask_dict,\n",
    "        colon_meta.set_index(\"sample_id\")[\"N\"]\n",
    "    )\n",
    "    train_ds = Subset(dataset_fold, tr_idx)\n",
    "    val_ds   = Subset(dataset_fold, va_idx)\n",
    "\n",
    "    train_loader = make_loader(train_ds, BATCH_SIZE, True,  GLOBAL_SEED+fold)\n",
    "    val_loader   = make_loader(val_ds,   BATCH_SIZE, False, GLOBAL_SEED+fold)\n",
    "\n",
    "    # --- model ---\n",
    "    model = MaskedGIN(static_dim=node_meta_numeric.shape[1],\n",
    "                      dyn_dim=dyn_dim,\n",
    "                      hidden_dim=BEST_CFG['hidden'],\n",
    "                      num_layers=BEST_CFG['layers'],\n",
    "                      dropout=BEST_CFG['dropout'],\n",
    "                      readout=BEST_CFG['readout']).to(DEVICE)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(),\n",
    "                            lr=BEST_CFG['lr'],\n",
    "                            weight_decay=BEST_CFG['wd'])\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, patience=5, factor=0.5)\n",
    "\n",
    "    best_ba = best_f1 = 0\n",
    "    since_best = 0\n",
    "\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        train_one_epoch(model, train_loader, DEVICE, criterion, opt)\n",
    "        ba, f1 = evaluate(model, val_loader, DEVICE)\n",
    "        sched.step(ba)\n",
    "\n",
    "        if ba > best_ba:\n",
    "            best_ba, best_f1 = ba, f1\n",
    "            since_best = 0\n",
    "        else:\n",
    "            since_best += 1\n",
    "        if since_best >= PATIENCE:\n",
    "            break\n",
    "\n",
    "    fold_balacc.append(best_ba)\n",
    "    fold_f1.append(best_f1)\n",
    "    print(f\"fold{fold}: val BA={best_ba:.3f}, F1={best_f1:.3f}\")\n",
    "\n",
    "print(\"\\nCV mean BA = {:.3f}  {:.3f}\".format(np.mean(fold_balacc),\n",
    "                                             np.std(fold_balacc)))\n",
    "print(\"CV mean F1 = {:.3f}  {:.3f}\".format(np.mean(fold_f1),\n",
    "                                            np.std(fold_f1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5c4947",
   "metadata": {},
   "source": [
    "### TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22a36dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler mean first dyn feature: 3.377226971803393e-06\n"
     ]
    }
   ],
   "source": [
    "# 2. fit scaler ON (train+val) of first split ONLY to verify mean0\n",
    "scaler = fit_scaler(all_samples[:120], orig_dyn_feat_dict)\n",
    "print('Scaler mean first dyn feature:', scaler.mean_[0])   # sanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fb6683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A] dyn std  = 1.658\n",
      "[A] stat std = 1.000  (should be roughly same order)\n"
     ]
    }
   ],
   "source": [
    "# --- choose one labelled sample in the current train fold ---\n",
    "sample_id = train_ids[0]            # first sample in this fold\n",
    "dyn_std   = dyn_scaled[sample_id].std().item()\n",
    "stat_std  = node_meta_numeric.values.std()\n",
    "\n",
    "print(f\"[A] dyn std  = {dyn_std:.3f}\")\n",
    "print(f\"[A] stat std = {stat_std:.3f}  (should be roughly same order)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee77b52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B] mask true ratio = 0.122  (expected ~0.15-0.25)\n"
     ]
    }
   ],
   "source": [
    "mask_ratio = presence_mask_dict[sample_id].float().mean().item()\n",
    "print(f\"[B] mask true ratio = {mask_ratio:.3f}  (expected ~0.15-0.25)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14493c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D] tiny-set bal-acc = 1.000  (should reach 0.90)\n"
     ]
    }
   ],
   "source": [
    "def overfit_tiny_set(model_cls, train_dataset, epochs=200, seed=0):\n",
    "    tiny_idx = random.sample(range(len(train_dataset)), 8)   # stratified optional\n",
    "    tiny_loader = DataLoader(Subset(train_dataset, tiny_idx),\n",
    "                             batch_size=8, shuffle=True,\n",
    "                             collate_fn=collate_fn)\n",
    "    model = model_cls().to(DEVICE)\n",
    "    opt   = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss  = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        train_one_epoch(model, tiny_loader, DEVICE, loss, opt)\n",
    "    ba, _ = evaluate(model, tiny_loader, DEVICE)\n",
    "    return ba\n",
    "\n",
    "ba_tiny = overfit_tiny_set(\n",
    "    lambda: MaskedGIN(static_dim=node_meta_numeric.shape[1],\n",
    "                      dyn_dim=dyn_dim, hidden_dim=128, num_layers=3,\n",
    "                      dropout=0.203, readout='att'),\n",
    "    train_ds)\n",
    "print(f\"[D] tiny-set bal-acc = {ba_tiny:.3f}  (should reach 0.90)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e14171",
   "metadata": {},
   "source": [
    "# Running the Pipeline Without Test Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52b770b",
   "metadata": {},
   "source": [
    "## Create samples groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a901e35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e105039",
   "metadata": {},
   "source": [
    "separate samples into train-val, test, and unlabled.\\n\n",
    "Calculate corrolation stats, and construct overlap graph using largest lcc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc49c88",
   "metadata": {},
   "source": [
    "exclude_samples = ['P2-S8', 'P2-S9', 'P2-S19', 'P8-S18', 'P9-S25'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb2f7236",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_State_test = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86ae7c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixcr_dir = \"/dsi/efroni-lab/sbm/OrrBavly/colon_data/new_mixcr/TRB/\"\n",
    "meta_file = \"/home/dsi/orrbavly/GNN_project/data/colon_meta_time.csv\"\n",
    "\n",
    "time_threshold = 750\n",
    "meta_df = pd.read_csv(meta_file)\n",
    "meta_df = meta_df[[\"Sample_ID\", \"extraction_time\"]]\n",
    "\n",
    "# Columns to keep from MiXCR files\n",
    "mixcr_cols = [\n",
    "    \"aaSeqCDR3\", \"nSeqCDR3\", \"readCount\", 'readFraction',\n",
    "    \"allVHitsWithScore\", \"allDHitsWithScore\", \"allJHitsWithScore\"\n",
    "]\n",
    "clonotype_dfs = []\n",
    "\n",
    "for fname in os.listdir(mixcr_dir):\n",
    "    if fname.endswith(\".tsv\") and os.path.isfile(os.path.join(mixcr_dir, fname)):\n",
    "        sample_prefix = fname.split(\"_\")[0]\n",
    "        file_path = os.path.join(mixcr_dir, fname)\n",
    "        \n",
    "        df = pd.read_csv(file_path, sep=\"\\t\", usecols=mixcr_cols)\n",
    "        df[\"Sample_ID\"] = sample_prefix\n",
    "        clonotype_dfs.append(df)\n",
    "\n",
    "clonotype_df = pd.concat(clonotype_dfs, ignore_index=True)\n",
    "\n",
    "# Merge with metadata to get extraction time\n",
    "clonotype_df = clonotype_df.merge(meta_df, on=\"Sample_ID\", how=\"inner\")\n",
    "# Create 'group' column: fast vs slow\n",
    "clonotype_df[\"group\"] = clonotype_df[\"extraction_time\"].apply(lambda x: \"fast\" if x <= time_threshold else \"slow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f4edfdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212\n"
     ]
    }
   ],
   "source": [
    "all_samples = clonotype_df['Sample_ID'].unique().tolist()\n",
    "print(clonotype_df['Sample_ID'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3b3c991",
   "metadata": {},
   "outputs": [],
   "source": [
    "colon_meta = pd.read_csv(\"/home/dsi/orrbavly/GNN_project/data/metadata/colon_meta.csv\")\n",
    "colon_meta = colon_meta.loc[:, ~colon_meta.columns.str.contains('^Unnamed')]\n",
    "relevant_values = ['0', '1', '1a', '1b', '2']\n",
    "# List of samples to exclude - did not undergo mixcr due to missing lanes.\n",
    "exclude_samples = ['P2-S8', 'P2-S9', 'P2-S19', 'P8-S18'] # 'P4-S1'\n",
    "colon_meta = colon_meta[colon_meta['N'].isin(relevant_values)]\n",
    "colon_meta = colon_meta[~colon_meta['sample_id'].isin(exclude_samples)].copy()\n",
    "\n",
    "valid_samples = colon_meta['sample_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90820cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. match labels to valid_samples ---\n",
    "labels = []\n",
    "for sid in valid_samples:\n",
    "    row = colon_meta.loc[colon_meta[\"sample_id\"] == sid, \"N\"]\n",
    "    if row.empty:\n",
    "        raise ValueError(f\"Sample {sid} not found in colon_meta\")\n",
    "    labels.append(0 if row.iloc[0] == \"0\" else 1)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a76d01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val: 136 | Test: 24 | Unlabeled: 52\n"
     ]
    }
   ],
   "source": [
    "# --- 2. split into train_val / test ---\n",
    "trainval_ids, test_ids, y_trainval, y_test = train_test_split(\n",
    "    valid_samples, labels, test_size=0.15, stratify=labels, random_state=r_State_test\n",
    ")\n",
    "\n",
    "# --- 3. identify unlabeled samples ---\n",
    "unlabeled_ids = [s for s in all_samples if s not in valid_samples]\n",
    "\n",
    "# --- 4. save splits ---\n",
    "splits = {\n",
    "    \"trainval\": trainval_ids,\n",
    "    \"test\": test_ids,\n",
    "    \"unlabeled\": unlabeled_ids,\n",
    "}\n",
    "json.dump(splits, open(\"/dsi/efroni-lab/sbm/OrrBavly/colon_data/GNN_data/sample_splits.json\", \"w\"), indent=2)\n",
    "\n",
    "print(f\"Train/Val: {len(trainval_ids)} | Test: {len(test_ids)} | Unlabeled: {len(unlabeled_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa616a9",
   "metadata": {},
   "source": [
    "## TCR Corrolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13f42d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_universe_ids = trainval_ids + unlabeled_ids\n",
    "len(graph_universe_ids) # should be 136 + 52 = 188"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54f8368",
   "metadata": {},
   "source": [
    "Now lets focus on construction set (train-val + unlabled samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7ba051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = clonotype_df.loc[\n",
    "    clonotype_df['Sample_ID'].isin(all_samples) &\n",
    "    clonotype_df['group'].isin(['fast','slow'])\n",
    "].copy()\n",
    "\n",
    "# A narrow view ONLY for the survivor/qualification logic\n",
    "logic_df = sub[['Sample_ID','group','aaSeqCDR3']].dropna(subset=['aaSeqCDR3','group'])\n",
    "\n",
    "# 1) survivors: AA present in BOTH groups\n",
    "presence = (logic_df.groupby(['aaSeqCDR3','group'])['Sample_ID']\n",
    "                    .size().unstack(fill_value=0))\n",
    "survivors = presence.index[(presence.get('fast',0) > 0) & (presence.get('slow',0) > 0)]\n",
    "\n",
    "survivor_df = sub[sub['aaSeqCDR3'].isin(survivors)].copy()  # keeps ALL columns\n",
    "\n",
    "threshold = 4\n",
    "# 2) qualify: AA must appear in  threshold UNIQUE samples in BOTH groups\n",
    "counts = (survivor_df.groupby(['aaSeqCDR3','group'])['Sample_ID']\n",
    "          .nunique().unstack(fill_value=0))\n",
    "qualified_tcrs = counts[(counts.get('fast',0) >= threshold) &\n",
    "                        (counts.get('slow',0) >= threshold)].index\n",
    "\n",
    "# === keep these objects with ALL columns ===\n",
    "qualified_df = sub[sub['aaSeqCDR3'].isin(qualified_tcrs)].copy()\n",
    "\n",
    "fast_df = qualified_df[qualified_df['group'] == 'fast'].copy()\n",
    "slow_df = qualified_df[qualified_df['group'] == 'slow'].copy()\n",
    "\n",
    "# Expression matrices (needs 'readFraction' present in sub)\n",
    "fast_expr = fast_df.pivot_table(index='aaSeqCDR3', columns='Sample_ID',\n",
    "                                values='readFraction', fill_value=0)\n",
    "slow_expr = slow_df.pivot_table(index='aaSeqCDR3', columns='Sample_ID',\n",
    "                                values='readFraction', fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c723ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Pearson correlations\n",
    "# fast_corr = fast_expr.T.corr(method='pearson')\n",
    "# slow_corr = slow_expr.T.corr(method='pearson')\n",
    "fast_expr_numpy = fast_expr.to_numpy()\n",
    "fast_corr_numpy = np.corrcoef(fast_expr_numpy)\n",
    "fast_corr = pd.DataFrame(fast_corr_numpy, index=fast_expr.index, columns=fast_expr.index)\n",
    "\n",
    "slow_expr_numpy = slow_expr.to_numpy()\n",
    "slow_corr_numpy = np.corrcoef(slow_expr_numpy)\n",
    "slow_corr = pd.DataFrame(slow_corr_numpy, index=slow_expr.index, columns=slow_expr.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa664352",
   "metadata": {},
   "source": [
    "## Overlap Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4961da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def intersection_graph_with_weights(G1, G2, attr1='weight', attr2='weight', new_attr1='fast_weight', new_attr2='slow_weight'):\n",
    "    \"\"\"\n",
    "    Returns a new graph with only edges present in both G1 and G2.\n",
    "    Edge attributes from G1 and G2 are preserved as new_attr1 and new_attr2.\n",
    "    Only nodes with at least one shared edge will be included.\n",
    "    \"\"\"\n",
    "    G_inter = nx.Graph()\n",
    "    # Create sets of edges as (nodeA, nodeB) sorted tuples for undirected comparison\n",
    "    edges1 = set(tuple(sorted(e)) for e in G1.edges())\n",
    "    edges2 = set(tuple(sorted(e)) for e in G2.edges())\n",
    "    shared_edges = edges1 & edges2\n",
    "    \n",
    "    for u, v in shared_edges:\n",
    "        # Copy correlation weights from both graphs\n",
    "        w1 = G1[u][v][attr1]\n",
    "        w2 = G2[u][v][attr2]\n",
    "        G_inter.add_edge(u, v, **{new_attr1: w1, new_attr2: w2})\n",
    "    return G_inter\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "def largest_component_size(G):\n",
    "    if G.number_of_nodes() == 0:\n",
    "        return 0\n",
    "    return max(len(c) for c in nx.connected_components(G))\n",
    "\n",
    "def tune_overlap_graph(\n",
    "    fast_corr, slow_corr, \n",
    "    target_size=80, \n",
    "    init_min_r_fast=None, \n",
    "    init_min_r_slow=None,\n",
    "    step=0.01, \n",
    "    max_iter=30, \n",
    "    tol=5, \n",
    "    verbose=True\n",
    "):\n",
    "    # Estimate good starting thresholds if not given\n",
    "    if init_min_r_fast is None:\n",
    "        fast_vals = fast_corr.values[np.triu_indices_from(fast_corr, k=1)]\n",
    "        init_min_r_fast = np.percentile(fast_vals, 99)  # or adjust\n",
    "    if init_min_r_slow is None:\n",
    "        slow_vals = slow_corr.values[np.triu_indices_from(slow_corr, k=1)]\n",
    "        init_min_r_slow = np.percentile(slow_vals, 99)\n",
    "        \n",
    "    min_r_fast = init_min_r_fast\n",
    "    min_r_slow = init_min_r_slow\n",
    "    best_diff = float('inf')\n",
    "    best_result = None\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        # Build graphs\n",
    "        G_fast = build_pos_corr_graph_faster(fast_corr, min_r_fast)\n",
    "        G_slow = build_pos_corr_graph_faster(slow_corr, min_r_slow)\n",
    "        # Intersect\n",
    "        G_overlap = intersection_graph_with_weights(G_fast, G_slow)\n",
    "        # Largest component size\n",
    "        lcc_size = largest_component_size(G_overlap)\n",
    "        diff = abs(lcc_size - target_size)\n",
    "        if verbose:\n",
    "            print(f\"Iter {i}: min_r_fast={min_r_fast:.4f}, min_r_slow={min_r_slow:.4f}, \"\n",
    "                  f\"LCC size={lcc_size}, nodes={G_overlap.number_of_nodes()}, edges={G_overlap.number_of_edges()}\")\n",
    "        # Save best result so far\n",
    "        if diff < best_diff:\n",
    "            best_diff = diff\n",
    "            best_result = (min_r_fast, min_r_slow, G_fast, G_slow, G_overlap, lcc_size)\n",
    "        # Converged?\n",
    "        if diff <= tol:\n",
    "            break\n",
    "        # Adjust thresholds:\n",
    "        if lcc_size > target_size:\n",
    "            # Too big: increase thresholds\n",
    "            min_r_fast += step\n",
    "            min_r_slow += step\n",
    "        else:\n",
    "            # Too small: decrease thresholds\n",
    "            min_r_fast -= step\n",
    "            min_r_slow -= step\n",
    "    # Return best result\n",
    "    return {\n",
    "        \"min_r_fast\": best_result[0],\n",
    "        \"min_r_slow\": best_result[1],\n",
    "        \"G_fast\": best_result[2],\n",
    "        \"G_slow\": best_result[3],\n",
    "        \"G_overlap\": best_result[4],\n",
    "        \"lcc_size\": best_result[5]\n",
    "    }\n",
    "\n",
    "def build_pos_corr_graph_faster(corr_matrix, min_r=None):\n",
    "    \"\"\"\n",
    "    Efficiently build a NetworkX graph from a correlation matrix using a threshold.\n",
    "    Precompute the edge list directly from the correlation matrix (using NumPy or pandas).\n",
    "    Only adds edges where r >= min_r (positive correlation).\n",
    "    \"\"\"\n",
    "    tcrs = np.array(corr_matrix.index)\n",
    "    # Get upper triangle indices (excluding diagonal)\n",
    "    mask = np.triu(np.ones(corr_matrix.shape, dtype=bool), k=1)\n",
    "    # Get pairs and correlations\n",
    "    i_idx, j_idx = np.where(mask)\n",
    "    corrs = corr_matrix.values[mask]\n",
    "    if min_r is not None:\n",
    "        select = np.where(corrs >= min_r)[0]\n",
    "        i_idx = i_idx[select]\n",
    "        j_idx = j_idx[select]\n",
    "        corrs = corrs[select]\n",
    "    # Build edge list\n",
    "    edge_list = [\n",
    "        (tcrs[i], tcrs[j], {'weight': float(corr)})\n",
    "        for i, j, corr in zip(i_idx, j_idx, corrs)\n",
    "    ]\n",
    "    # Create graph\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(edge_list)\n",
    "    return G\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save function\n",
    "def save_graph(graph, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(graph, f)\n",
    "\n",
    "# Load function\n",
    "def load_graph(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1993febb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune_results = tune_overlap_graph(\n",
    "#     fast_corr, slow_corr,\n",
    "#     init_min_r_fast= 0.860, \n",
    "#     init_min_r_slow= 0.880,\n",
    "#     target_size=10, # LCC target size\n",
    "#     step=0.02,    # Try 0.01 or smaller for fine control\n",
    "#     max_iter=40,\n",
    "#     tol=5,         # Acceptable range\n",
    "#     verbose=True\n",
    "# )\n",
    "# G_overlap = tune_results[\"G_overlap\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86e097d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_fast_sub = build_pos_corr_graph_faster(fast_corr, min_r=0.75)\n",
    "G_slow_sub = build_pos_corr_graph_faster(slow_corr, min_r=0.77)\n",
    "\n",
    "G_overlap = intersection_graph_with_weights(G_fast_sub, G_slow_sub)\n",
    "\n",
    "# # Get all connected components sorted by size (largest to smallest)\n",
    "# components = sorted(nx.connected_components(G_overlap), key=len, reverse=True)\n",
    "# # Extract the largest and second-largest and Create subgraphs\n",
    "# G_overlap_lcc_largest = G_overlap.subgraph(components[0]).copy()\n",
    "G_overlap_lcc_largest = G_overlap.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c810e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def create_node_meta_df_for_gnn(G_overlap):\n",
    "    # This function creates static (predetermined) information about nodes (TCRs) that will be later used\n",
    "    # as initial node embedding input in GNN algorithm.\n",
    "    \n",
    "    def aggregate_edge_weights(G, weight_key):\n",
    "        \"\"\"Return a dict mapping node -> sum of edge weights for that key.\"\"\"\n",
    "        return {\n",
    "            node: sum(G[node][nbr].get(weight_key, 0) for nbr in G.neighbors(node))\n",
    "            for node in G.nodes\n",
    "        }\n",
    "\n",
    "    sum_fast = aggregate_edge_weights(G_overlap, 'fast_weight')\n",
    "    avg_fast = {k: v / G_overlap.degree[k] if G_overlap.degree[k] else 0 for k, v in sum_fast.items()}\n",
    "    sum_slow = aggregate_edge_weights(G_overlap, 'slow_weight')\n",
    "    avg_slow = {k: v / G_overlap.degree[k] if G_overlap.degree[k] else 0 for k, v in sum_slow.items()}\n",
    "\n",
    "    overlap_node_meta_df = pd.DataFrame({'id': list(G_overlap.nodes())})\n",
    "    overlap_node_meta_df['sum_fast_weight'] = overlap_node_meta_df['id'].map(sum_fast)\n",
    "    overlap_node_meta_df['avg_fast_weight'] = overlap_node_meta_df['id'].map(avg_fast)\n",
    "    overlap_node_meta_df['sum_slow_weight'] = overlap_node_meta_df['id'].map(sum_slow)\n",
    "    overlap_node_meta_df['avg_slow_weight'] = overlap_node_meta_df['id'].map(avg_slow)\n",
    "\n",
    "    # Define function to clean allele info from V/J genes\n",
    "    def clean_vj(gene_str):\n",
    "        return re.sub(r\"\\*.*\", \"\", gene_str) if isinstance(gene_str, str) else gene_str\n",
    "\n",
    "    # Generate per-group metadata from fast_df\n",
    "    vj_map_fast = (\n",
    "        fast_df.groupby(\"aaSeqCDR3\")\n",
    "        .agg({\n",
    "            \"allVHitsWithScore\": lambda x: x.mode()[0] if not x.mode().empty else \"\",\n",
    "            \"allJHitsWithScore\": lambda x: x.mode()[0] if not x.mode().empty else \"\",\n",
    "            \"nSeqCDR3\": pd.Series.nunique,\n",
    "            \"Sample_ID\": pd.Series.nunique\n",
    "        })\n",
    "        .rename(columns={\n",
    "            \"allVHitsWithScore\": \"V_gene_fast\",\n",
    "            \"allJHitsWithScore\": \"J_gene_fast\",\n",
    "            \"nSeqCDR3\": \"codon_diversity_fast\",\n",
    "            \"Sample_ID\": \"sample_prevalence_fast\"\n",
    "        })\n",
    "    )\n",
    "\n",
    "    # Generate per-group metadata from slow_df\n",
    "    vj_map_slow = (\n",
    "        slow_df.groupby(\"aaSeqCDR3\")\n",
    "        .agg({\n",
    "            \"allVHitsWithScore\": lambda x: x.mode()[0] if not x.mode().empty else \"\",\n",
    "            \"allJHitsWithScore\": lambda x: x.mode()[0] if not x.mode().empty else \"\",\n",
    "            \"nSeqCDR3\": pd.Series.nunique,\n",
    "            \"Sample_ID\": pd.Series.nunique\n",
    "        })\n",
    "        .rename(columns={\n",
    "            \"allVHitsWithScore\": \"V_gene_slow\",\n",
    "            \"allJHitsWithScore\": \"J_gene_slow\",\n",
    "            \"nSeqCDR3\": \"codon_diversity_slow\",\n",
    "            \"Sample_ID\": \"sample_prevalence_slow\"\n",
    "        })\n",
    "    )\n",
    "\n",
    "    # Clean allele info\n",
    "    vj_map_fast[\"V_gene_fast\"] = vj_map_fast[\"V_gene_fast\"].apply(clean_vj)\n",
    "    vj_map_fast[\"J_gene_fast\"] = vj_map_fast[\"J_gene_fast\"].apply(clean_vj)\n",
    "    vj_map_slow[\"V_gene_slow\"] = vj_map_slow[\"V_gene_slow\"].apply(clean_vj)\n",
    "    vj_map_slow[\"J_gene_slow\"] = vj_map_slow[\"J_gene_slow\"].apply(clean_vj)\n",
    "\n",
    "    # Merge both metadata sets into it\n",
    "    overlap_node_meta_df = (\n",
    "        overlap_node_meta_df\n",
    "        .merge(vj_map_fast, left_on='id', right_index=True, how='left')\n",
    "        .merge(vj_map_slow, left_on='id', right_index=True, how='left')\n",
    "    )\n",
    "\n",
    "    # graph level stats\n",
    "    degree_dict = dict(G_overlap.degree())\n",
    "    betweenness = nx.betweenness_centrality(G_overlap)\n",
    "    closeness = nx.closeness_centrality(G_overlap)\n",
    "    eigenvector = nx.eigenvector_centrality(G_overlap, max_iter=500)\n",
    "\n",
    "    # Convert metrics to Series with index as node IDs\n",
    "    degree_series = pd.Series(degree_dict, name='degree')\n",
    "    betweenness_series = pd.Series(betweenness, name='betweenness')\n",
    "    closeness_series = pd.Series(closeness, name='closeness')\n",
    "    eigenvector_series = pd.Series(eigenvector, name='eigenvector')\n",
    "\n",
    "    # Merge all into overlap_node_meta_df\n",
    "    overlap_node_meta_df = (\n",
    "        overlap_node_meta_df\n",
    "        .merge(degree_series, left_on='id', right_index=True, how='left')\n",
    "        .merge(betweenness_series, left_on='id', right_index=True, how='left')\n",
    "        .merge(closeness_series, left_on='id', right_index=True, how='left')\n",
    "        .merge(eigenvector_series, left_on='id', right_index=True, how='left')\n",
    "    )\n",
    "\n",
    "    # Step 1: Combine all V genes to create a unified vocabulary\n",
    "    all_v_genes = pd.concat([\n",
    "        overlap_node_meta_df['V_gene_fast'],\n",
    "        overlap_node_meta_df['V_gene_slow']\n",
    "    ]).dropna().unique()\n",
    "    # Step 2: Create mapping from V gene name to integer ID\n",
    "    unique_v_genes = sorted(set(all_v_genes))\n",
    "    v_gene_to_id = {v: i for i, v in enumerate(unique_v_genes)}\n",
    "    # Step 3: Map V_gene columns to integer IDs\n",
    "    overlap_node_meta_df['V_gene_fast_id'] = overlap_node_meta_df['V_gene_fast'].map(v_gene_to_id)\n",
    "    overlap_node_meta_df['V_gene_slow_id'] = overlap_node_meta_df['V_gene_slow'].map(v_gene_to_id)\n",
    "    # Step 1: Combine all J genes to create a unified vocabulary\n",
    "    all_j_genes = pd.concat([\n",
    "        overlap_node_meta_df['J_gene_fast'],\n",
    "        overlap_node_meta_df['J_gene_slow']\n",
    "    ]).dropna().unique()\n",
    "    unique_j_genes = sorted(set(all_j_genes))\n",
    "    j_gene_to_id = {j: i for i, j in enumerate(unique_j_genes)}\n",
    "    overlap_node_meta_df['J_gene_fast_id'] = overlap_node_meta_df['J_gene_fast'].map(j_gene_to_id)\n",
    "    overlap_node_meta_df['J_gene_slow_id'] = overlap_node_meta_df['J_gene_slow'].map(j_gene_to_id)\n",
    "\n",
    "    return overlap_node_meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b4e115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_node_meta_df = create_node_meta_df_for_gnn(G_overlap_lcc_largest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c42bf0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "\n",
    "def add_tcr_biochem_features(df: pd.DataFrame, seq_col: str = \"id\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add biochemical features for AA sequences in `seq_col` to the same DataFrame.\n",
    "    Returns a copy with new columns:\n",
    "      ['length','aromaticity','instability_index','isoelectric_point','gravy',\n",
    "       'charge_7.0','molecular_weight','helix_frac','turn_frac','sheet_frac']\n",
    "    \"\"\"\n",
    "    if seq_col not in df.columns:\n",
    "        raise KeyError(f\"Column '{seq_col}' not found in df\")\n",
    "\n",
    "    # Define which columns well produce\n",
    "    biochem_cols = [\n",
    "        \"length\", \"aromaticity\", \"instability_index\", \"isoelectric_point\",\n",
    "        \"gravy\", \"charge_7.0\", \"molecular_weight\", \"helix_frac\", \"turn_frac\", \"sheet_frac\"\n",
    "    ]\n",
    "\n",
    "    # Helper for one sequence\n",
    "    def analyze_sequence(seq: str):\n",
    "        try:\n",
    "            if not isinstance(seq, str) or not seq:\n",
    "                raise ValueError(\"empty\")\n",
    "            s = seq.upper()\n",
    "            # Optionally reject non-standard residues to avoid ProtParam errors\n",
    "            if any(c not in \"ACDEFGHIKLMNPQRSTVWY\" for c in s):\n",
    "                raise ValueError(\"non-standard AA\")\n",
    "            prot = ProteinAnalysis(s)\n",
    "            ss_h, ss_t, ss_s = prot.secondary_structure_fraction()  # helix, turn, sheet\n",
    "            return {\n",
    "                \"length\": len(s),\n",
    "                \"aromaticity\": prot.aromaticity(),\n",
    "                \"instability_index\": prot.instability_index(),\n",
    "                \"isoelectric_point\": prot.isoelectric_point(),\n",
    "                \"gravy\": prot.gravy(),\n",
    "                \"charge_7.0\": prot.charge_at_pH(7.0),\n",
    "                \"molecular_weight\": prot.molecular_weight(),\n",
    "                \"helix_frac\": ss_h,\n",
    "                \"turn_frac\": ss_t,\n",
    "                \"sheet_frac\": ss_s,\n",
    "            }\n",
    "        except Exception:\n",
    "            # Fill NaNs on any issue; keeps shapes consistent\n",
    "            return {k: np.nan for k in biochem_cols}\n",
    "\n",
    "    # mapping\n",
    "    uniq = df[seq_col].astype(str).unique()\n",
    "    feat_map = {seq: analyze_sequence(seq) for seq in uniq}\n",
    "\n",
    "    feats_df = pd.DataFrame.from_dict(feat_map, orient=\"index\")\n",
    "    feats_df.index.name = seq_col\n",
    "\n",
    "    # -- NaN handling --\n",
    "    nans_before = feats_df.isna().sum().sum()\n",
    "    if nans_before > 0:\n",
    "        print(f\"Imputing NaNs in biochemical features... ({nans_before} missing values detected)\")\n",
    "        # Calculate the mean for each feature column (ignoring NaNs)\n",
    "        column_means = feats_df.mean()\n",
    "        feats_df = feats_df.fillna(column_means)\n",
    "        # Check for any columns that are still all-NaN (entire column NaN)\n",
    "        for col in feats_df.columns:\n",
    "            if feats_df[col].isna().all():\n",
    "                print(f\"Column '{col}' was all NaN! Filling with zeros.\")\n",
    "        # Fill any remaining NaNs with 0\n",
    "        feats_df = feats_df.fillna(0)\n",
    "\n",
    "    # Merge back\n",
    "    out = df.merge(feats_df, left_on=seq_col, right_index=True, how=\"left\")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d060a9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing NaNs in biochemical features... (10 missing values detected)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sum_fast_weight",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_fast_weight",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sum_slow_weight",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_slow_weight",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "V_gene_fast",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "J_gene_fast",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "codon_diversity_fast",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sample_prevalence_fast",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "V_gene_slow",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "J_gene_slow",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "codon_diversity_slow",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sample_prevalence_slow",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "degree",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "betweenness",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "closeness",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "eigenvector",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "V_gene_fast_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "V_gene_slow_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "J_gene_fast_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "J_gene_slow_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "length",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "aromaticity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "instability_index",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "isoelectric_point",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "gravy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "charge_7.0",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "molecular_weight",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "helix_frac",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "turn_frac",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sheet_frac",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "6236517e-0d98-4c11-8adc-618f6795cdfe",
       "rows": [
        [
         "0",
         "CASSLTAGANVLTF",
         "26.469125324158767",
         "0.8823041774719589",
         "28.58337654072659",
         "0.9527792180242197",
         "TRBV11-2",
         "TRBJ2-6",
         "24",
         "34",
         "TRBV11-2",
         "TRBJ2-6",
         "8",
         "6",
         "30",
         "1.994119281645249e-06",
         "0.012815378454144974",
         "0.002224354300747186",
         "4",
         "4",
         "11",
         "11",
         "14.0",
         "0.07142857142857144",
         "25.792857142857144",
         "5.518122673034666",
         "1.1142857142857143",
         "-0.24979937590970713",
         "1354.5282",
         "0.35714285714285715",
         "0.28571428571428575",
         "0.4285714285714286"
        ],
        [
         "1",
         "CASSPGLAGDTGELFF",
         "24.465926830844335",
         "0.8737831011015834",
         "27.117365152213903",
         "0.9684773268647823",
         "TRBV18",
         "TRBJ2-2",
         "16",
         "19",
         "TRBV10-1",
         "TRBJ2-2",
         "7",
         "7",
         "28",
         "5.500787406884089e-07",
         "0.012322479282831707",
         "0.00216363315037493",
         "14",
         "0",
         "7",
         "7",
         "16.0",
         "0.125",
         "29.4625",
         "4.0500284194946286",
         "0.4499999999999999",
         "-2.2458681529966524",
         "1571.7058000000002",
         "0.3125",
         "0.4375",
         "0.3125"
        ],
        [
         "2",
         "CASSLEDTQYF",
         "1.6082717558723039",
         "0.8041358779361519",
         "1.5728335451283963",
         "0.7864167725641982",
         "TRBV12-3",
         "TRBJ2-3",
         "33",
         "105",
         "TRBV11-1",
         "TRBJ2-3",
         "16",
         "28",
         "2",
         "0.0",
         "0.0008009611533840609",
         "2.798638306202253e-126",
         "7",
         "3",
         "8",
         "8",
         "11.0",
         "0.18181818181818182",
         "16.736363636363638",
         "4.0500284194946286",
         "-0.29090909090909095",
         "-2.2468671539956535",
         "1263.3298",
         "0.2727272727272727",
         "0.2727272727272727",
         "0.36363636363636365"
        ],
        [
         "3",
         "CASSLGGPNEQYF",
         "1.717529484750234",
         "0.858764742375117",
         "1.6418747289446034",
         "0.8209373644723017",
         "TRBV12-3",
         "TRBJ2-7",
         "9",
         "9",
         "TRBV12-4",
         "TRBJ2-7",
         "4",
         "4",
         "2",
         "0.0",
         "0.0008009611533840609",
         "2.798638306202253e-126",
         "7",
         "8",
         "12",
         "12",
         "13.0",
         "0.15384615384615385",
         "42.55384615384616",
         "4.0500284194946286",
         "-0.37692307692307697",
         "-1.2479879149354982",
         "1372.4589",
         "0.23076923076923078",
         "0.46153846153846156",
         "0.23076923076923078"
        ],
        [
         "4",
         "CASSFQNYGYTF",
         "0.8871296016044999",
         "0.8871296016044999",
         "0.87245800442912",
         "0.87245800442912",
         "TRBV11-2",
         "TRBJ1-2",
         "10",
         "25",
         "TRBV12-3",
         "TRBJ1-2",
         "5",
         "5",
         "1",
         "0.0",
         "0.00040048057669203043",
         "2.7260849945981908e-148",
         "4",
         "7",
         "1",
         "1",
         "12.0",
         "0.33333333333333337",
         "14.275000000000002",
         "5.516758537292478",
         "-0.20000000000000004",
         "-0.25179737790770906",
         "1387.4717",
         "0.08333333333333334",
         "0.33333333333333337",
         "0.41666666666666674"
        ],
        [
         "5",
         "CASSVAGSGNTIYF",
         "0.8871296016044999",
         "0.8871296016044999",
         "0.87245800442912",
         "0.87245800442912",
         "TRBV12-3",
         "TRBJ1-3",
         "13",
         "15",
         "TRBV9",
         "TRBJ1-3",
         "4",
         "4",
         "1",
         "0.0",
         "0.00040048057669203043",
         "2.7260849945981908e-148",
         "7",
         "45",
         "2",
         "2",
         "14.0",
         "0.14285714285714288",
         "10.914285714285716",
         "5.517440605163573",
         "0.6357142857142858",
         "-0.2507983769087082",
         "1376.4905999999999",
         "0.14285714285714288",
         "0.4285714285714286",
         "0.3571428571428572"
        ],
        [
         "6",
         "CASSLTGVGEKLFF",
         "0.8361388228175451",
         "0.8361388228175451",
         "0.7706029567809185",
         "0.7706029567809185",
         "TRBV12-3",
         "TRBJ1-4",
         "14",
         "14",
         "TRBV4-1",
         "TRBJ1-4",
         "4",
         "4",
         "1",
         "0.0",
         "0.00040048057669203043",
         "2.7260849945981908e-148",
         "7",
         "26",
         "3",
         "3",
         "14.0",
         "0.14285714285714288",
         "-0.5357142857142838",
         "5.994262886047363",
         "0.7999999999999999",
         "-0.24798791493549777",
         "1458.6773",
         "0.3571428571428572",
         "0.28571428571428575",
         "0.42857142857142866"
        ],
        [
         "7",
         "CASSSGINTEAFF",
         "0.8361388228175451",
         "0.8361388228175451",
         "0.7706029567809185",
         "0.7706029567809185",
         "TRBV12-3",
         "TRBJ1-1",
         "4",
         "4",
         "TRBV12-3",
         "TRBJ1-1",
         "4",
         "4",
         "1",
         "0.0",
         "0.00040048057669203043",
         "2.7260849945981908e-148",
         "7",
         "7",
         "0",
         "0",
         "13.0",
         "0.15384615384615385",
         "40.61538461538461",
         "4.0500284194946286",
         "0.4384615384615384",
         "-1.2469889139364971",
         "1333.4229",
         "0.23076923076923078",
         "0.3846153846153846",
         "0.3076923076923077"
        ],
        [
         "8",
         "CASSLGAANNEQFF",
         "1.7448978927154815",
         "0.8724489463577407",
         "1.5979094287559814",
         "0.7989547143779907",
         "TRBV18",
         "TRBJ2-1",
         "6",
         "6",
         "TRBV5-1",
         "TRBJ2-1",
         "4",
         "4",
         "2",
         "3.20897897990409e-07",
         "0.0008009611533840609",
         "5.438714870904662e-138",
         "14",
         "29",
         "6",
         "6",
         "14.0",
         "0.14285714285714288",
         "25.35",
         "4.0500284194946286",
         "0.09285714285714278",
         "-1.2469889139364971",
         "1458.5514",
         "0.35714285714285715",
         "0.3571428571428572",
         "0.2142857142857143"
        ],
        [
         "9",
         "CASSRLASTDTQYF",
         "0.9526465762958833",
         "0.9526465762958833",
         "0.7782150689458663",
         "0.7782150689458663",
         "TRBV10-1",
         "TRBJ2-3",
         "17",
         "23",
         "TRBV18",
         "TRBJ2-3",
         "6",
         "5",
         "1",
         "0.0",
         "0.0005339741022560405",
         "3.845752166156805e-138",
         "0",
         "14",
         "8",
         "8",
         "14.0",
         "0.14285714285714288",
         "15.292857142857146",
         "5.828690910339356",
         "-0.2785714285714286",
         "-0.24968761586886457",
         "1549.6606000000002",
         "0.2142857142857143",
         "0.2857142857142857",
         "0.3571428571428572"
        ],
        [
         "10",
         "CASSLGGGNTGELFF",
         "14.910735221778216",
         "0.7847755379883271",
         "17.715087492687097",
         "0.9323730259308999",
         "TRBV7-2",
         "TRBJ2-2",
         "56",
         "57",
         "TRBV11-2",
         "TRBJ2-2",
         "11",
         "8",
         "19",
         "3.817425360929387e-07",
         "0.011451579733518059",
         "0.12049444162254157",
         "39",
         "4",
         "7",
         "7",
         "15.0",
         "0.13333333333333333",
         "16.619999999999994",
         "4.0500284194946286",
         "0.43999999999999984",
         "-1.2469889139364971",
         "1459.5792000000001",
         "0.26666666666666666",
         "0.4666666666666667",
         "0.3333333333333333"
        ],
        [
         "11",
         "CASSLLAGNTDTQYF",
         "23.39226612168928",
         "0.835438075774617",
         "25.27857701576936",
         "0.9028063219917629",
         "TRBV12-4",
         "TRBJ2-3",
         "14",
         "20",
         "TRBV12-4",
         "TRBJ2-3",
         "6",
         "7",
         "28",
         "1.0231133323042543e-06",
         "0.013037183081235944",
         "0.1676524286367892",
         "8",
         "8",
         "8",
         "8",
         "15.0",
         "0.13333333333333333",
         "-9.219999999999995",
         "4.0500284194946286",
         "0.0866666666666667",
         "-1.2496776159688636",
         "1590.7091",
         "0.26666666666666666",
         "0.3333333333333333",
         "0.4"
        ],
        [
         "12",
         "CASSFGGSTEAFF",
         "4.040680801075119",
         "0.8081361602150239",
         "4.360044633587228",
         "0.8720089267174457",
         "TRBV12-4",
         "TRBJ1-1",
         "34",
         "41",
         "TRBV12-3",
         "TRBJ1-1",
         "6",
         "6",
         "5",
         "4.492570571865726e-06",
         "0.0018637749915282956",
         "1.5386748723909682e-104",
         "8",
         "7",
         "0",
         "0",
         "13.0",
         "0.23076923076923075",
         "48.353846153846156",
         "4.0500284194946286",
         "0.5461538461538461",
         "-1.2469889139364971",
         "1310.3878",
         "0.23076923076923078",
         "0.3846153846153846",
         "0.3076923076923077"
        ],
        [
         "13",
         "CASSPRGIGNQPQHF",
         "3.4590976192780944",
         "0.8647744048195236",
         "3.3441517763286015",
         "0.8360379440821504",
         "TRBV12-4",
         "TRBJ1-5",
         "5",
         "5",
         "TRBV12-3",
         "TRBJ1-5",
         "4",
         "4",
         "4",
         "9.62693693971227e-06",
         "0.0022026431718061676",
         "1.3464000994997739e-104",
         "8",
         "7",
         "4",
         "4",
         "15.0",
         "0.06666666666666667",
         "43.199999999999996",
         "8.258609580993653",
         "-0.8133333333333332",
         "0.8373648124448787",
         "1598.7411000000002",
         "0.06666666666666667",
         "0.4666666666666667",
         "0.13333333333333333"
        ],
        [
         "14",
         "CASSFRTGVSYEQYF",
         "2.5201446279451907",
         "0.8400482093150635",
         "2.7607246437765958",
         "0.9202415479255319",
         "TRBV11-2",
         "TRBJ2-7",
         "4",
         "4",
         "TRBV12-3",
         "TRBJ2-7",
         "4",
         "4",
         "3",
         "9.62693693971227e-07",
         "0.0012014417300760913",
         "3.186729201526722e-131",
         "4",
         "7",
         "12",
         "12",
         "15.0",
         "0.26666666666666666",
         "19.300000000000004",
         "5.9935808181762695",
         "-0.23333333333333328",
         "-0.24899691583450023",
         "1744.8771000000002",
         "0.13333333333333333",
         "0.26666666666666666",
         "0.4"
        ],
        [
         "15",
         "CASSFTSGGNEQFF",
         "0.9143076333758205",
         "0.9143076333758205",
         "0.8976890981831636",
         "0.8976890981831636",
         "TRBV12-3",
         "TRBJ2-1",
         "5",
         "6",
         "TRBV11-3",
         "TRBJ2-1",
         "4",
         "4",
         "1",
         "0.0",
         "0.0007208650380456547",
         "1.8398589623358941e-131",
         "7",
         "5",
         "6",
         "6",
         "14.0",
         "0.21428571428571427",
         "34.16428571428572",
         "4.0500284194946286",
         "-0.12142857142857144",
         "-1.2469889139364971",
         "1481.5417",
         "0.14285714285714288",
         "0.4285714285714286",
         "0.2857142857142857"
        ],
        [
         "16",
         "CASSLRLAGGSYNEQFF",
         "0.7629459329846987",
         "0.7629459329846987",
         "0.8773614841770251",
         "0.8773614841770251",
         "TRBV12-3",
         "TRBJ2-1",
         "11",
         "11",
         "TRBV12-3",
         "TRBJ2-1",
         "4",
         "4",
         "1",
         "0.0",
         "0.0006007208650380457",
         "8.1884675977636225e-134",
         "7",
         "7",
         "6",
         "6",
         "17.0",
         "0.17647058823529413",
         "46.22352941176471",
         "5.994262886047363",
         "-0.011764705882353003",
         "-0.24799791483549916",
         "1850.016",
         "0.29411764705882354",
         "0.35294117647058826",
         "0.29411764705882354"
        ],
        [
         "17",
         "CASSSSGGANEQFF",
         "1.6047741985183275",
         "0.8023870992591637",
         "1.7152029015496606",
         "0.8576014507748303",
         "TRBV12-3",
         "TRBJ2-1",
         "12",
         "16",
         "TRBV13",
         "TRBJ2-1",
         "5",
         "5",
         "2",
         "6.41795795980818e-07",
         "0.0009010812975570685",
         "1.3249218888958744e-133",
         "7",
         "10",
         "6",
         "6",
         "14.0",
         "0.14285714285714288",
         "61.67857142857144",
         "4.0500284194946286",
         "-0.20000000000000004",
         "-1.2469889139364971",
         "1391.4192",
         "0.2142857142857143",
         "0.5",
         "0.14285714285714288"
        ],
        [
         "18",
         "CASSPDPSTDTQYF",
         "24.892559604625852",
         "0.8583641242974431",
         "26.89794350348526",
         "0.9275152932236296",
         "TRBV11-1",
         "TRBJ2-3",
         "15",
         "16",
         "TRBV12-3",
         "TRBJ2-3",
         "4",
         "4",
         "29",
         "9.225200825585397e-07",
         "0.013240889066880255",
         "0.17336131884168798",
         "3",
         "7",
         "8",
         "8",
         "14.0",
         "0.14285714285714288",
         "41.292857142857144",
         "4.0500284194946286",
         "-0.8357142857142856",
         "-2.2485568550290194",
         "1518.5572000000002",
         "0.07142857142857144",
         "0.5",
         "0.28571428571428575"
        ],
        [
         "19",
         "CASSVDYTDTQYF",
         "16.00606584255842",
         "0.8003032921279211",
         "18.77957330623873",
         "0.9389786653119365",
         "TRBV27",
         "TRBJ2-3",
         "2",
         "4",
         "TRBV19",
         "TRBJ2-3",
         "6",
         "6",
         "20",
         "2.544155046388496e-07",
         "0.01160845068877173",
         "0.12850446880921773",
         "21",
         "15",
         "8",
         "8",
         "13.0",
         "0.23076923076923078",
         "-17.207692307692305",
         "4.0500284194946286",
         "-0.3692307692307692",
         "-2.2495558560280204",
         "1499.5537",
         "0.07692307692307693",
         "0.3076923076923077",
         "0.46153846153846156"
        ],
        [
         "20",
         "CASRASGTDTQYF",
         "0.9982270516382795",
         "0.9982270516382795",
         "0.8423396769420844",
         "0.8423396769420844",
         "TRBV11-2",
         "TRBJ2-3",
         "12",
         "21",
         "TRBV12-4",
         "TRBJ2-3",
         "6",
         "6",
         "1",
         "0.0",
         "0.00040048057669203043",
         "2.7260849945981908e-148",
         "4",
         "8",
         "8",
         "8",
         "13.0",
         "0.15384615384615385",
         "-5.646153846153844",
         "5.828690910339356",
         "-0.5615384615384617",
         "-0.24968761586886457",
         "1406.4769999999999",
         "0.15384615384615385",
         "0.3076923076923077",
         "0.3076923076923077"
        ],
        [
         "21",
         "CASSLDYTGELFF",
         "0.9982270516382795",
         "0.9982270516382795",
         "0.8423396769420844",
         "0.8423396769420844",
         "TRBV5-1",
         "TRBJ2-2",
         "23",
         "40",
         "TRBV11-3",
         "TRBJ2-2",
         "7",
         "9",
         "1",
         "0.0",
         "0.00040048057669203043",
         "2.7260849945981908e-148",
         "29",
         "5",
         "7",
         "7",
         "13.0",
         "0.23076923076923078",
         "5.1846153846153875",
         "4.0500284194946286",
         "0.5",
         "-2.2468671539956535",
         "1452.5833",
         "0.3076923076923077",
         "0.3076923076923077",
         "0.46153846153846156"
        ],
        [
         "22",
         "CASSDRGRTEAFF",
         "23.127301417086866",
         "0.8565667191513654",
         "24.81048811622184",
         "0.9189069672674756",
         "TRBV12-3",
         "TRBJ1-1",
         "13",
         "15",
         "TRBV10-1",
         "TRBJ1-1",
         "5",
         "5",
         "27",
         "1.2221487670119673e-07",
         "0.011866091161245346",
         "0.0021128553760495533",
         "7",
         "0",
         "0",
         "0",
         "13.0",
         "0.15384615384615385",
         "26.530769230769234",
         "6.063265419006346",
         "-0.5384615384615382",
         "-0.24588815279665432",
         "1446.5442000000003",
         "0.23076923076923078",
         "0.3076923076923077",
         "0.23076923076923078"
        ],
        [
         "23",
         "CASSQDFNEQFF",
         "19.289937073183385",
         "0.8037473780493077",
         "23.347339076334293",
         "0.9728057948472623",
         "TRBV14",
         "TRBJ2-1",
         "5",
         "10",
         "TRBV14",
         "TRBJ2-1",
         "2",
         "5",
         "24",
         "1.2342226845784963e-08",
         "0.011047740046676703",
         "0.0019115542474434804",
         "11",
         "11",
         "6",
         "6",
         "12.0",
         "0.25",
         "60.8",
         "4.0500284194946286",
         "-0.5333333333333333",
         "-2.2458681529966524",
         "1422.4745",
         "0.16666666666666669",
         "0.33333333333333337",
         "0.25"
        ],
        [
         "24",
         "CASSLGGQGSYNEQFF",
         "0.8046356462589785",
         "0.8046356462589785",
         "0.773805354152077",
         "0.773805354152077",
         "TRBV11-3",
         "TRBJ2-1",
         "16",
         "20",
         "TRBV12-4",
         "TRBJ2-1",
         "4",
         "4",
         "1",
         "0.0",
         "0.00040048057669203043",
         "2.7260849945981908e-148",
         "5",
         "8",
         "6",
         "6",
         "16.0",
         "0.1875",
         "36.45",
         "4.0500284194946286",
         "-0.325",
         "-1.2479879149354982",
         "1694.7753000000002",
         "0.1875",
         "0.4375",
         "0.25"
        ],
        [
         "25",
         "CASSLTGRNTGELFF",
         "0.8046356462589785",
         "0.8046356462589785",
         "0.773805354152077",
         "0.773805354152077",
         "TRBV11-2",
         "TRBJ2-2",
         "18",
         "17",
         "TRBV12-3",
         "TRBJ2-2",
         "4",
         "4",
         "1",
         "0.0",
         "0.00040048057669203043",
         "2.7260849945981908e-148",
         "4",
         "7",
         "7",
         "7",
         "15.0",
         "0.13333333333333333",
         "8.393333333333334",
         "5.9949449539184565",
         "0.14666666666666656",
         "-0.24699891383649808",
         "1602.7662000000005",
         "0.26666666666666666",
         "0.33333333333333337",
         "0.4"
        ],
        [
         "26",
         "CASSSDSNTGELFF",
         "37.72373876186915",
         "0.8772962502760268",
         "40.51798959264092",
         "0.9422788277358354",
         "TRBV11-2",
         "TRBJ2-2",
         "18",
         "23",
         "TRBV5-1",
         "TRBJ2-2",
         "4",
         "4",
         "43",
         "2.5294743548855345e-05",
         "0.016948338005606727",
         "0.20673928684258702",
         "4",
         "29",
         "7",
         "7",
         "14.0",
         "0.14285714285714288",
         "33.042857142857144",
         "4.0500284194946286",
         "-0.07857142857142861",
         "-2.2458681529966524",
         "1464.5097",
         "0.2142857142857143",
         "0.5",
         "0.28571428571428575"
        ],
        [
         "27",
         "CASSVSNEQFF",
         "32.534200239646715",
         "0.8561631642012294",
         "35.83100182278424",
         "0.9429211005995852",
         "TRBV10-2",
         "TRBJ2-1",
         "17",
         "19",
         "TRBV18",
         "TRBJ2-1",
         "4",
         "4",
         "38",
         "8.15792756592403e-06",
         "0.015407580005097024",
         "0.19974984829752077",
         "1",
         "14",
         "6",
         "6",
         "11.0",
         "0.18181818181818182",
         "37.25454545454546",
         "4.0500284194946286",
         "0.10909090909090911",
         "-1.2469889139364971",
         "1218.2924",
         "0.18181818181818182",
         "0.36363636363636365",
         "0.2727272727272727"
        ],
        [
         "28",
         "CASSFSGGNTEAFF",
         "0.766594783525363",
         "0.766594783525363",
         "0.8172129806320695",
         "0.8172129806320695",
         "TRBV11-2",
         "TRBJ1-1",
         "43",
         "49",
         "TRBV11-2",
         "TRBJ1-1",
         "9",
         "11",
         "1",
         "0.0",
         "0.0014016820184221065",
         "1.149107403631724e-106",
         "4",
         "4",
         "0",
         "0",
         "14.0",
         "0.21428571428571427",
         "33.48571428571428",
         "4.0500284194946286",
         "0.2571428571428571",
         "-1.2469889139364971",
         "1424.4904000000001",
         "0.2142857142857143",
         "0.4285714285714286",
         "0.2857142857142857"
        ],
        [
         "29",
         "CASSLLTGNQPQHF",
         "4.994125476897327",
         "0.8323542461495546",
         "5.034620554161673",
         "0.8391034256936122",
         "TRBV12-3",
         "TRBJ1-5",
         "7",
         "7",
         "TRBV28",
         "TRBJ1-5",
         "4",
         "4",
         "6",
         "4.813468469856135e-06",
         "0.0024529435322386862",
         "3.8333614219086974e-106",
         "7",
         "22",
         "4",
         "4",
         "14.0",
         "0.07142857142857144",
         "25.635714285714293",
         "6.731919288635253",
         "-0.2357142857142857",
         "-0.16262518765512024",
         "1502.6504",
         "0.2142857142857143",
         "0.3571428571428572",
         "0.28571428571428575"
        ],
        [
         "30",
         "CASSEAGGYNEQFF",
         "23.9086215321445",
         "0.8538793404337321",
         "27.396778177572774",
         "0.978456363484742",
         "TRBV2",
         "TRBJ2-1",
         "15",
         "21",
         "TRBV2",
         "TRBJ2-1",
         "6",
         "8",
         "28",
         "1.2637232898327147e-05",
         "0.012089979673721675",
         "0.002115489592847841",
         "16",
         "16",
         "6",
         "6",
         "14.0",
         "0.2142857142857143",
         "47.921428571428564",
         "4.0500284194946286",
         "-0.42857142857142866",
         "-2.245177452962288",
         "1509.5518",
         "0.28571428571428575",
         "0.3571428571428572",
         "0.2142857142857143"
        ],
        [
         "31",
         "CASSLVEQFF",
         "0.839344551239922",
         "0.839344551239922",
         "0.8029683167093904",
         "0.8029683167093904",
         "TRBV11-2",
         "TRBJ2-1",
         "13",
         "25",
         "TRBV11-2",
         "TRBJ2-1",
         "4",
         "5",
         "1",
         "0.0",
         "0.0005339741022560405",
         "3.845752166156805e-138",
         "4",
         "4",
         "6",
         "6",
         "10.0",
         "0.2",
         "39.980000000000004",
         "4.0500284194946286",
         "0.9299999999999999",
         "-1.2469889139364971",
         "1130.2701",
         "0.30000000000000004",
         "0.2",
         "0.4"
        ],
        [
         "32",
         "CASSRLAGGRTDTQYF",
         "1.6626543936262683",
         "0.8313271968131342",
         "1.6321074887729101",
         "0.8160537443864551",
         "TRBV12-3",
         "TRBJ2-3",
         "15",
         "14",
         "TRBV19",
         "TRBJ2-3",
         "5",
         "5",
         "2",
         "3.20897897990409e-07",
         "0.0008009611533840609",
         "5.438714870904662e-138",
         "7",
         "15",
         "8",
         "8",
         "16.0",
         "0.125",
         "22.34375",
         "8.2217981338501",
         "-0.5250000000000001",
         "0.7503023842311345",
         "1732.8716000000002",
         "0.1875",
         "0.3125",
         "0.3125"
        ],
        [
         "33",
         "CASSLTGSETQYF",
         "1.5547005512247223",
         "0.7773502756123611",
         "1.65839780433853",
         "0.829198902169265",
         "TRBV11-2",
         "TRBJ2-5",
         "32",
         "29",
         "TRBV11-2",
         "TRBJ2-5",
         "5",
         "5",
         "2",
         "3.20897897990409e-07",
         "0.0008009611533840609",
         "5.438714870904662e-138",
         "4",
         "4",
         "10",
         "10",
         "13.0",
         "0.15384615384615385",
         "20.730769230769234",
         "4.0500284194946286",
         "-0.12307692307692311",
         "-1.2479879149354982",
         "1393.4749000000002",
         "0.23076923076923078",
         "0.3076923076923077",
         "0.38461538461538464"
        ],
        [
         "34",
         "CASSRPGQGNTEAFF",
         "0.7991638885991532",
         "0.7991638885991532",
         "0.8750621079556475",
         "0.8750621079556475",
         "TRBV12-3",
         "TRBJ1-1",
         "20",
         "20",
         "TRBV12-4",
         "TRBJ1-1",
         "8",
         "6",
         "1",
         "0.0",
         "0.0005339741022560405",
         "3.845752166156805e-138",
         "7",
         "8",
         "0",
         "0",
         "15.0",
         "0.13333333333333333",
         "49.373333333333335",
         "5.9949449539184565",
         "-0.5333333333333331",
         "-0.24699891383649808",
         "1571.6694000000002",
         "0.2",
         "0.4",
         "0.2"
        ],
        [
         "35",
         "CASSEGQGRYNEQFF",
         "1.7146768996769213",
         "0.8573384498384606",
         "1.7397293818418615",
         "0.8698646909209308",
         "TRBV10-2",
         "TRBJ2-1",
         "8",
         "8",
         "TRBV25-1",
         "TRBJ2-1",
         "6",
         "5",
         "2",
         "0.0",
         "0.0008009611533840609",
         "2.798638306202253e-126",
         "1",
         "20",
         "6",
         "6",
         "15.0",
         "0.2",
         "37.800000000000004",
         "4.531340980529785",
         "-1.0533333333333332",
         "-1.245187452862289",
         "1722.7888000000003",
         "0.2",
         "0.33333333333333337",
         "0.2"
        ],
        [
         "36",
         "CASSLAVAGGNEQFF",
         "1.6732878384679115",
         "0.8366439192339558",
         "1.5465261953206715",
         "0.7732630976603357",
         "TRBV5-1",
         "TRBJ2-1",
         "4",
         "4",
         "TRBV5-1",
         "TRBJ2-1",
         "4",
         "4",
         "2",
         "0.0",
         "0.0008009611533840609",
         "2.798638306202253e-126",
         "29",
         "29",
         "6",
         "6",
         "15.0",
         "0.13333333333333333",
         "32.553333333333335",
         "4.0500284194946286",
         "0.5733333333333334",
         "-1.2469889139364971",
         "1500.6311000000005",
         "0.3333333333333333",
         "0.33333333333333337",
         "0.26666666666666666"
        ],
        [
         "37",
         "CASSLGPGSSYEQYF",
         "1.6083024370684136",
         "0.8041512185342068",
         "1.8977301945119622",
         "0.9488650972559811",
         "TRBV11-2",
         "TRBJ2-7",
         "27",
         "26",
         "TRBV11-2",
         "TRBJ2-7",
         "6",
         "4",
         "2",
         "0.0",
         "0.0008009611533840609",
         "2.798638306202253e-126",
         "4",
         "4",
         "12",
         "12",
         "15.0",
         "0.2",
         "37.800000000000004",
         "4.0500284194946286",
         "-0.2866666666666667",
         "-1.2489869159344993",
         "1595.6841",
         "0.2",
         "0.4666666666666667",
         "0.26666666666666666"
        ],
        [
         "38",
         "CASSLVDSGNTIYF",
         "1.611757887481342",
         "0.805878943740671",
         "1.936516022575556",
         "0.968258011287778",
         "TRBV12-4",
         "TRBJ1-3",
         "8",
         "8",
         "TRBV12-4",
         "TRBJ1-3",
         "4",
         "4",
         "2",
         "0.0",
         "0.0008009611533840609",
         "2.798638306202253e-126",
         "8",
         "8",
         "2",
         "2",
         "14.0",
         "0.14285714285714288",
         "13.935714285714285",
         "4.0500284194946286",
         "0.557142857142857",
         "-1.2496776159688636",
         "1476.6064000000001",
         "0.14285714285714288",
         "0.4285714285714286",
         "0.42857142857142866"
        ],
        [
         "39",
         "CASSFPNQPQHF",
         "0.9699787534291302",
         "0.9699787534291302",
         "0.7763775778389154",
         "0.7763775778389154",
         "TRBV12-4",
         "TRBJ1-5",
         "17",
         "51",
         "TRBV12-3",
         "TRBJ1-5",
         "9",
         "11",
         "1",
         "0.0",
         "0.00040048057669203043",
         "2.7260849945981908e-148",
         "8",
         "7",
         "4",
         "4",
         "12.0",
         "0.16666666666666669",
         "58.44166666666668",
         "6.731919288635253",
         "-0.7166666666666665",
         "-0.16262518765512024",
         "1362.469",
         "0.08333333333333334",
         "0.4166666666666667",
         "0.16666666666666669"
        ],
        [
         "40",
         "CASSLVAGSGANVLTF",
         "0.9699787534291302",
         "0.9699787534291302",
         "0.7763775778389154",
         "0.7763775778389154",
         "TRBV10-3",
         "TRBJ2-6",
         "21",
         "19",
         "TRBV11-2",
         "TRBJ2-6",
         "4",
         "4",
         "1",
         "0.0",
         "0.00040048057669203043",
         "2.7260849945981908e-148",
         "2",
         "4",
         "11",
         "11",
         "16.0",
         "0.0625",
         "23.81875",
         "5.518122673034666",
         "1.20625",
         "-0.24979937590970713",
         "1496.6839",
         "0.3125",
         "0.375",
         "0.375"
        ],
        [
         "41",
         "CASSHSNQPQHF",
         "0.8345316706705637",
         "0.8345316706705637",
         "0.8914331267296223",
         "0.8914331267296223",
         "TRBV10-2",
         "TRBJ1-5",
         "9",
         "28",
         "TRBV19",
         "TRBJ1-5",
         "5",
         "5",
         "1",
         "0.0",
         "0.00040048057669203043",
         "2.7260849945981908e-148",
         "1",
         "15",
         "4",
         "4",
         "12.0",
         "0.08333333333333334",
         "42.39166666666668",
         "6.907779121398925",
         "-1.1500000000000001",
         "-0.07545099940053324",
         "1342.3966",
         "0.08333333333333334",
         "0.4166666666666667",
         "0.08333333333333334"
        ],
        [
         "42",
         "CASSYSNQPQHF",
         "0.8345316706705637",
         "0.8345316706705637",
         "0.8914331267296223",
         "0.8914331267296223",
         "TRBV12-4",
         "TRBJ1-5",
         "22",
         "99",
         "TRBV11-2",
         "TRBJ1-5",
         "14",
         "25",
         "1",
         "0.0",
         "0.00040048057669203043",
         "2.7260849945981908e-148",
         "8",
         "4",
         "4",
         "4",
         "12.0",
         "0.16666666666666669",
         "42.39166666666668",
         "6.731009864807128",
         "-0.9916666666666666",
         "-0.16362418865412132",
         "1368.4305",
         "0.08333333333333334",
         "0.4166666666666667",
         "0.16666666666666669"
        ],
        [
         "43",
         "CASSLLQNTGELFF",
         "1.6768404182527201",
         "0.8384202091263601",
         "1.668264069482602",
         "0.834132034741301",
         "TRBV11-2",
         "TRBJ2-2",
         "21",
         "24",
         "TRBV12-3",
         "TRBJ2-2",
         "8",
         "6",
         "2",
         "0.0",
         "0.0009010812975570685",
         "2.7958077512519104e-123",
         "4",
         "7",
         "7",
         "7",
         "14.0",
         "0.14285714285714288",
         "28.814285714285713",
         "4.0500284194946286",
         "0.5785714285714286",
         "-1.2469889139364971",
         "1529.7121",
         "0.35714285714285715",
         "0.28571428571428575",
         "0.4285714285714286"
        ],
        [
         "44",
         "CASSLRQGNTIYF",
         "1.646638236847532",
         "0.823319118423766",
         "1.7648280476156013",
         "0.8824140238078007",
         "TRBV12-4",
         "TRBJ1-3",
         "8",
         "8",
         "TRBV12-4",
         "TRBJ1-3",
         "4",
         "4",
         "2",
         "0.0",
         "0.0009010812975570685",
         "2.7958077512519104e-123",
         "8",
         "8",
         "2",
         "2",
         "13.0",
         "0.15384615384615385",
         "40.61538461538461",
         "8.221991539001465",
         "-0.0076923076923077335",
         "0.7491916231912907",
         "1459.6256",
         "0.15384615384615385",
         "0.3076923076923077",
         "0.38461538461538464"
        ],
        [
         "45",
         "CASSAGQGNSPLHF",
         "0.865420323802899",
         "0.865420323802899",
         "0.9525684938571884",
         "0.9525684938571884",
         "TRBV11-2",
         "TRBJ1-6",
         "19",
         "24",
         "TRBV6-2",
         "TRBJ1-6",
         "5",
         "5",
         "1",
         "0.0",
         "0.001001201441730076",
         "4.4447859117248615e-125",
         "4",
         "35",
         "5",
         "5",
         "14.0",
         "0.07142857142857144",
         "40.95714285714286",
         "6.731919288635253",
         "-0.1642857142857144",
         "-0.16262518765512024",
         "1375.4662",
         "0.2142857142857143",
         "0.5",
         "0.14285714285714288"
        ],
        [
         "46",
         "CASSLNSDEQFF",
         "3.1850494385597594",
         "0.7962623596399399",
         "3.7566115704682495",
         "0.9391528926170624",
         "TRBV11-2",
         "TRBJ2-1",
         "7",
         "8",
         "TRBV11-2",
         "TRBJ2-1",
         "4",
         "4",
         "4",
         "2.888081081913681e-06",
         "0.001668669069550127",
         "9.21987850146088e-125",
         "4",
         "4",
         "6",
         "6",
         "12.0",
         "0.16666666666666669",
         "34.98333333333334",
         "4.0500284194946286",
         "-0.2250000000000001",
         "-2.2458681529966524",
         "1347.4064",
         "0.25",
         "0.41666666666666674",
         "0.25"
        ],
        [
         "47",
         "CASSLVKDTQYF",
         "31.39435576411003",
         "0.8969815932602866",
         "33.99686473449278",
         "0.9713389924140795",
         "TRBV5-1",
         "TRBJ2-3",
         "8",
         "10",
         "TRBV11-3",
         "TRBJ2-3",
         "5",
         "6",
         "35",
         "8.626009173447036e-06",
         "0.014239309393494414",
         "0.002348679289087816",
         "29",
         "5",
         "8",
         "8",
         "12.0",
         "0.16666666666666669",
         "-2.2749999999999977",
         "5.828008842468263",
         "0.0499999999999999",
         "-0.25067661696786425",
         "1361.5191",
         "0.25",
         "0.25",
         "0.41666666666666674"
        ],
        [
         "48",
         "CASSLREGGETQYF",
         "2.4705616128772787",
         "0.8235205376257596",
         "2.606692628593743",
         "0.868897542864581",
         "TRBV19",
         "TRBJ2-5",
         "5",
         "6",
         "TRBV12-3",
         "TRBJ2-5",
         "5",
         "5",
         "3",
         "1.604489489952045e-07",
         "0.0012815378454144974",
         "4.681489150068033e-115",
         "15",
         "7",
         "10",
         "10",
         "14.0",
         "0.14285714285714288",
         "29.45714285714286",
         "4.531340980529785",
         "-0.6071428571428571",
         "-1.245187452862289",
         "1547.6447000000003",
         "0.28571428571428575",
         "0.28571428571428575",
         "0.28571428571428575"
        ],
        [
         "49",
         "CASSRQANTGELFF",
         "3.1532246572375078",
         "0.7883061643093769",
         "3.4159454024484264",
         "0.8539863506121066",
         "TRBV11-2",
         "TRBJ2-2",
         "20",
         "25",
         "TRBV12-3",
         "TRBJ2-2",
         "7",
         "5",
         "4",
         "1.1231426429664315e-06",
         "0.0016019223067681217",
         "5.207635719878542e-115",
         "4",
         "7",
         "7",
         "7",
         "14.0",
         "0.14285714285714288",
         "33.042857142857144",
         "5.9949449539184565",
         "-0.15714285714285717",
         "-0.24699891383649808",
         "1530.6605000000002",
         "0.28571428571428575",
         "0.28571428571428575",
         "0.28571428571428575"
        ]
       ],
       "shape": {
        "columns": 31,
        "rows": 2498
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sum_fast_weight</th>\n",
       "      <th>avg_fast_weight</th>\n",
       "      <th>sum_slow_weight</th>\n",
       "      <th>avg_slow_weight</th>\n",
       "      <th>V_gene_fast</th>\n",
       "      <th>J_gene_fast</th>\n",
       "      <th>codon_diversity_fast</th>\n",
       "      <th>sample_prevalence_fast</th>\n",
       "      <th>V_gene_slow</th>\n",
       "      <th>...</th>\n",
       "      <th>length</th>\n",
       "      <th>aromaticity</th>\n",
       "      <th>instability_index</th>\n",
       "      <th>isoelectric_point</th>\n",
       "      <th>gravy</th>\n",
       "      <th>charge_7.0</th>\n",
       "      <th>molecular_weight</th>\n",
       "      <th>helix_frac</th>\n",
       "      <th>turn_frac</th>\n",
       "      <th>sheet_frac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASSLTAGANVLTF</td>\n",
       "      <td>26.469125</td>\n",
       "      <td>0.882304</td>\n",
       "      <td>28.583377</td>\n",
       "      <td>0.952779</td>\n",
       "      <td>TRBV11-2</td>\n",
       "      <td>TRBJ2-6</td>\n",
       "      <td>24</td>\n",
       "      <td>34</td>\n",
       "      <td>TRBV11-2</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>25.792857</td>\n",
       "      <td>5.518123</td>\n",
       "      <td>1.114286</td>\n",
       "      <td>-0.249799</td>\n",
       "      <td>1354.5282</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CASSPGLAGDTGELFF</td>\n",
       "      <td>24.465927</td>\n",
       "      <td>0.873783</td>\n",
       "      <td>27.117365</td>\n",
       "      <td>0.968477</td>\n",
       "      <td>TRBV18</td>\n",
       "      <td>TRBJ2-2</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>TRBV10-1</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>29.462500</td>\n",
       "      <td>4.050028</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>-2.245868</td>\n",
       "      <td>1571.7058</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CASSLEDTQYF</td>\n",
       "      <td>1.608272</td>\n",
       "      <td>0.804136</td>\n",
       "      <td>1.572834</td>\n",
       "      <td>0.786417</td>\n",
       "      <td>TRBV12-3</td>\n",
       "      <td>TRBJ2-3</td>\n",
       "      <td>33</td>\n",
       "      <td>105</td>\n",
       "      <td>TRBV11-1</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>16.736364</td>\n",
       "      <td>4.050028</td>\n",
       "      <td>-0.290909</td>\n",
       "      <td>-2.246867</td>\n",
       "      <td>1263.3298</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CASSLGGPNEQYF</td>\n",
       "      <td>1.717529</td>\n",
       "      <td>0.858765</td>\n",
       "      <td>1.641875</td>\n",
       "      <td>0.820937</td>\n",
       "      <td>TRBV12-3</td>\n",
       "      <td>TRBJ2-7</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>TRBV12-4</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>42.553846</td>\n",
       "      <td>4.050028</td>\n",
       "      <td>-0.376923</td>\n",
       "      <td>-1.247988</td>\n",
       "      <td>1372.4589</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CASSFQNYGYTF</td>\n",
       "      <td>0.887130</td>\n",
       "      <td>0.887130</td>\n",
       "      <td>0.872458</td>\n",
       "      <td>0.872458</td>\n",
       "      <td>TRBV11-2</td>\n",
       "      <td>TRBJ1-2</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>TRBV12-3</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>14.275000</td>\n",
       "      <td>5.516759</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>-0.251797</td>\n",
       "      <td>1387.4717</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2493</th>\n",
       "      <td>CASRLAGRETQYF</td>\n",
       "      <td>0.864802</td>\n",
       "      <td>0.864802</td>\n",
       "      <td>0.825638</td>\n",
       "      <td>0.825638</td>\n",
       "      <td>TRBV27</td>\n",
       "      <td>TRBJ2-5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>TRBV5-1</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>12.446154</td>\n",
       "      <td>8.221992</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.751992</td>\n",
       "      <td>1501.6657</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2494</th>\n",
       "      <td>CASSLEGSGGTDTQYF</td>\n",
       "      <td>0.864802</td>\n",
       "      <td>0.864802</td>\n",
       "      <td>0.825638</td>\n",
       "      <td>0.825638</td>\n",
       "      <td>TRBV11-2</td>\n",
       "      <td>TRBJ2-3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>TRBV5-1</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.050028</td>\n",
       "      <td>-0.368750</td>\n",
       "      <td>-2.246867</td>\n",
       "      <td>1622.6649</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>CASSLASGYEQYF</td>\n",
       "      <td>0.819376</td>\n",
       "      <td>0.819376</td>\n",
       "      <td>0.948234</td>\n",
       "      <td>0.948234</td>\n",
       "      <td>TRBV11-1</td>\n",
       "      <td>TRBJ2-7</td>\n",
       "      <td>59</td>\n",
       "      <td>60</td>\n",
       "      <td>TRBV12-3</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>20.730769</td>\n",
       "      <td>4.050028</td>\n",
       "      <td>0.023077</td>\n",
       "      <td>-1.248987</td>\n",
       "      <td>1425.5182</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>CASSPGLGNTEAFF</td>\n",
       "      <td>0.819376</td>\n",
       "      <td>0.819376</td>\n",
       "      <td>0.948234</td>\n",
       "      <td>0.948234</td>\n",
       "      <td>TRBV11-2</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "      <td>31</td>\n",
       "      <td>26</td>\n",
       "      <td>TRBV12-3</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>56.057143</td>\n",
       "      <td>4.050028</td>\n",
       "      <td>0.271429</td>\n",
       "      <td>-1.246989</td>\n",
       "      <td>1400.5121</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>CASRRGAGELFF</td>\n",
       "      <td>0.752373</td>\n",
       "      <td>0.752373</td>\n",
       "      <td>0.939778</td>\n",
       "      <td>0.939778</td>\n",
       "      <td>TRBV10-2</td>\n",
       "      <td>TRBJ2-2</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "      <td>TRBV11-3</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>52.516667</td>\n",
       "      <td>8.249713</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.752991</td>\n",
       "      <td>1313.4845</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2498 rows  31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id  sum_fast_weight  avg_fast_weight  sum_slow_weight  \\\n",
       "0       CASSLTAGANVLTF        26.469125         0.882304        28.583377   \n",
       "1     CASSPGLAGDTGELFF        24.465927         0.873783        27.117365   \n",
       "2          CASSLEDTQYF         1.608272         0.804136         1.572834   \n",
       "3        CASSLGGPNEQYF         1.717529         0.858765         1.641875   \n",
       "4         CASSFQNYGYTF         0.887130         0.887130         0.872458   \n",
       "...                ...              ...              ...              ...   \n",
       "2493     CASRLAGRETQYF         0.864802         0.864802         0.825638   \n",
       "2494  CASSLEGSGGTDTQYF         0.864802         0.864802         0.825638   \n",
       "2495     CASSLASGYEQYF         0.819376         0.819376         0.948234   \n",
       "2496    CASSPGLGNTEAFF         0.819376         0.819376         0.948234   \n",
       "2497      CASRRGAGELFF         0.752373         0.752373         0.939778   \n",
       "\n",
       "      avg_slow_weight V_gene_fast J_gene_fast  codon_diversity_fast  \\\n",
       "0            0.952779    TRBV11-2     TRBJ2-6                    24   \n",
       "1            0.968477      TRBV18     TRBJ2-2                    16   \n",
       "2            0.786417    TRBV12-3     TRBJ2-3                    33   \n",
       "3            0.820937    TRBV12-3     TRBJ2-7                     9   \n",
       "4            0.872458    TRBV11-2     TRBJ1-2                    10   \n",
       "...               ...         ...         ...                   ...   \n",
       "2493         0.825638      TRBV27     TRBJ2-5                     7   \n",
       "2494         0.825638    TRBV11-2     TRBJ2-3                     7   \n",
       "2495         0.948234    TRBV11-1     TRBJ2-7                    59   \n",
       "2496         0.948234    TRBV11-2     TRBJ1-1                    31   \n",
       "2497         0.939778    TRBV10-2     TRBJ2-2                    21   \n",
       "\n",
       "      sample_prevalence_fast V_gene_slow  ... length  aromaticity  \\\n",
       "0                         34    TRBV11-2  ...   14.0     0.071429   \n",
       "1                         19    TRBV10-1  ...   16.0     0.125000   \n",
       "2                        105    TRBV11-1  ...   11.0     0.181818   \n",
       "3                          9    TRBV12-4  ...   13.0     0.153846   \n",
       "4                         25    TRBV12-3  ...   12.0     0.333333   \n",
       "...                      ...         ...  ...    ...          ...   \n",
       "2493                       7     TRBV5-1  ...   13.0     0.153846   \n",
       "2494                       8     TRBV5-1  ...   16.0     0.125000   \n",
       "2495                      60    TRBV12-3  ...   13.0     0.230769   \n",
       "2496                      26    TRBV12-3  ...   14.0     0.142857   \n",
       "2497                      29    TRBV11-3  ...   12.0     0.166667   \n",
       "\n",
       "      instability_index  isoelectric_point     gravy  charge_7.0  \\\n",
       "0             25.792857           5.518123  1.114286   -0.249799   \n",
       "1             29.462500           4.050028  0.450000   -2.245868   \n",
       "2             16.736364           4.050028 -0.290909   -2.246867   \n",
       "3             42.553846           4.050028 -0.376923   -1.247988   \n",
       "4             14.275000           5.516759 -0.200000   -0.251797   \n",
       "...                 ...                ...       ...         ...   \n",
       "2493          12.446154           8.221992 -0.500000    0.751992   \n",
       "2494           5.000000           4.050028 -0.368750   -2.246867   \n",
       "2495          20.730769           4.050028  0.023077   -1.248987   \n",
       "2496          56.057143           4.050028  0.271429   -1.246989   \n",
       "2497          52.516667           8.249713  0.116667    0.752991   \n",
       "\n",
       "      molecular_weight  helix_frac  turn_frac  sheet_frac  \n",
       "0            1354.5282    0.357143   0.285714    0.428571  \n",
       "1            1571.7058    0.312500   0.437500    0.312500  \n",
       "2            1263.3298    0.272727   0.272727    0.363636  \n",
       "3            1372.4589    0.230769   0.461538    0.230769  \n",
       "4            1387.4717    0.083333   0.333333    0.416667  \n",
       "...                ...         ...        ...         ...  \n",
       "2493         1501.6657    0.307692   0.153846    0.307692  \n",
       "2494         1622.6649    0.187500   0.437500    0.312500  \n",
       "2495         1425.5182    0.307692   0.307692    0.307692  \n",
       "2496         1400.5121    0.285714   0.428571    0.285714  \n",
       "2497         1313.4845    0.333333   0.250000    0.250000  \n",
       "\n",
       "[2498 rows x 31 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overlap_node_meta_df = add_tcr_biochem_features(overlap_node_meta_df, seq_col=\"id\")\n",
    "overlap_node_meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f512fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_graph(G_overlap_lcc_largest, \"/dsi/efroni-lab/sbm/OrrBavly/colon_data/graphs/G_overlap_whole_corr_3_87_89.pkl\")\n",
    "# overlap_node_meta_df.to_csv(\"/dsi/efroni-lab/sbm/OrrBavly/colon_data/new_mixcr/tables/g_overlap_whole_node_meta_cor_3_87_89.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983a982a",
   "metadata": {},
   "source": [
    "## Run Multi-Seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b5f44a",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d05ae48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Number of devices: 2\n",
      "Using GPU device 0: NVIDIA A100 80GB PCIe\n",
      "Scaling numeric node features...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "device = connect_gpu()\n",
    "\n",
    "# If G_overlap is a NetworkX graph:\n",
    "G_overlap_dgl_filtered = dgl.from_networkx(G_overlap_lcc_largest)\n",
    "\n",
    "node_list = list(G_overlap_lcc_largest.nodes)\n",
    "node_idx_filtered = {tcr: i for i, tcr in enumerate(node_list)}\n",
    "\n",
    "# 2. Define the list of all columns that need to be numerically scaled.\n",
    "cols_to_normalize = [\n",
    "    'sum_fast_weight', 'avg_fast_weight', 'sum_slow_weight',\n",
    "    'avg_slow_weight', 'codon_diversity_fast', 'sample_prevalence_fast',\n",
    "    'codon_diversity_slow', 'sample_prevalence_slow',\n",
    "    'degree', 'betweenness', 'closeness', 'eigenvector',\n",
    "    'length', 'aromaticity', 'instability_index', 'isoelectric_point',\n",
    "    'gravy', 'charge_7.0', 'molecular_weight',\n",
    "    'helix_frac', 'turn_frac', 'sheet_frac'\n",
    "]\n",
    "\n",
    "# 3. Scale the numeric features directly on the main DataFrame.\n",
    "# The string and ID columns are left untouched.\n",
    "print(\"Scaling numeric node features...\")\n",
    "scaler = StandardScaler()\n",
    "# Use a copy to avoid SettingWithCopyWarning\n",
    "node_meta_scaled = overlap_node_meta_df.copy()\n",
    "node_meta_scaled[cols_to_normalize] = scaler.fit_transform(node_meta_scaled[cols_to_normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a67b711b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Checking Data Types in node_meta_scaled ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2498 entries, 0 to 2497\n",
      "Data columns (total 31 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   id                      2498 non-null   object \n",
      " 1   sum_fast_weight         2498 non-null   float64\n",
      " 2   avg_fast_weight         2498 non-null   float64\n",
      " 3   sum_slow_weight         2498 non-null   float64\n",
      " 4   avg_slow_weight         2498 non-null   float64\n",
      " 5   V_gene_fast             2498 non-null   object \n",
      " 6   J_gene_fast             2498 non-null   object \n",
      " 7   codon_diversity_fast    2498 non-null   float64\n",
      " 8   sample_prevalence_fast  2498 non-null   float64\n",
      " 9   V_gene_slow             2498 non-null   object \n",
      " 10  J_gene_slow             2498 non-null   object \n",
      " 11  codon_diversity_slow    2498 non-null   float64\n",
      " 12  sample_prevalence_slow  2498 non-null   float64\n",
      " 13  degree                  2498 non-null   float64\n",
      " 14  betweenness             2498 non-null   float64\n",
      " 15  closeness               2498 non-null   float64\n",
      " 16  eigenvector             2498 non-null   float64\n",
      " 17  V_gene_fast_id          2498 non-null   int64  \n",
      " 18  V_gene_slow_id          2498 non-null   int64  \n",
      " 19  J_gene_fast_id          2498 non-null   int64  \n",
      " 20  J_gene_slow_id          2498 non-null   int64  \n",
      " 21  length                  2498 non-null   float64\n",
      " 22  aromaticity             2498 non-null   float64\n",
      " 23  instability_index       2498 non-null   float64\n",
      " 24  isoelectric_point       2498 non-null   float64\n",
      " 25  gravy                   2498 non-null   float64\n",
      " 26  charge_7.0              2498 non-null   float64\n",
      " 27  molecular_weight        2498 non-null   float64\n",
      " 28  helix_frac              2498 non-null   float64\n",
      " 29  turn_frac               2498 non-null   float64\n",
      " 30  sheet_frac              2498 non-null   float64\n",
      "dtypes: float64(22), int64(4), object(5)\n",
      "memory usage: 605.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Add this after creating node_meta_scaled\n",
    "print(\"--- Checking Data Types in node_meta_scaled ---\")\n",
    "print(node_meta_scaled.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47f04cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, create the train and validation sets for this single run\n",
    "train_ids, val_ids, train_labels, val_labels = train_test_split(\n",
    "    trainval_ids,\n",
    "    y_trainval,\n",
    "    test_size=0.1,  # e.g., ~10% of the 85% becomes your validation set\n",
    "    stratify=y_trainval,\n",
    "    random_state=r_State_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e6d4235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def build_orig_dyn_feat_dict_for_filtered(\n",
    "    clonotype_df: pd.DataFrame,\n",
    "    node_idx_map: dict,\n",
    "    all_samples: list\n",
    ") -> (dict, dict):\n",
    "    \"\"\"\n",
    "    Builds dictionaries for dynamic features and presence masks using a fast, vectorized approach.\n",
    "\n",
    "    Args:\n",
    "        clonotype_df (pd.DataFrame): DataFrame with TCR data for ALL samples.\n",
    "                                     Must contain columns 'Sample_ID', 'aaSeqCDR3',\n",
    "                                     and 'readFraction'.\n",
    "        node_idx_map (dict): A dictionary mapping TCR strings (aaSeqCDR3) to their\n",
    "                             integer index in the DGL graph.\n",
    "        all_samples (list): A list of all sample_ids to process.\n",
    "\n",
    "    Returns:\n",
    "        tuple[dict, dict]:\n",
    "        - A dictionary mapping sample_id to a (num_nodes, 1) tensor of log(readFraction).\n",
    "        - A dictionary mapping sample_id to a 1D tensor of indices for present nodes.\n",
    "    \"\"\"\n",
    "    dyn_feat_dict = {}\n",
    "    presence_mask_dict = {}\n",
    "    num_nodes = len(node_idx_map)\n",
    "\n",
    "    # --- 1. Vectorized Pre-processing ---\n",
    "    \n",
    "    # Keep only rows corresponding to TCRs that are nodes in our graph.\n",
    "    relevant_tcrs = list(node_idx_map.keys())\n",
    "    filtered_df = clonotype_df[clonotype_df['aaSeqCDR3'].isin(relevant_tcrs)].copy() # <-- CHANGED\n",
    "\n",
    "    # Convert TCR strings to their integer indices all at once.\n",
    "    filtered_df['node_index'] = filtered_df['aaSeqCDR3'].map(node_idx_map) # <-- CHANGED\n",
    "    \n",
    "    # Calculate logFraction for all relevant TCRs at once.\n",
    "    filtered_df['logFraction'] = np.log10(filtered_df['readFraction'] + 1e-10)\n",
    "\n",
    "    # Group the pre-processed DataFrame for fast access.\n",
    "    grouped = filtered_df.groupby('Sample_ID') # <-- CHANGED\n",
    "    \n",
    "    # --- 2. Build Dictionaries ---\n",
    "    \n",
    "    for sample_id in all_samples:\n",
    "        dyn_features = torch.zeros(num_nodes, 1, dtype=torch.float32)\n",
    "        \n",
    "        try:\n",
    "            sample_df = grouped.get_group(sample_id)\n",
    "            \n",
    "            indices = torch.tensor(sample_df['node_index'].values, dtype=torch.long)\n",
    "            values = torch.tensor(sample_df['logFraction'].values, dtype=torch.float32)\n",
    "            \n",
    "            dyn_features[indices, 0] = values\n",
    "            presence_mask_dict[sample_id] = indices\n",
    "        except KeyError:\n",
    "            presence_mask_dict[sample_id] = torch.tensor([], dtype=torch.long)\n",
    "            \n",
    "        dyn_feat_dict[sample_id] = dyn_features\n",
    "            \n",
    "    return dyn_feat_dict, presence_mask_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5ef269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Build dynamic features for ALL samples, but based on the NEW graph's nodes.\n",
    "orig_dyn_feat_dict, presence_mask_dict = build_orig_dyn_feat_dict_for_filtered(\n",
    "    clonotype_df,\n",
    "    node_idx_filtered,      # <-- Using node indices from the new, honest graph\n",
    "    all_samples   # <-- It's okay to build for all samples here, as we create the raw, unscaled logFraction value for every\n",
    "                  # node in your \"honest\" graph, for every sample in your entire dataset.\n",
    ")\n",
    "\n",
    "# Fit the scaler ONLY on the training data.\n",
    "dyn_feat_dict_scaled = scale_dyn_dict(\n",
    "    train_ids,     # <-- Fit scaler on these samples\n",
    "    all_samples,   # <-- Transform all samples\n",
    "    orig_dyn_feat_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b17ef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_filtered(batch):\n",
    "    \"\"\"\n",
    "    Batches samples by creating a unified binary mask for node presence.\n",
    "    \"\"\"\n",
    "    # 1. Unpack the batch. `mask_indices_list` now contains tensors of different lengths.\n",
    "    g_list, dyn_list, mask_indices_list, label_list = zip(*batch)\n",
    "\n",
    "    # 2. Handle the graph, dynamic features, and labels (your existing logic is correct).\n",
    "    g = g_list[0]  # All graphs are the same object.\n",
    "    dyn = torch.stack(dyn_list, dim=0)      # Shape: [batch_size, num_nodes, dyn_dim]\n",
    "    labels = torch.tensor(label_list)        # Shape: [batch_size]\n",
    "\n",
    "    # 3. --- THE FIX: Build the batch-level binary mask ---\n",
    "    batch_size = dyn.shape[0]\n",
    "    num_nodes = dyn.shape[1]\n",
    "\n",
    "    # Create an empty \"switchboard\" of all zeros.\n",
    "    mask = torch.zeros(batch_size, num_nodes, dtype=torch.float32) # Using float32 for broad compatibility.\n",
    "\n",
    "    # Iterate through each sample in the batch to flip the right switches.\n",
    "    for i, indices in enumerate(mask_indices_list):\n",
    "        # `i` is the sample's row in the batch.\n",
    "        # `indices` is the list of TCR node indices to turn on for that sample.\n",
    "        if indices.numel() > 0:  # Check if there are any present nodes.\n",
    "            mask[i, indices] = 1.0\n",
    "\n",
    "    return g, dyn, mask, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86089988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create the master dataset using your new, honest assets\n",
    "dataset = SingleGraphSampleDataset(\n",
    "    G_overlap_dgl_filtered,         # <-- NEW leak-proof graph\n",
    "    node_meta_scaled,     # <-- NEW node features\n",
    "    dyn_feat_dict_scaled,  # <-- The correctly scaled dynamic features\n",
    "    presence_mask_dict,    # <-- The new presence masks\n",
    "    colon_meta.set_index(\"sample_id\")[\"N\"]\n",
    ")\n",
    "\n",
    "# 4. Create the subsets for your loaders (your function is perfect for this)\n",
    "train_set = sample_id_subset(dataset, train_ids)\n",
    "val_set   = sample_id_subset(dataset, val_ids)\n",
    "test_set  = sample_id_subset(dataset, test_ids)\n",
    "\n",
    "# 5. Create DataLoaders\n",
    "BATCH_SIZE = 16\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_fn_filtered)\n",
    "val_loader   = DataLoader(val_set,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn_filtered)\n",
    "test_loader  = DataLoader(test_set,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497db5e6",
   "metadata": {},
   "source": [
    "### Gin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52e46347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the vocabulary size for each categorical feature\n",
    "v_vocab_size = torch.max(G_overlap_dgl_filtered.ndata[\"V_gene_fast_id\"]).item() + 1\n",
    "j_vocab_size = torch.max(G_overlap_dgl_filtered.ndata[\"J_gene_fast_id\"]).item() + 1\n",
    "v_slow_vocab_size = torch.max(G_overlap_dgl_filtered.ndata[\"V_gene_slow_id\"]).item() + 1\n",
    "j_slow_vocab_size = torch.max(G_overlap_dgl_filtered.ndata[\"J_gene_slow_id\"]).item() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "18d3671b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | Loss 0.7282 | Val bal-acc 0.422 | Val F1 0.333\n",
      "Epoch 05 | Loss 0.5143 | Val bal-acc 0.400 | Val F1 0.444\n",
      "Epoch 10 | Loss 0.2454 | Val bal-acc 0.356 | Val F1 0.375\n",
      "Epoch 15 | Loss 0.0640 | Val bal-acc 0.433 | Val F1 0.222\n",
      "Epoch 20 | Loss 0.0237 | Val bal-acc 0.478 | Val F1 0.364\n",
      "Epoch 25 | Loss 0.0203 | Val bal-acc 0.378 | Val F1 0.200\n",
      "Epoch 30 | Loss 0.0252 | Val bal-acc 0.378 | Val F1 0.200\n",
      "Epoch 35 | Loss 0.0140 | Val bal-acc 0.378 | Val F1 0.200\n",
      "Epoch 40 | Loss 0.0195 | Val bal-acc 0.378 | Val F1 0.200\n",
      "Epoch 45 | Loss 0.0137 | Val bal-acc 0.378 | Val F1 0.200\n",
      "Epoch 50 | Loss 0.0164 | Val bal-acc 0.378 | Val F1 0.200\n",
      "Epoch 55 | Loss 0.0203 | Val bal-acc 0.378 | Val F1 0.200\n",
      "Epoch 60 | Loss 0.0236 | Val bal-acc 0.378 | Val F1 0.200\n",
      "Epoch 65 | Loss 0.0124 | Val bal-acc 0.378 | Val F1 0.200\n",
      "Epoch 70 | Loss 0.0264 | Val bal-acc 0.378 | Val F1 0.200\n",
      "Epoch 75 | Loss 0.0120 | Val bal-acc 0.378 | Val F1 0.200\n",
      "Epoch 80 | Loss 0.0113 | Val bal-acc 0.378 | Val F1 0.200\n",
      "Epoch 85 | Loss 0.0265 | Val bal-acc 0.378 | Val F1 0.200\n",
      "Epoch 90 | Loss 0.0121 | Val bal-acc 0.378 | Val F1 0.200\n",
      "Epoch 95 | Loss 0.0215 | Val bal-acc 0.378 | Val F1 0.200\n",
      "Epoch 100 | Loss 0.0149 | Val bal-acc 0.378 | Val F1 0.200\n",
      "Early stopping at epoch 101.\n",
      "\n",
      "Loading best model state for final evaluation...\n",
      "\n",
      "--- Final Test Results ---\n",
      "Test Balanced Accuracy: 0.5000\n",
      "Test F1-Score:          0.0000\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import torch.nn as nn\n",
    "\n",
    "# --- MODEL ---\n",
    "# Double-check the static_dim from your NEW node_meta_numeric\n",
    "static_dim = node_meta_numeric.shape[1]\n",
    "dyn_dim = next(iter(dyn_feat_dict_scaled.values())).shape[-1]\n",
    "\n",
    "model = MaskedGIN(\n",
    "    static_dim=static_dim, # <-- Based on new node features\n",
    "    dyn_dim=dyn_dim,\n",
    "    hidden_dim=128,\n",
    "    num_layers=3,\n",
    "    num_classes=2,\n",
    "    dropout=0.25,\n",
    "    readout='mean',\n",
    "    v_vocab=v_vocab_size,\n",
    "    j_vocab=j_vocab_size,\n",
    "    v_slow_vocab=v_slow_vocab_size,\n",
    "    j_slow_vocab=j_slow_vocab_size\n",
    ").to(device)\n",
    "\n",
    "# --- LOSS FUNCTION ---\n",
    "# Calculate weights ONLY from the training set to avoid data leakage.\n",
    "class_counts = np.bincount(train_labels)\n",
    "weights = 1.0 / torch.tensor(class_counts, dtype=torch.float32)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights.to(device))\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001317, weight_decay=0.000156)\n",
    "\n",
    "# Monitor validation balanced accuracy. If it doesn't improve for 'patience' epochs,\n",
    "# reduce the learning rate by a 'factor'.\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='max',      # We want to maximize balanced accuracy\n",
    "    factor=0.2,      # Reduce LR by a factor of 5 (lr * 0.2)\n",
    "    patience=10     # Wait 10 epochs for improvement before reducing LR\n",
    ")\n",
    "\n",
    "# --- TRAINING LOOP ---\n",
    "best_ba = 0\n",
    "patience = 100\n",
    "epochs_since_best = 0\n",
    "max_epochs = 250\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    train_loss = train_one_epoch(model, train_loader, device, criterion, optimizer)\n",
    "    ba, f1 = evaluate(model, val_loader, device)\n",
    "    # The scheduler looks at the validation balanced accuracy to make a decision\n",
    "    scheduler.step(ba) \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch:02d} | Loss {train_loss:.4f} | Val bal-acc {ba:.3f} | Val F1 {f1:.3f}\")\n",
    "\n",
    "    if ba > best_ba:\n",
    "        best_ba = ba\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "        epochs_since_best = 0\n",
    "    else:\n",
    "        epochs_since_best += 1\n",
    "\n",
    "    if epochs_since_best >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch}.\")\n",
    "        break\n",
    "\n",
    "# --- FINAL EVALUATION ---\n",
    "# Load the best model weights before evaluating on the test set.\n",
    "print(\"\\nLoading best model state for final evaluation...\")\n",
    "model.load_state_dict(best_state)\n",
    "\n",
    "test_balacc, test_f1 = evaluate(model, test_loader, device)\n",
    "print(f\"\\n--- Final Test Results ---\")\n",
    "print(f\"Test Balanced Accuracy: {test_balacc:.4f}\")\n",
    "print(f\"Test F1-Score:          {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f7504a",
   "metadata": {},
   "source": [
    "### Hyper Parameters Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b2e14903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- HYPERPARAMETER SEARCH SPACE -----\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "import copy\n",
    "\n",
    "np.random.seed(r_State_test)\n",
    "random.seed(r_State_test)\n",
    "\n",
    "HIDDEN_OPTIONS = [32, 64, 128, 256]\n",
    "LAYER_OPTIONS = [2, 3, 4]\n",
    "DROPOUT_RANGE = (0.3, 0.6)\n",
    "LR_RANGE = (1e-4, 5e-3)                # Keep a wide range for learning rate\n",
    "WD_RANGE = (5e-5, 5e-3)                # Encourage stronger weight decay\n",
    "READOUT_OPTIONS = ['mean', 'att', 's2s']\n",
    "BATCH_SIZE = 16\n",
    "N_TRIALS = 50  # Can reduce for quick testing\n",
    "NUM_HEADS = [2, 4, 6]\n",
    "\n",
    "def sample_config():\n",
    "    cfg = {\n",
    "        'hidden': random.choice(HIDDEN_OPTIONS),\n",
    "        'layers': random.choice(LAYER_OPTIONS),\n",
    "        'heads': random.choice(NUM_HEADS),\n",
    "        'dropout': round(random.uniform(*DROPOUT_RANGE), 3),\n",
    "        'lr': 10**random.uniform(np.log10(LR_RANGE[0]), np.log10(LR_RANGE[1])),\n",
    "        'wd': 10**random.uniform(np.log10(WD_RANGE[0]), np.log10(WD_RANGE[1])),\n",
    "        'readout': random.choice(READOUT_OPTIONS)\n",
    "    }\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730f86d0",
   "metadata": {},
   "source": [
    "Here instead of using all_samples(160 labeled samples) like original code, we use the trainval ids we calculated in the first step of this pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e847d732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trial 1/50 ---\n",
      "Config: {'hidden': 32, 'layers': 4, 'heads': 4, 'dropout': 0.31, 'lr': np.float64(0.0001442731127840666), 'wd': np.float64(0.00014597947288075484), 'readout': 's2s'}\n",
      "[trial 1 | fold 0 | ep 000] loss=2.3074  val_balacc=0.500  val_F1=0.526\n",
      "[trial 1 | fold 0 | ep 010] loss=2.0024  val_balacc=0.456  val_F1=0.300\n",
      "[trial 1 | fold 0 | ep 020] loss=1.7898  val_balacc=0.433  val_F1=0.438\n",
      "[trial 1 | fold 0 | ep 030] loss=2.3396  val_balacc=0.483  val_F1=0.485\n",
      "[trial 1 | fold 0 | ep 040] loss=2.0719  val_balacc=0.483  val_F1=0.485\n",
      "[trial 1 | fold 0 | ep 050] loss=1.7208  val_balacc=0.483  val_F1=0.485\n",
      "Early stop @ epoch 50 (fold 0). Best BA was 0.500.\n",
      "fold 0: best val_balacc=0.500, val_F1=0.526\n",
      "[trial 1 | fold 1 | ep 000] loss=3.8171  val_balacc=0.472  val_F1=0.250\n",
      "[trial 1 | fold 1 | ep 010] loss=2.5466  val_balacc=0.417  val_F1=0.286\n",
      "[trial 1 | fold 1 | ep 020] loss=2.6098  val_balacc=0.361  val_F1=0.200\n",
      "[trial 1 | fold 1 | ep 030] loss=2.5835  val_balacc=0.417  val_F1=0.424\n",
      "[trial 1 | fold 1 | ep 040] loss=1.7611  val_balacc=0.417  val_F1=0.424\n",
      "[trial 1 | fold 1 | ep 050] loss=2.2950  val_balacc=0.417  val_F1=0.424\n",
      "[trial 1 | fold 1 | ep 060] loss=1.9273  val_balacc=0.417  val_F1=0.424\n",
      "Early stop @ epoch 66 (fold 1). Best BA was 0.611.\n",
      "fold 1: best val_balacc=0.611, val_F1=0.538\n",
      "[trial 1 | fold 2 | ep 000] loss=2.6356  val_balacc=0.471  val_F1=0.000\n",
      "[trial 1 | fold 2 | ep 010] loss=2.5898  val_balacc=0.491  val_F1=0.154\n",
      "[trial 1 | fold 2 | ep 020] loss=1.9819  val_balacc=0.474  val_F1=0.316\n",
      "[trial 1 | fold 2 | ep 030] loss=2.3093  val_balacc=0.524  val_F1=0.400\n",
      "[trial 1 | fold 2 | ep 040] loss=2.1475  val_balacc=0.524  val_F1=0.400\n",
      "[trial 1 | fold 2 | ep 050] loss=2.6568  val_balacc=0.524  val_F1=0.400\n",
      "Early stop @ epoch 53 (fold 2). Best BA was 0.547.\n",
      "fold 2: best val_balacc=0.547, val_F1=0.533\n",
      "[trial 1 | fold 3 | ep 000] loss=4.0552  val_balacc=0.497  val_F1=0.483\n",
      "[trial 1 | fold 3 | ep 010] loss=3.2538  val_balacc=0.482  val_F1=0.250\n",
      "[trial 1 | fold 3 | ep 020] loss=2.9709  val_balacc=0.465  val_F1=0.364\n",
      "[trial 1 | fold 3 | ep 030] loss=3.1598  val_balacc=0.435  val_F1=0.348\n",
      "[trial 1 | fold 3 | ep 040] loss=2.3997  val_balacc=0.494  val_F1=0.381\n",
      "[trial 1 | fold 3 | ep 050] loss=2.7035  val_balacc=0.465  val_F1=0.364\n",
      "[trial 1 | fold 3 | ep 060] loss=2.2137  val_balacc=0.465  val_F1=0.364\n",
      "Early stop @ epoch 64 (fold 3). Best BA was 0.582.\n",
      "fold 3: best val_balacc=0.582, val_F1=0.444\n",
      "[trial 1 | fold 4 | ep 000] loss=2.9716  val_balacc=0.509  val_F1=0.529\n",
      "[trial 1 | fold 4 | ep 010] loss=1.7996  val_balacc=0.509  val_F1=0.529\n",
      "[trial 1 | fold 4 | ep 020] loss=2.6680  val_balacc=0.500  val_F1=0.541\n",
      "[trial 1 | fold 4 | ep 030] loss=1.9551  val_balacc=0.500  val_F1=0.541\n",
      "[trial 1 | fold 4 | ep 040] loss=2.0376  val_balacc=0.500  val_F1=0.541\n",
      "[trial 1 | fold 4 | ep 050] loss=2.4965  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 56 (fold 4). Best BA was 0.597.\n",
      "fold 4: best val_balacc=0.597, val_F1=0.581\n",
      "TRIAL 1: mean bal-acc=0.5680.040, mean F1=0.5250.044\n",
      "\n",
      "--- Trial 2/50 ---\n",
      "Config: {'hidden': 32, 'layers': 4, 'heads': 2, 'dropout': 0.515, 'lr': np.float64(0.0015542829874488774), 'wd': np.float64(0.0003451514055235045), 'readout': 'att'}\n",
      "[trial 2 | fold 0 | ep 000] loss=60.0589  val_balacc=0.500  val_F1=0.526\n",
      "[trial 2 | fold 0 | ep 010] loss=21.8071  val_balacc=0.500  val_F1=0.526\n",
      "[trial 2 | fold 0 | ep 020] loss=16.2547  val_balacc=0.500  val_F1=0.526\n",
      "[trial 2 | fold 0 | ep 030] loss=9.7179  val_balacc=0.500  val_F1=0.526\n",
      "[trial 2 | fold 0 | ep 040] loss=8.2446  val_balacc=0.500  val_F1=0.526\n",
      "[trial 2 | fold 0 | ep 050] loss=11.9726  val_balacc=0.500  val_F1=0.526\n",
      "Early stop @ epoch 54 (fold 0). Best BA was 0.689.\n",
      "fold 0: best val_balacc=0.689, val_F1=0.600\n",
      "[trial 2 | fold 1 | ep 000] loss=91.6697  val_balacc=0.500  val_F1=0.500\n",
      "[trial 2 | fold 1 | ep 010] loss=35.9661  val_balacc=0.500  val_F1=0.500\n",
      "[trial 2 | fold 1 | ep 020] loss=22.6457  val_balacc=0.500  val_F1=0.500\n",
      "[trial 2 | fold 1 | ep 030] loss=19.7398  val_balacc=0.500  val_F1=0.500\n",
      "[trial 2 | fold 1 | ep 040] loss=26.7271  val_balacc=0.500  val_F1=0.500\n",
      "[trial 2 | fold 1 | ep 050] loss=24.0675  val_balacc=0.500  val_F1=0.500\n",
      "Early stop @ epoch 50 (fold 1). Best BA was 0.500.\n",
      "fold 1: best val_balacc=0.500, val_F1=0.500\n",
      "[trial 2 | fold 2 | ep 000] loss=104.2278  val_balacc=0.500  val_F1=0.541\n",
      "[trial 2 | fold 2 | ep 010] loss=44.8218  val_balacc=0.491  val_F1=0.154\n",
      "[trial 2 | fold 2 | ep 020] loss=17.7975  val_balacc=0.500  val_F1=0.541\n",
      "[trial 2 | fold 2 | ep 030] loss=13.0532  val_balacc=0.500  val_F1=0.541\n",
      "[trial 2 | fold 2 | ep 040] loss=11.1058  val_balacc=0.500  val_F1=0.541\n",
      "[trial 2 | fold 2 | ep 050] loss=11.2385  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 59 (fold 2). Best BA was 0.503.\n",
      "fold 2: best val_balacc=0.503, val_F1=0.333\n",
      "[trial 2 | fold 3 | ep 000] loss=97.0004  val_balacc=0.559  val_F1=0.571\n",
      "[trial 2 | fold 3 | ep 010] loss=34.7549  val_balacc=0.500  val_F1=0.541\n",
      "[trial 2 | fold 3 | ep 020] loss=16.0351  val_balacc=0.471  val_F1=0.000\n",
      "[trial 2 | fold 3 | ep 030] loss=20.4869  val_balacc=0.462  val_F1=0.143\n",
      "[trial 2 | fold 3 | ep 040] loss=19.2464  val_balacc=0.403  val_F1=0.125\n",
      "[trial 2 | fold 3 | ep 050] loss=16.9222  val_balacc=0.403  val_F1=0.125\n",
      "Early stop @ epoch 50 (fold 3). Best BA was 0.559.\n",
      "fold 3: best val_balacc=0.559, val_F1=0.571\n",
      "[trial 2 | fold 4 | ep 000] loss=113.9705  val_balacc=0.500  val_F1=0.000\n",
      "[trial 2 | fold 4 | ep 010] loss=42.4145  val_balacc=0.500  val_F1=0.000\n",
      "[trial 2 | fold 4 | ep 020] loss=14.6358  val_balacc=0.500  val_F1=0.000\n",
      "[trial 2 | fold 4 | ep 030] loss=9.3007  val_balacc=0.500  val_F1=0.000\n",
      "[trial 2 | fold 4 | ep 040] loss=11.1848  val_balacc=0.500  val_F1=0.000\n",
      "[trial 2 | fold 4 | ep 050] loss=11.3109  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 59 (fold 4). Best BA was 0.521.\n",
      "fold 4: best val_balacc=0.521, val_F1=0.167\n",
      "TRIAL 2: mean bal-acc=0.5540.071, mean F1=0.4340.163\n",
      "\n",
      "--- Trial 3/50 ---\n",
      "Config: {'hidden': 128, 'layers': 2, 'heads': 2, 'dropout': 0.509, 'lr': np.float64(0.0003785083124576263), 'wd': np.float64(0.00010231257237203595), 'readout': 'att'}\n",
      "[trial 3 | fold 0 | ep 000] loss=2.8554  val_balacc=0.433  val_F1=0.438\n",
      "[trial 3 | fold 0 | ep 010] loss=1.2759  val_balacc=0.500  val_F1=0.000\n",
      "[trial 3 | fold 0 | ep 020] loss=1.1680  val_balacc=0.500  val_F1=0.526\n",
      "[trial 3 | fold 0 | ep 030] loss=1.1381  val_balacc=0.500  val_F1=0.526\n",
      "[trial 3 | fold 0 | ep 040] loss=1.0448  val_balacc=0.500  val_F1=0.526\n",
      "[trial 3 | fold 0 | ep 050] loss=1.2793  val_balacc=0.500  val_F1=0.526\n",
      "Early stop @ epoch 51 (fold 0). Best BA was 0.500.\n",
      "fold 0: best val_balacc=0.500, val_F1=0.526\n",
      "[trial 3 | fold 1 | ep 000] loss=3.1970  val_balacc=0.500  val_F1=0.000\n",
      "[trial 3 | fold 1 | ep 010] loss=1.9234  val_balacc=0.500  val_F1=0.000\n",
      "[trial 3 | fold 1 | ep 020] loss=1.3082  val_balacc=0.500  val_F1=0.000\n",
      "[trial 3 | fold 1 | ep 030] loss=1.2468  val_balacc=0.556  val_F1=0.200\n",
      "[trial 3 | fold 1 | ep 040] loss=1.4233  val_balacc=0.556  val_F1=0.200\n",
      "[trial 3 | fold 1 | ep 050] loss=1.2988  val_balacc=0.556  val_F1=0.200\n",
      "Early stop @ epoch 54 (fold 1). Best BA was 0.556.\n",
      "fold 1: best val_balacc=0.556, val_F1=0.421\n",
      "[trial 3 | fold 2 | ep 000] loss=2.4057  val_balacc=0.500  val_F1=0.541\n",
      "[trial 3 | fold 2 | ep 010] loss=1.4978  val_balacc=0.500  val_F1=0.000\n",
      "[trial 3 | fold 2 | ep 020] loss=0.7417  val_balacc=0.512  val_F1=0.267\n",
      "[trial 3 | fold 2 | ep 030] loss=0.7669  val_balacc=0.503  val_F1=0.333\n",
      "[trial 3 | fold 2 | ep 040] loss=0.7795  val_balacc=0.376  val_F1=0.320\n",
      "[trial 3 | fold 2 | ep 050] loss=0.8087  val_balacc=0.435  val_F1=0.348\n",
      "Early stop @ epoch 55 (fold 2). Best BA was 0.559.\n",
      "fold 2: best val_balacc=0.559, val_F1=0.571\n",
      "[trial 3 | fold 3 | ep 000] loss=2.4209  val_balacc=0.500  val_F1=0.541\n",
      "[trial 3 | fold 3 | ep 010] loss=1.7164  val_balacc=0.500  val_F1=0.000\n",
      "[trial 3 | fold 3 | ep 020] loss=1.4889  val_balacc=0.450  val_F1=0.500\n",
      "[trial 3 | fold 3 | ep 030] loss=1.4386  val_balacc=0.500  val_F1=0.541\n",
      "[trial 3 | fold 3 | ep 040] loss=1.4254  val_balacc=0.500  val_F1=0.541\n",
      "[trial 3 | fold 3 | ep 050] loss=1.3238  val_balacc=0.500  val_F1=0.541\n",
      "[trial 3 | fold 3 | ep 060] loss=1.4370  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 66 (fold 3). Best BA was 0.553.\n",
      "fold 3: best val_balacc=0.553, val_F1=0.421\n",
      "[trial 3 | fold 4 | ep 000] loss=2.6958  val_balacc=0.588  val_F1=0.588\n",
      "[trial 3 | fold 4 | ep 010] loss=1.9724  val_balacc=0.500  val_F1=0.541\n",
      "[trial 3 | fold 4 | ep 020] loss=1.8793  val_balacc=0.500  val_F1=0.541\n",
      "[trial 3 | fold 4 | ep 030] loss=1.9256  val_balacc=0.500  val_F1=0.541\n",
      "[trial 3 | fold 4 | ep 040] loss=1.6225  val_balacc=0.500  val_F1=0.541\n",
      "[trial 3 | fold 4 | ep 050] loss=1.0263  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 50 (fold 4). Best BA was 0.588.\n",
      "fold 4: best val_balacc=0.588, val_F1=0.588\n",
      "TRIAL 3: mean bal-acc=0.5510.029, mean F1=0.5060.072\n",
      "\n",
      "--- Trial 4/50 ---\n",
      "Config: {'hidden': 32, 'layers': 2, 'heads': 4, 'dropout': 0.329, 'lr': np.float64(0.0027533885829989745), 'wd': np.float64(0.0008061615249431441), 'readout': 'mean'}\n",
      "[trial 4 | fold 0 | ep 000] loss=0.9003  val_balacc=0.500  val_F1=0.000\n",
      "[trial 4 | fold 0 | ep 010] loss=0.7016  val_balacc=0.500  val_F1=0.000\n",
      "[trial 4 | fold 0 | ep 020] loss=0.6902  val_balacc=0.500  val_F1=0.000\n",
      "[trial 4 | fold 0 | ep 030] loss=0.6874  val_balacc=0.500  val_F1=0.000\n",
      "[trial 4 | fold 0 | ep 040] loss=0.6832  val_balacc=0.500  val_F1=0.000\n",
      "[trial 4 | fold 0 | ep 050] loss=0.6794  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 57 (fold 0). Best BA was 0.528.\n",
      "fold 0: best val_balacc=0.528, val_F1=0.541\n",
      "[trial 4 | fold 1 | ep 000] loss=1.0089  val_balacc=0.500  val_F1=0.000\n",
      "[trial 4 | fold 1 | ep 010] loss=0.6868  val_balacc=0.500  val_F1=0.000\n",
      "[trial 4 | fold 1 | ep 020] loss=0.6835  val_balacc=0.500  val_F1=0.000\n",
      "[trial 4 | fold 1 | ep 030] loss=0.6872  val_balacc=0.500  val_F1=0.000\n",
      "[trial 4 | fold 1 | ep 040] loss=0.6731  val_balacc=0.472  val_F1=0.000\n",
      "[trial 4 | fold 1 | ep 050] loss=0.6625  val_balacc=0.583  val_F1=0.333\n",
      "[trial 4 | fold 1 | ep 060] loss=0.6561  val_balacc=0.472  val_F1=0.000\n",
      "[trial 4 | fold 1 | ep 070] loss=0.6791  val_balacc=0.472  val_F1=0.000\n",
      "Early stop @ epoch 77 (fold 1). Best BA was 0.583.\n",
      "fold 1: best val_balacc=0.583, val_F1=0.400\n",
      "[trial 4 | fold 2 | ep 000] loss=0.7245  val_balacc=0.500  val_F1=0.541\n",
      "[trial 4 | fold 2 | ep 010] loss=0.6952  val_balacc=0.500  val_F1=0.000\n",
      "[trial 4 | fold 2 | ep 020] loss=0.6824  val_balacc=0.588  val_F1=0.588\n",
      "[trial 4 | fold 2 | ep 030] loss=0.6768  val_balacc=0.515  val_F1=0.435\n",
      "[trial 4 | fold 2 | ep 040] loss=0.6509  val_balacc=0.606  val_F1=0.571\n",
      "[trial 4 | fold 2 | ep 050] loss=0.6503  val_balacc=0.565  val_F1=0.500\n",
      "[trial 4 | fold 2 | ep 060] loss=0.6523  val_balacc=0.535  val_F1=0.480\n",
      "Early stop @ epoch 69 (fold 2). Best BA was 0.656.\n",
      "fold 2: best val_balacc=0.656, val_F1=0.621\n",
      "[trial 4 | fold 3 | ep 000] loss=1.0264  val_balacc=0.500  val_F1=0.000\n",
      "[trial 4 | fold 3 | ep 010] loss=0.6832  val_balacc=0.529  val_F1=0.556\n",
      "[trial 4 | fold 3 | ep 020] loss=0.6752  val_balacc=0.547  val_F1=0.533\n",
      "[trial 4 | fold 3 | ep 030] loss=0.6770  val_balacc=0.585  val_F1=0.538\n",
      "[trial 4 | fold 3 | ep 040] loss=0.6650  val_balacc=0.535  val_F1=0.480\n",
      "[trial 4 | fold 3 | ep 050] loss=0.6386  val_balacc=0.535  val_F1=0.480\n",
      "[trial 4 | fold 3 | ep 060] loss=0.6442  val_balacc=0.535  val_F1=0.480\n",
      "Early stop @ epoch 69 (fold 3). Best BA was 0.618.\n",
      "fold 3: best val_balacc=0.618, val_F1=0.606\n",
      "[trial 4 | fold 4 | ep 000] loss=0.8466  val_balacc=0.500  val_F1=0.541\n",
      "[trial 4 | fold 4 | ep 010] loss=0.7047  val_balacc=0.500  val_F1=0.541\n",
      "[trial 4 | fold 4 | ep 020] loss=0.6903  val_balacc=0.532  val_F1=0.353\n",
      "[trial 4 | fold 4 | ep 030] loss=0.6758  val_balacc=0.462  val_F1=0.143\n",
      "[trial 4 | fold 4 | ep 040] loss=0.6738  val_balacc=0.462  val_F1=0.143\n",
      "[trial 4 | fold 4 | ep 050] loss=0.6707  val_balacc=0.521  val_F1=0.167\n",
      "[trial 4 | fold 4 | ep 060] loss=0.6667  val_balacc=0.521  val_F1=0.167\n",
      "[trial 4 | fold 4 | ep 070] loss=0.6745  val_balacc=0.521  val_F1=0.167\n",
      "Early stop @ epoch 70 (fold 4). Best BA was 0.532.\n",
      "fold 4: best val_balacc=0.532, val_F1=0.353\n",
      "TRIAL 4: mean bal-acc=0.5830.049, mean F1=0.5040.109\n",
      "\n",
      "--- Trial 5/50 ---\n",
      "Config: {'hidden': 256, 'layers': 4, 'heads': 2, 'dropout': 0.592, 'lr': np.float64(0.00043966161665064894), 'wd': np.float64(0.0006354059347820527), 'readout': 's2s'}\n",
      "[trial 5 | fold 0 | ep 000] loss=65.3609  val_balacc=0.500  val_F1=0.526\n",
      "[trial 5 | fold 0 | ep 010] loss=113.0293  val_balacc=0.444  val_F1=0.000\n",
      "[trial 5 | fold 0 | ep 020] loss=53.4510  val_balacc=0.500  val_F1=0.000\n",
      "[trial 5 | fold 0 | ep 030] loss=59.2007  val_balacc=0.500  val_F1=0.000\n",
      "[trial 5 | fold 0 | ep 040] loss=58.2878  val_balacc=0.500  val_F1=0.000\n",
      "[trial 5 | fold 0 | ep 050] loss=36.5337  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 0). Best BA was 0.500.\n",
      "fold 0: best val_balacc=0.500, val_F1=0.526\n",
      "[trial 5 | fold 1 | ep 000] loss=95.6558  val_balacc=0.500  val_F1=0.000\n",
      "[trial 5 | fold 1 | ep 010] loss=90.7022  val_balacc=0.500  val_F1=0.000\n",
      "[trial 5 | fold 1 | ep 020] loss=89.1688  val_balacc=0.500  val_F1=0.000\n",
      "[trial 5 | fold 1 | ep 030] loss=63.4587  val_balacc=0.500  val_F1=0.000\n",
      "[trial 5 | fold 1 | ep 040] loss=64.7391  val_balacc=0.500  val_F1=0.000\n",
      "[trial 5 | fold 1 | ep 050] loss=64.3669  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 1). Best BA was 0.500.\n",
      "fold 1: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 5 | fold 2 | ep 000] loss=96.2927  val_balacc=0.500  val_F1=0.541\n",
      "[trial 5 | fold 2 | ep 010] loss=70.8701  val_balacc=0.500  val_F1=0.000\n",
      "[trial 5 | fold 2 | ep 020] loss=56.5832  val_balacc=0.500  val_F1=0.000\n",
      "[trial 5 | fold 2 | ep 030] loss=51.3587  val_balacc=0.500  val_F1=0.000\n",
      "[trial 5 | fold 2 | ep 040] loss=65.4352  val_balacc=0.500  val_F1=0.000\n",
      "[trial 5 | fold 2 | ep 050] loss=66.2211  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 53 (fold 2). Best BA was 0.521.\n",
      "fold 2: best val_balacc=0.521, val_F1=0.167\n",
      "[trial 5 | fold 3 | ep 000] loss=83.4114  val_balacc=0.500  val_F1=0.000\n",
      "[trial 5 | fold 3 | ep 010] loss=71.6299  val_balacc=0.500  val_F1=0.000\n",
      "[trial 5 | fold 3 | ep 020] loss=60.4117  val_balacc=0.429  val_F1=0.471\n",
      "[trial 5 | fold 3 | ep 030] loss=71.7331  val_balacc=0.429  val_F1=0.471\n",
      "[trial 5 | fold 3 | ep 040] loss=68.9180  val_balacc=0.429  val_F1=0.471\n",
      "[trial 5 | fold 3 | ep 050] loss=57.4351  val_balacc=0.429  val_F1=0.471\n",
      "Early stop @ epoch 50 (fold 3). Best BA was 0.500.\n",
      "fold 3: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 5 | fold 4 | ep 000] loss=91.2939  val_balacc=0.500  val_F1=0.541\n",
      "[trial 5 | fold 4 | ep 010] loss=101.7321  val_balacc=0.553  val_F1=0.421\n",
      "[trial 5 | fold 4 | ep 020] loss=65.5699  val_balacc=0.500  val_F1=0.000\n",
      "[trial 5 | fold 4 | ep 030] loss=60.7994  val_balacc=0.500  val_F1=0.000\n",
      "[trial 5 | fold 4 | ep 040] loss=73.1041  val_balacc=0.500  val_F1=0.000\n",
      "[trial 5 | fold 4 | ep 050] loss=56.5240  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 53 (fold 4). Best BA was 0.600.\n",
      "fold 4: best val_balacc=0.600, val_F1=0.333\n",
      "TRIAL 5: mean bal-acc=0.5240.039, mean F1=0.2050.203\n",
      "\n",
      "--- Trial 6/50 ---\n",
      "Config: {'hidden': 128, 'layers': 4, 'heads': 2, 'dropout': 0.511, 'lr': np.float64(0.00011963389795293417), 'wd': np.float64(0.00014281260979546632), 'readout': 'att'}\n",
      "[trial 6 | fold 0 | ep 000] loss=28.4099  val_balacc=0.500  val_F1=0.526\n",
      "[trial 6 | fold 0 | ep 010] loss=31.2151  val_balacc=0.500  val_F1=0.526\n",
      "[trial 6 | fold 0 | ep 020] loss=29.0909  val_balacc=0.500  val_F1=0.526\n",
      "[trial 6 | fold 0 | ep 030] loss=33.1379  val_balacc=0.500  val_F1=0.526\n",
      "[trial 6 | fold 0 | ep 040] loss=29.5415  val_balacc=0.500  val_F1=0.526\n",
      "[trial 6 | fold 0 | ep 050] loss=29.4339  val_balacc=0.500  val_F1=0.526\n",
      "Early stop @ epoch 50 (fold 0). Best BA was 0.500.\n",
      "fold 0: best val_balacc=0.500, val_F1=0.526\n",
      "[trial 6 | fold 1 | ep 000] loss=47.3684  val_balacc=0.500  val_F1=0.000\n",
      "[trial 6 | fold 1 | ep 010] loss=42.1637  val_balacc=0.500  val_F1=0.000\n",
      "[trial 6 | fold 1 | ep 020] loss=38.6577  val_balacc=0.500  val_F1=0.000\n",
      "[trial 6 | fold 1 | ep 030] loss=29.0560  val_balacc=0.500  val_F1=0.000\n",
      "[trial 6 | fold 1 | ep 040] loss=34.9948  val_balacc=0.500  val_F1=0.000\n",
      "[trial 6 | fold 1 | ep 050] loss=24.5769  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 1). Best BA was 0.500.\n",
      "fold 1: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 6 | fold 2 | ep 000] loss=39.5173  val_balacc=0.559  val_F1=0.571\n",
      "[trial 6 | fold 2 | ep 010] loss=26.5825  val_balacc=0.479  val_F1=0.514\n",
      "[trial 6 | fold 2 | ep 020] loss=32.0562  val_balacc=0.606  val_F1=0.571\n",
      "[trial 6 | fold 2 | ep 030] loss=36.9650  val_balacc=0.500  val_F1=0.000\n",
      "[trial 6 | fold 2 | ep 040] loss=27.7170  val_balacc=0.500  val_F1=0.000\n",
      "[trial 6 | fold 2 | ep 050] loss=33.3159  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 56 (fold 2). Best BA was 0.656.\n",
      "fold 2: best val_balacc=0.656, val_F1=0.621\n",
      "[trial 6 | fold 3 | ep 000] loss=29.8171  val_balacc=0.500  val_F1=0.541\n",
      "[trial 6 | fold 3 | ep 010] loss=27.7832  val_balacc=0.500  val_F1=0.541\n",
      "[trial 6 | fold 3 | ep 020] loss=32.8321  val_balacc=0.500  val_F1=0.541\n",
      "[trial 6 | fold 3 | ep 030] loss=28.4646  val_balacc=0.500  val_F1=0.541\n",
      "[trial 6 | fold 3 | ep 040] loss=30.7391  val_balacc=0.500  val_F1=0.541\n",
      "[trial 6 | fold 3 | ep 050] loss=16.4529  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 50 (fold 3). Best BA was 0.500.\n",
      "fold 3: best val_balacc=0.500, val_F1=0.541\n",
      "[trial 6 | fold 4 | ep 000] loss=34.5201  val_balacc=0.500  val_F1=0.541\n",
      "[trial 6 | fold 4 | ep 010] loss=24.1552  val_balacc=0.500  val_F1=0.541\n",
      "[trial 6 | fold 4 | ep 020] loss=17.5874  val_balacc=0.500  val_F1=0.541\n",
      "[trial 6 | fold 4 | ep 030] loss=25.8856  val_balacc=0.500  val_F1=0.541\n",
      "[trial 6 | fold 4 | ep 040] loss=26.3994  val_balacc=0.500  val_F1=0.541\n",
      "[trial 6 | fold 4 | ep 050] loss=36.0282  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 50 (fold 4). Best BA was 0.500.\n",
      "fold 4: best val_balacc=0.500, val_F1=0.541\n",
      "TRIAL 6: mean bal-acc=0.5310.062, mean F1=0.4460.225\n",
      "\n",
      "--- Trial 7/50 ---\n",
      "Config: {'hidden': 32, 'layers': 2, 'heads': 2, 'dropout': 0.414, 'lr': np.float64(0.0005892919430910705), 'wd': np.float64(0.002329114599241839), 'readout': 'mean'}\n",
      "[trial 7 | fold 0 | ep 000] loss=0.7783  val_balacc=0.500  val_F1=0.000\n",
      "[trial 7 | fold 0 | ep 010] loss=0.6802  val_balacc=0.500  val_F1=0.000\n",
      "[trial 7 | fold 0 | ep 020] loss=0.6937  val_balacc=0.522  val_F1=0.167\n",
      "[trial 7 | fold 0 | ep 030] loss=0.7333  val_balacc=0.494  val_F1=0.154\n",
      "[trial 7 | fold 0 | ep 040] loss=0.7037  val_balacc=0.539  val_F1=0.353\n",
      "[trial 7 | fold 0 | ep 050] loss=0.7085  val_balacc=0.489  val_F1=0.250\n",
      "[trial 7 | fold 0 | ep 060] loss=0.6934  val_balacc=0.544  val_F1=0.286\n",
      "[trial 7 | fold 0 | ep 070] loss=0.7068  val_balacc=0.544  val_F1=0.286\n",
      "Early stop @ epoch 76 (fold 0). Best BA was 0.594.\n",
      "fold 0: best val_balacc=0.594, val_F1=0.400\n",
      "[trial 7 | fold 1 | ep 000] loss=0.7764  val_balacc=0.500  val_F1=0.000\n",
      "[trial 7 | fold 1 | ep 010] loss=0.6881  val_balacc=0.500  val_F1=0.000\n",
      "[trial 7 | fold 1 | ep 020] loss=0.7108  val_balacc=0.500  val_F1=0.000\n",
      "[trial 7 | fold 1 | ep 030] loss=0.6909  val_balacc=0.500  val_F1=0.000\n",
      "[trial 7 | fold 1 | ep 040] loss=0.6726  val_balacc=0.500  val_F1=0.000\n",
      "[trial 7 | fold 1 | ep 050] loss=0.6874  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 1). Best BA was 0.500.\n",
      "fold 1: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 7 | fold 2 | ep 000] loss=0.6989  val_balacc=0.500  val_F1=0.000\n",
      "[trial 7 | fold 2 | ep 010] loss=0.6971  val_balacc=0.529  val_F1=0.556\n",
      "[trial 7 | fold 2 | ep 020] loss=0.6935  val_balacc=0.529  val_F1=0.556\n",
      "[trial 7 | fold 2 | ep 030] loss=0.7005  val_balacc=0.547  val_F1=0.533\n",
      "[trial 7 | fold 2 | ep 040] loss=0.6856  val_balacc=0.479  val_F1=0.514\n",
      "[trial 7 | fold 2 | ep 050] loss=0.6898  val_balacc=0.479  val_F1=0.514\n",
      "Early stop @ epoch 57 (fold 2). Best BA was 0.547.\n",
      "fold 2: best val_balacc=0.547, val_F1=0.533\n",
      "[trial 7 | fold 3 | ep 000] loss=1.0029  val_balacc=0.500  val_F1=0.000\n",
      "[trial 7 | fold 3 | ep 010] loss=0.7177  val_balacc=0.500  val_F1=0.541\n",
      "[trial 7 | fold 3 | ep 020] loss=0.6986  val_balacc=0.500  val_F1=0.541\n",
      "[trial 7 | fold 3 | ep 030] loss=0.7191  val_balacc=0.500  val_F1=0.541\n",
      "[trial 7 | fold 3 | ep 040] loss=0.7278  val_balacc=0.500  val_F1=0.541\n",
      "[trial 7 | fold 3 | ep 050] loss=0.7098  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 51 (fold 3). Best BA was 0.632.\n",
      "fold 3: best val_balacc=0.632, val_F1=0.526\n",
      "[trial 7 | fold 4 | ep 000] loss=0.7886  val_balacc=0.500  val_F1=0.541\n",
      "[trial 7 | fold 4 | ep 010] loss=0.7081  val_balacc=0.465  val_F1=0.364\n",
      "[trial 7 | fold 4 | ep 020] loss=0.7082  val_balacc=0.426  val_F1=0.385\n",
      "[trial 7 | fold 4 | ep 030] loss=0.6695  val_balacc=0.438  val_F1=0.452\n",
      "[trial 7 | fold 4 | ep 040] loss=0.6918  val_balacc=0.497  val_F1=0.483\n",
      "[trial 7 | fold 4 | ep 050] loss=0.6788  val_balacc=0.497  val_F1=0.483\n",
      "Early stop @ epoch 57 (fold 4). Best BA was 0.662.\n",
      "fold 4: best val_balacc=0.662, val_F1=0.556\n",
      "TRIAL 7: mean bal-acc=0.5870.058, mean F1=0.4030.209\n",
      "\n",
      "--- Trial 8/50 ---\n",
      "Config: {'hidden': 128, 'layers': 3, 'heads': 2, 'dropout': 0.501, 'lr': np.float64(0.0015572977674772554), 'wd': np.float64(0.001164301174099801), 'readout': 'mean'}\n",
      "[trial 8 | fold 0 | ep 000] loss=4.6347  val_balacc=0.500  val_F1=0.000\n",
      "[trial 8 | fold 0 | ep 010] loss=0.7344  val_balacc=0.500  val_F1=0.526\n",
      "[trial 8 | fold 0 | ep 020] loss=0.6795  val_balacc=0.500  val_F1=0.526\n",
      "[trial 8 | fold 0 | ep 030] loss=0.7131  val_balacc=0.500  val_F1=0.526\n",
      "[trial 8 | fold 0 | ep 040] loss=0.6948  val_balacc=0.500  val_F1=0.526\n",
      "[trial 8 | fold 0 | ep 050] loss=0.6970  val_balacc=0.500  val_F1=0.526\n",
      "Early stop @ epoch 50 (fold 0). Best BA was 0.500.\n",
      "fold 0: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 8 | fold 1 | ep 000] loss=2.7972  val_balacc=0.500  val_F1=0.000\n",
      "[trial 8 | fold 1 | ep 010] loss=0.7401  val_balacc=0.500  val_F1=0.000\n",
      "[trial 8 | fold 1 | ep 020] loss=0.7151  val_balacc=0.500  val_F1=0.000\n",
      "[trial 8 | fold 1 | ep 030] loss=0.6957  val_balacc=0.500  val_F1=0.000\n",
      "[trial 8 | fold 1 | ep 040] loss=0.7007  val_balacc=0.500  val_F1=0.000\n",
      "[trial 8 | fold 1 | ep 050] loss=0.6959  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 53 (fold 1). Best BA was 0.583.\n",
      "fold 1: best val_balacc=0.583, val_F1=0.333\n",
      "[trial 8 | fold 2 | ep 000] loss=5.2972  val_balacc=0.500  val_F1=0.000\n",
      "[trial 8 | fold 2 | ep 010] loss=0.7234  val_balacc=0.500  val_F1=0.541\n",
      "[trial 8 | fold 2 | ep 020] loss=0.6973  val_balacc=0.500  val_F1=0.541\n",
      "[trial 8 | fold 2 | ep 030] loss=0.7118  val_balacc=0.500  val_F1=0.541\n",
      "[trial 8 | fold 2 | ep 040] loss=0.7021  val_balacc=0.500  val_F1=0.541\n",
      "[trial 8 | fold 2 | ep 050] loss=0.7073  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 50 (fold 2). Best BA was 0.500.\n",
      "fold 2: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 8 | fold 3 | ep 000] loss=3.8660  val_balacc=0.500  val_F1=0.000\n",
      "[trial 8 | fold 3 | ep 010] loss=0.7498  val_balacc=0.500  val_F1=0.541\n",
      "[trial 8 | fold 3 | ep 020] loss=0.7057  val_balacc=0.500  val_F1=0.000\n",
      "[trial 8 | fold 3 | ep 030] loss=0.6846  val_balacc=0.535  val_F1=0.480\n",
      "[trial 8 | fold 3 | ep 040] loss=0.6898  val_balacc=0.494  val_F1=0.381\n",
      "[trial 8 | fold 3 | ep 050] loss=0.7129  val_balacc=0.500  val_F1=0.541\n",
      "[trial 8 | fold 3 | ep 060] loss=0.6978  val_balacc=0.500  val_F1=0.541\n",
      "[trial 8 | fold 3 | ep 070] loss=0.7101  val_balacc=0.529  val_F1=0.556\n",
      "[trial 8 | fold 3 | ep 080] loss=0.7011  val_balacc=0.529  val_F1=0.556\n",
      "Early stop @ epoch 88 (fold 3). Best BA was 0.585.\n",
      "fold 3: best val_balacc=0.585, val_F1=0.538\n",
      "[trial 8 | fold 4 | ep 000] loss=2.0329  val_balacc=0.500  val_F1=0.541\n",
      "[trial 8 | fold 4 | ep 010] loss=0.7054  val_balacc=0.500  val_F1=0.541\n",
      "[trial 8 | fold 4 | ep 020] loss=0.7131  val_balacc=0.500  val_F1=0.541\n",
      "[trial 8 | fold 4 | ep 030] loss=0.6871  val_balacc=0.500  val_F1=0.541\n",
      "[trial 8 | fold 4 | ep 040] loss=0.7096  val_balacc=0.500  val_F1=0.541\n",
      "[trial 8 | fold 4 | ep 050] loss=0.7169  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 50 (fold 4). Best BA was 0.500.\n",
      "fold 4: best val_balacc=0.500, val_F1=0.541\n",
      "TRIAL 8: mean bal-acc=0.5340.041, mean F1=0.2820.243\n",
      "\n",
      "--- Trial 9/50 ---\n",
      "Config: {'hidden': 64, 'layers': 4, 'heads': 6, 'dropout': 0.373, 'lr': np.float64(0.0006100511275091178), 'wd': np.float64(0.0001733267755987225), 'readout': 's2s'}\n",
      "[trial 9 | fold 0 | ep 000] loss=4.6515  val_balacc=0.500  val_F1=0.526\n",
      "[trial 9 | fold 0 | ep 010] loss=3.7764  val_balacc=0.472  val_F1=0.000\n",
      "[trial 9 | fold 0 | ep 020] loss=2.5161  val_balacc=0.467  val_F1=0.429\n",
      "[trial 9 | fold 0 | ep 030] loss=1.9315  val_balacc=0.567  val_F1=0.533\n",
      "[trial 9 | fold 0 | ep 040] loss=1.9154  val_balacc=0.478  val_F1=0.500\n",
      "[trial 9 | fold 0 | ep 050] loss=1.5947  val_balacc=0.478  val_F1=0.500\n",
      "[trial 9 | fold 0 | ep 060] loss=2.4528  val_balacc=0.428  val_F1=0.457\n",
      "[trial 9 | fold 0 | ep 070] loss=2.0759  val_balacc=0.428  val_F1=0.457\n",
      "Early stop @ epoch 73 (fold 0). Best BA was 0.567.\n",
      "fold 0: best val_balacc=0.567, val_F1=0.375\n",
      "[trial 9 | fold 1 | ep 000] loss=4.7784  val_balacc=0.444  val_F1=0.235\n",
      "[trial 9 | fold 1 | ep 010] loss=3.6078  val_balacc=0.500  val_F1=0.500\n",
      "[trial 9 | fold 1 | ep 020] loss=2.2453  val_balacc=0.500  val_F1=0.500\n",
      "[trial 9 | fold 1 | ep 030] loss=2.3699  val_balacc=0.500  val_F1=0.500\n",
      "[trial 9 | fold 1 | ep 040] loss=2.1724  val_balacc=0.500  val_F1=0.500\n",
      "[trial 9 | fold 1 | ep 050] loss=2.2675  val_balacc=0.500  val_F1=0.500\n",
      "Early stop @ epoch 52 (fold 1). Best BA was 0.583.\n",
      "fold 1: best val_balacc=0.583, val_F1=0.400\n",
      "[trial 9 | fold 2 | ep 000] loss=9.2994  val_balacc=0.500  val_F1=0.541\n",
      "[trial 9 | fold 2 | ep 010] loss=3.8594  val_balacc=0.509  val_F1=0.529\n",
      "[trial 9 | fold 2 | ep 020] loss=2.3761  val_balacc=0.524  val_F1=0.400\n",
      "[trial 9 | fold 2 | ep 030] loss=1.6693  val_balacc=0.685  val_F1=0.643\n",
      "[trial 9 | fold 2 | ep 040] loss=1.6989  val_balacc=0.518  val_F1=0.516\n",
      "[trial 9 | fold 2 | ep 050] loss=1.4943  val_balacc=0.326  val_F1=0.250\n",
      "[trial 9 | fold 2 | ep 060] loss=1.3431  val_balacc=0.418  val_F1=0.414\n",
      "[trial 9 | fold 2 | ep 070] loss=1.3011  val_balacc=0.426  val_F1=0.385\n",
      "[trial 9 | fold 2 | ep 080] loss=1.5762  val_balacc=0.376  val_F1=0.320\n",
      "Early stop @ epoch 80 (fold 2). Best BA was 0.685.\n",
      "fold 2: best val_balacc=0.685, val_F1=0.643\n",
      "[trial 9 | fold 3 | ep 000] loss=9.0124  val_balacc=0.500  val_F1=0.541\n",
      "[trial 9 | fold 3 | ep 010] loss=3.8731  val_balacc=0.553  val_F1=0.421\n",
      "[trial 9 | fold 3 | ep 020] loss=2.1585  val_balacc=0.500  val_F1=0.541\n",
      "[trial 9 | fold 3 | ep 030] loss=1.4007  val_balacc=0.529  val_F1=0.556\n",
      "[trial 9 | fold 3 | ep 040] loss=1.0689  val_balacc=0.465  val_F1=0.364\n",
      "[trial 9 | fold 3 | ep 050] loss=0.9567  val_balacc=0.415  val_F1=0.286\n",
      "[trial 9 | fold 3 | ep 060] loss=1.1060  val_balacc=0.465  val_F1=0.364\n",
      "Early stop @ epoch 69 (fold 3). Best BA was 0.624.\n",
      "fold 3: best val_balacc=0.624, val_F1=0.545\n",
      "[trial 9 | fold 4 | ep 000] loss=5.0262  val_balacc=0.500  val_F1=0.541\n",
      "[trial 9 | fold 4 | ep 010] loss=3.1855  val_balacc=0.500  val_F1=0.000\n",
      "[trial 9 | fold 4 | ep 020] loss=2.6671  val_balacc=0.500  val_F1=0.000\n",
      "[trial 9 | fold 4 | ep 030] loss=1.8500  val_balacc=0.500  val_F1=0.000\n",
      "[trial 9 | fold 4 | ep 040] loss=2.3165  val_balacc=0.500  val_F1=0.000\n",
      "[trial 9 | fold 4 | ep 050] loss=1.5598  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 51 (fold 4). Best BA was 0.635.\n",
      "fold 4: best val_balacc=0.635, val_F1=0.593\n",
      "TRIAL 9: mean bal-acc=0.6190.042, mean F1=0.5110.106\n",
      "\n",
      "--- Trial 10/50 ---\n",
      "Config: {'hidden': 64, 'layers': 4, 'heads': 4, 'dropout': 0.553, 'lr': np.float64(0.0020816157877655748), 'wd': np.float64(0.00014357080922822423), 'readout': 'mean'}\n",
      "[trial 10 | fold 0 | ep 000] loss=21.9053  val_balacc=0.500  val_F1=0.000\n",
      "[trial 10 | fold 0 | ep 010] loss=0.7743  val_balacc=0.500  val_F1=0.526\n",
      "[trial 10 | fold 0 | ep 020] loss=0.7345  val_balacc=0.500  val_F1=0.526\n",
      "[trial 10 | fold 0 | ep 030] loss=0.7366  val_balacc=0.500  val_F1=0.526\n",
      "[trial 10 | fold 0 | ep 040] loss=0.7363  val_balacc=0.500  val_F1=0.526\n",
      "[trial 10 | fold 0 | ep 050] loss=0.7262  val_balacc=0.500  val_F1=0.526\n",
      "Early stop @ epoch 52 (fold 0). Best BA was 0.517.\n",
      "fold 0: best val_balacc=0.517, val_F1=0.267\n",
      "[trial 10 | fold 1 | ep 000] loss=22.3242  val_balacc=0.500  val_F1=0.500\n",
      "[trial 10 | fold 1 | ep 010] loss=0.7139  val_balacc=0.500  val_F1=0.500\n",
      "[trial 10 | fold 1 | ep 020] loss=0.7309  val_balacc=0.500  val_F1=0.500\n",
      "[trial 10 | fold 1 | ep 030] loss=0.6972  val_balacc=0.500  val_F1=0.500\n",
      "[trial 10 | fold 1 | ep 040] loss=0.7474  val_balacc=0.500  val_F1=0.500\n",
      "[trial 10 | fold 1 | ep 050] loss=0.6832  val_balacc=0.500  val_F1=0.500\n",
      "Early stop @ epoch 50 (fold 1). Best BA was 0.500.\n",
      "fold 1: best val_balacc=0.500, val_F1=0.500\n",
      "[trial 10 | fold 2 | ep 000] loss=18.7851  val_balacc=0.500  val_F1=0.000\n",
      "[trial 10 | fold 2 | ep 010] loss=0.8781  val_balacc=0.500  val_F1=0.541\n",
      "[trial 10 | fold 2 | ep 020] loss=0.7209  val_balacc=0.500  val_F1=0.541\n",
      "[trial 10 | fold 2 | ep 030] loss=0.7188  val_balacc=0.500  val_F1=0.541\n",
      "[trial 10 | fold 2 | ep 040] loss=0.7112  val_balacc=0.500  val_F1=0.541\n",
      "[trial 10 | fold 2 | ep 050] loss=0.7054  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 54 (fold 2). Best BA was 0.600.\n",
      "fold 2: best val_balacc=0.600, val_F1=0.333\n",
      "[trial 10 | fold 3 | ep 000] loss=23.4788  val_balacc=0.500  val_F1=0.000\n",
      "[trial 10 | fold 3 | ep 010] loss=0.7672  val_balacc=0.500  val_F1=0.541\n",
      "[trial 10 | fold 3 | ep 020] loss=0.6928  val_balacc=0.500  val_F1=0.541\n",
      "[trial 10 | fold 3 | ep 030] loss=0.7264  val_balacc=0.500  val_F1=0.541\n",
      "[trial 10 | fold 3 | ep 040] loss=0.7123  val_balacc=0.500  val_F1=0.541\n",
      "[trial 10 | fold 3 | ep 050] loss=0.7058  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 50 (fold 3). Best BA was 0.500.\n",
      "fold 3: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 10 | fold 4 | ep 000] loss=18.9546  val_balacc=0.500  val_F1=0.000\n",
      "[trial 10 | fold 4 | ep 010] loss=0.8555  val_balacc=0.500  val_F1=0.541\n",
      "[trial 10 | fold 4 | ep 020] loss=0.8477  val_balacc=0.500  val_F1=0.541\n",
      "[trial 10 | fold 4 | ep 030] loss=0.7116  val_balacc=0.500  val_F1=0.541\n",
      "[trial 10 | fold 4 | ep 040] loss=0.6773  val_balacc=0.500  val_F1=0.541\n",
      "[trial 10 | fold 4 | ep 050] loss=0.7482  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 50 (fold 4). Best BA was 0.500.\n",
      "fold 4: best val_balacc=0.500, val_F1=0.000\n",
      "TRIAL 10: mean bal-acc=0.5230.039, mean F1=0.2200.195\n",
      "\n",
      "--- Trial 11/50 ---\n",
      "Config: {'hidden': 128, 'layers': 3, 'heads': 4, 'dropout': 0.32, 'lr': np.float64(0.0035595154708504538), 'wd': np.float64(0.0006812870082809862), 'readout': 's2s'}\n",
      "[trial 11 | fold 0 | ep 000] loss=15.7542  val_balacc=0.522  val_F1=0.167\n",
      "[trial 11 | fold 0 | ep 010] loss=0.7436  val_balacc=0.500  val_F1=0.000\n",
      "[trial 11 | fold 0 | ep 020] loss=0.7000  val_balacc=0.500  val_F1=0.000\n",
      "[trial 11 | fold 0 | ep 030] loss=0.7206  val_balacc=0.500  val_F1=0.000\n",
      "[trial 11 | fold 0 | ep 040] loss=0.6917  val_balacc=0.500  val_F1=0.526\n",
      "[trial 11 | fold 0 | ep 050] loss=0.6955  val_balacc=0.500  val_F1=0.526\n",
      "[trial 11 | fold 0 | ep 060] loss=0.6931  val_balacc=0.500  val_F1=0.526\n",
      "Early stop @ epoch 63 (fold 0). Best BA was 0.667.\n",
      "fold 0: best val_balacc=0.667, val_F1=0.625\n",
      "[trial 11 | fold 1 | ep 000] loss=13.4311  val_balacc=0.500  val_F1=0.500\n",
      "[trial 11 | fold 1 | ep 010] loss=0.8799  val_balacc=0.556  val_F1=0.529\n",
      "[trial 11 | fold 1 | ep 020] loss=0.6993  val_balacc=0.500  val_F1=0.500\n",
      "[trial 11 | fold 1 | ep 030] loss=0.6946  val_balacc=0.500  val_F1=0.500\n",
      "[trial 11 | fold 1 | ep 040] loss=0.6858  val_balacc=0.500  val_F1=0.500\n",
      "[trial 11 | fold 1 | ep 050] loss=0.6871  val_balacc=0.500  val_F1=0.500\n",
      "Early stop @ epoch 57 (fold 1). Best BA was 0.639.\n",
      "fold 1: best val_balacc=0.639, val_F1=0.500\n",
      "[trial 11 | fold 2 | ep 000] loss=19.7632  val_balacc=0.500  val_F1=0.000\n",
      "[trial 11 | fold 2 | ep 010] loss=0.7578  val_balacc=0.500  val_F1=0.000\n",
      "[trial 11 | fold 2 | ep 020] loss=0.7064  val_balacc=0.500  val_F1=0.541\n",
      "[trial 11 | fold 2 | ep 030] loss=0.7055  val_balacc=0.500  val_F1=0.541\n",
      "[trial 11 | fold 2 | ep 040] loss=0.6842  val_balacc=0.500  val_F1=0.541\n",
      "[trial 11 | fold 2 | ep 050] loss=0.6841  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 50 (fold 2). Best BA was 0.500.\n",
      "fold 2: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 11 | fold 3 | ep 000] loss=16.0637  val_balacc=0.500  val_F1=0.541\n",
      "[trial 11 | fold 3 | ep 010] loss=1.2361  val_balacc=0.500  val_F1=0.541\n",
      "[trial 11 | fold 3 | ep 020] loss=0.7053  val_balacc=0.553  val_F1=0.421\n",
      "[trial 11 | fold 3 | ep 030] loss=0.7150  val_balacc=0.565  val_F1=0.500\n",
      "[trial 11 | fold 3 | ep 040] loss=0.6982  val_balacc=0.565  val_F1=0.500\n",
      "[trial 11 | fold 3 | ep 050] loss=0.7190  val_balacc=0.565  val_F1=0.500\n",
      "[trial 11 | fold 3 | ep 060] loss=0.7131  val_balacc=0.565  val_F1=0.500\n",
      "Early stop @ epoch 66 (fold 3). Best BA was 0.594.\n",
      "fold 3: best val_balacc=0.594, val_F1=0.522\n",
      "[trial 11 | fold 4 | ep 000] loss=17.4975  val_balacc=0.500  val_F1=0.000\n",
      "[trial 11 | fold 4 | ep 010] loss=0.7723  val_balacc=0.500  val_F1=0.541\n",
      "[trial 11 | fold 4 | ep 020] loss=0.7609  val_balacc=0.500  val_F1=0.000\n",
      "[trial 11 | fold 4 | ep 030] loss=0.7080  val_balacc=0.415  val_F1=0.286\n",
      "[trial 11 | fold 4 | ep 040] loss=0.7159  val_balacc=0.512  val_F1=0.267\n",
      "[trial 11 | fold 4 | ep 050] loss=0.6838  val_balacc=0.512  val_F1=0.267\n",
      "[trial 11 | fold 4 | ep 060] loss=0.6882  val_balacc=0.462  val_F1=0.143\n",
      "Early stop @ epoch 61 (fold 4). Best BA was 0.600.\n",
      "fold 4: best val_balacc=0.600, val_F1=0.333\n",
      "TRIAL 11: mean bal-acc=0.6000.057, mean F1=0.3960.219\n",
      "\n",
      "--- Trial 12/50 ---\n",
      "Config: {'hidden': 128, 'layers': 2, 'heads': 6, 'dropout': 0.45, 'lr': np.float64(0.003184565176860959), 'wd': np.float64(0.000965325424625589), 'readout': 'mean'}\n",
      "[trial 12 | fold 0 | ep 000] loss=4.4900  val_balacc=0.500  val_F1=0.000\n",
      "[trial 12 | fold 0 | ep 010] loss=0.6913  val_balacc=0.500  val_F1=0.000\n",
      "[trial 12 | fold 0 | ep 020] loss=0.6947  val_balacc=0.500  val_F1=0.000\n",
      "[trial 12 | fold 0 | ep 030] loss=0.6952  val_balacc=0.500  val_F1=0.000\n",
      "[trial 12 | fold 0 | ep 040] loss=0.6928  val_balacc=0.500  val_F1=0.000\n",
      "[trial 12 | fold 0 | ep 050] loss=0.6952  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 0). Best BA was 0.500.\n",
      "fold 0: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 12 | fold 1 | ep 000] loss=4.1252  val_balacc=0.500  val_F1=0.000\n",
      "[trial 12 | fold 1 | ep 010] loss=0.6811  val_balacc=0.556  val_F1=0.308\n",
      "[trial 12 | fold 1 | ep 020] loss=0.6556  val_balacc=0.556  val_F1=0.375\n",
      "[trial 12 | fold 1 | ep 030] loss=0.6806  val_balacc=0.444  val_F1=0.385\n",
      "[trial 12 | fold 1 | ep 040] loss=0.6726  val_balacc=0.361  val_F1=0.345\n",
      "[trial 12 | fold 1 | ep 050] loss=0.6637  val_balacc=0.361  val_F1=0.345\n",
      "[trial 12 | fold 1 | ep 060] loss=0.6608  val_balacc=0.361  val_F1=0.345\n",
      "Early stop @ epoch 60 (fold 1). Best BA was 0.556.\n",
      "fold 1: best val_balacc=0.556, val_F1=0.308\n",
      "[trial 12 | fold 2 | ep 000] loss=3.9955  val_balacc=0.500  val_F1=0.000\n",
      "[trial 12 | fold 2 | ep 010] loss=0.6935  val_balacc=0.500  val_F1=0.000\n",
      "[trial 12 | fold 2 | ep 020] loss=0.6930  val_balacc=0.500  val_F1=0.000\n",
      "[trial 12 | fold 2 | ep 030] loss=0.6903  val_balacc=0.500  val_F1=0.000\n",
      "[trial 12 | fold 2 | ep 040] loss=0.6937  val_balacc=0.500  val_F1=0.000\n",
      "[trial 12 | fold 2 | ep 050] loss=0.6926  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 2). Best BA was 0.500.\n",
      "fold 2: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 12 | fold 3 | ep 000] loss=2.4387  val_balacc=0.500  val_F1=0.541\n",
      "[trial 12 | fold 3 | ep 010] loss=0.6947  val_balacc=0.500  val_F1=0.000\n",
      "[trial 12 | fold 3 | ep 020] loss=0.6984  val_balacc=0.529  val_F1=0.556\n",
      "[trial 12 | fold 3 | ep 030] loss=0.6923  val_balacc=0.326  val_F1=0.250\n",
      "[trial 12 | fold 3 | ep 040] loss=0.6941  val_balacc=0.326  val_F1=0.250\n",
      "[trial 12 | fold 3 | ep 050] loss=0.6861  val_balacc=0.597  val_F1=0.581\n",
      "[trial 12 | fold 3 | ep 060] loss=0.6922  val_balacc=0.547  val_F1=0.533\n",
      "[trial 12 | fold 3 | ep 070] loss=0.6930  val_balacc=0.547  val_F1=0.533\n",
      "[trial 12 | fold 3 | ep 080] loss=0.6942  val_balacc=0.547  val_F1=0.533\n",
      "Early stop @ epoch 84 (fold 3). Best BA was 0.618.\n",
      "fold 3: best val_balacc=0.618, val_F1=0.606\n",
      "[trial 12 | fold 4 | ep 000] loss=3.0174  val_balacc=0.500  val_F1=0.541\n",
      "[trial 12 | fold 4 | ep 010] loss=0.6877  val_balacc=0.521  val_F1=0.167\n",
      "[trial 12 | fold 4 | ep 020] loss=0.6820  val_balacc=0.500  val_F1=0.541\n",
      "[trial 12 | fold 4 | ep 030] loss=0.7231  val_balacc=0.500  val_F1=0.541\n",
      "[trial 12 | fold 4 | ep 040] loss=0.6609  val_balacc=0.479  val_F1=0.514\n",
      "[trial 12 | fold 4 | ep 050] loss=0.6623  val_balacc=0.459  val_F1=0.485\n",
      "[trial 12 | fold 4 | ep 060] loss=0.6608  val_balacc=0.459  val_F1=0.485\n",
      "[trial 12 | fold 4 | ep 070] loss=0.6369  val_balacc=0.359  val_F1=0.387\n",
      "Early stop @ epoch 71 (fold 4). Best BA was 0.529.\n",
      "fold 4: best val_balacc=0.529, val_F1=0.556\n",
      "TRIAL 12: mean bal-acc=0.5410.044, mean F1=0.2940.260\n",
      "\n",
      "--- Trial 13/50 ---\n",
      "Config: {'hidden': 128, 'layers': 2, 'heads': 2, 'dropout': 0.523, 'lr': np.float64(0.0008235808658662565), 'wd': np.float64(0.0015595439837018177), 'readout': 'att'}\n",
      "[trial 13 | fold 0 | ep 000] loss=3.5205  val_balacc=0.500  val_F1=0.000\n",
      "[trial 13 | fold 0 | ep 010] loss=1.5677  val_balacc=0.500  val_F1=0.000\n",
      "[trial 13 | fold 0 | ep 020] loss=1.2723  val_balacc=0.500  val_F1=0.000\n",
      "[trial 13 | fold 0 | ep 030] loss=1.1372  val_balacc=0.500  val_F1=0.000\n",
      "[trial 13 | fold 0 | ep 040] loss=1.3140  val_balacc=0.500  val_F1=0.000\n",
      "[trial 13 | fold 0 | ep 050] loss=1.4591  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 52 (fold 0). Best BA was 0.550.\n",
      "fold 0: best val_balacc=0.550, val_F1=0.182\n",
      "[trial 13 | fold 1 | ep 000] loss=4.8142  val_balacc=0.500  val_F1=0.000\n",
      "[trial 13 | fold 1 | ep 010] loss=2.1154  val_balacc=0.528  val_F1=0.514\n",
      "[trial 13 | fold 1 | ep 020] loss=1.0188  val_balacc=0.500  val_F1=0.000\n",
      "[trial 13 | fold 1 | ep 030] loss=1.0633  val_balacc=0.500  val_F1=0.000\n",
      "[trial 13 | fold 1 | ep 040] loss=0.8732  val_balacc=0.500  val_F1=0.000\n",
      "[trial 13 | fold 1 | ep 050] loss=0.7977  val_balacc=0.500  val_F1=0.000\n",
      "[trial 13 | fold 1 | ep 060] loss=0.9254  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 60 (fold 1). Best BA was 0.528.\n",
      "fold 1: best val_balacc=0.528, val_F1=0.514\n",
      "[trial 13 | fold 2 | ep 000] loss=3.0949  val_balacc=0.500  val_F1=0.541\n",
      "[trial 13 | fold 2 | ep 010] loss=0.7618  val_balacc=0.379  val_F1=0.424\n",
      "[trial 13 | fold 2 | ep 020] loss=0.7152  val_balacc=0.500  val_F1=0.541\n",
      "[trial 13 | fold 2 | ep 030] loss=0.6948  val_balacc=0.500  val_F1=0.541\n",
      "[trial 13 | fold 2 | ep 040] loss=0.6939  val_balacc=0.500  val_F1=0.541\n",
      "[trial 13 | fold 2 | ep 050] loss=0.7192  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 50 (fold 2). Best BA was 0.500.\n",
      "fold 2: best val_balacc=0.500, val_F1=0.541\n",
      "[trial 13 | fold 3 | ep 000] loss=2.8759  val_balacc=0.500  val_F1=0.541\n",
      "[trial 13 | fold 3 | ep 010] loss=2.1890  val_balacc=0.403  val_F1=0.125\n",
      "[trial 13 | fold 3 | ep 020] loss=1.3322  val_balacc=0.500  val_F1=0.541\n",
      "[trial 13 | fold 3 | ep 030] loss=1.4342  val_balacc=0.500  val_F1=0.541\n",
      "[trial 13 | fold 3 | ep 040] loss=1.4629  val_balacc=0.500  val_F1=0.541\n",
      "[trial 13 | fold 3 | ep 050] loss=1.3521  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 50 (fold 3). Best BA was 0.500.\n",
      "fold 3: best val_balacc=0.500, val_F1=0.541\n",
      "[trial 13 | fold 4 | ep 000] loss=3.4824  val_balacc=0.491  val_F1=0.154\n",
      "[trial 13 | fold 4 | ep 010] loss=1.4607  val_balacc=0.500  val_F1=0.000\n",
      "[trial 13 | fold 4 | ep 020] loss=0.7362  val_balacc=0.500  val_F1=0.541\n",
      "[trial 13 | fold 4 | ep 030] loss=0.7496  val_balacc=0.500  val_F1=0.000\n",
      "[trial 13 | fold 4 | ep 040] loss=0.7327  val_balacc=0.500  val_F1=0.000\n",
      "[trial 13 | fold 4 | ep 050] loss=0.7059  val_balacc=0.500  val_F1=0.000\n",
      "[trial 13 | fold 4 | ep 060] loss=0.7409  val_balacc=0.500  val_F1=0.000\n",
      "[trial 13 | fold 4 | ep 070] loss=0.6714  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 75 (fold 4). Best BA was 0.550.\n",
      "fold 4: best val_balacc=0.550, val_F1=0.182\n",
      "TRIAL 13: mean bal-acc=0.5260.022, mean F1=0.3920.172\n",
      "\n",
      "--- Trial 14/50 ---\n",
      "Config: {'hidden': 256, 'layers': 3, 'heads': 2, 'dropout': 0.599, 'lr': np.float64(0.00017179933768341109), 'wd': np.float64(0.00048529037085238665), 'readout': 'mean'}\n",
      "[trial 14 | fold 0 | ep 000] loss=2.3802  val_balacc=0.500  val_F1=0.526\n",
      "[trial 14 | fold 0 | ep 010] loss=1.1695  val_balacc=0.500  val_F1=0.526\n",
      "[trial 14 | fold 0 | ep 020] loss=0.8680  val_balacc=0.500  val_F1=0.526\n",
      "[trial 14 | fold 0 | ep 030] loss=0.8918  val_balacc=0.456  val_F1=0.471\n",
      "[trial 14 | fold 0 | ep 040] loss=0.7806  val_balacc=0.600  val_F1=0.333\n",
      "[trial 14 | fold 0 | ep 050] loss=0.8100  val_balacc=0.550  val_F1=0.182\n",
      "[trial 14 | fold 0 | ep 060] loss=0.8288  val_balacc=0.500  val_F1=0.000\n",
      "[trial 14 | fold 0 | ep 070] loss=0.7684  val_balacc=0.500  val_F1=0.000\n",
      "[trial 14 | fold 0 | ep 080] loss=0.8241  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 81 (fold 0). Best BA was 0.600.\n",
      "fold 0: best val_balacc=0.600, val_F1=0.333\n",
      "[trial 14 | fold 1 | ep 000] loss=2.9782  val_balacc=0.500  val_F1=0.500\n",
      "[trial 14 | fold 1 | ep 010] loss=0.9720  val_balacc=0.472  val_F1=0.000\n",
      "[trial 14 | fold 1 | ep 020] loss=0.9233  val_balacc=0.472  val_F1=0.000\n",
      "[trial 14 | fold 1 | ep 030] loss=0.9215  val_balacc=0.444  val_F1=0.000\n",
      "[trial 14 | fold 1 | ep 040] loss=0.8629  val_balacc=0.444  val_F1=0.000\n",
      "[trial 14 | fold 1 | ep 050] loss=0.7140  val_balacc=0.417  val_F1=0.000\n",
      "[trial 14 | fold 1 | ep 060] loss=0.7360  val_balacc=0.444  val_F1=0.000\n",
      "Early stop @ epoch 61 (fold 1). Best BA was 0.639.\n",
      "fold 1: best val_balacc=0.639, val_F1=0.526\n",
      "[trial 14 | fold 2 | ep 000] loss=2.6210  val_balacc=0.500  val_F1=0.541\n",
      "[trial 14 | fold 2 | ep 010] loss=1.2569  val_balacc=0.500  val_F1=0.000\n",
      "[trial 14 | fold 2 | ep 020] loss=0.8660  val_balacc=0.500  val_F1=0.000\n",
      "[trial 14 | fold 2 | ep 030] loss=0.8274  val_balacc=0.488  val_F1=0.500\n",
      "[trial 14 | fold 2 | ep 040] loss=0.7576  val_balacc=0.476  val_F1=0.444\n",
      "[trial 14 | fold 2 | ep 050] loss=0.7912  val_balacc=0.521  val_F1=0.167\n",
      "[trial 14 | fold 2 | ep 060] loss=0.7289  val_balacc=0.521  val_F1=0.167\n",
      "[trial 14 | fold 2 | ep 070] loss=0.7405  val_balacc=0.521  val_F1=0.167\n",
      "[trial 14 | fold 2 | ep 080] loss=0.6893  val_balacc=0.521  val_F1=0.167\n",
      "Early stop @ epoch 87 (fold 2). Best BA was 0.603.\n",
      "fold 2: best val_balacc=0.603, val_F1=0.500\n",
      "[trial 14 | fold 3 | ep 000] loss=2.2584  val_balacc=0.500  val_F1=0.000\n",
      "[trial 14 | fold 3 | ep 010] loss=1.2608  val_balacc=0.612  val_F1=0.471\n",
      "[trial 14 | fold 3 | ep 020] loss=1.0113  val_balacc=0.503  val_F1=0.333\n",
      "[trial 14 | fold 3 | ep 030] loss=0.8467  val_balacc=0.500  val_F1=0.541\n",
      "[trial 14 | fold 3 | ep 040] loss=0.7509  val_balacc=0.500  val_F1=0.541\n",
      "[trial 14 | fold 3 | ep 050] loss=0.6899  val_balacc=0.500  val_F1=0.541\n",
      "[trial 14 | fold 3 | ep 060] loss=0.8126  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 60 (fold 3). Best BA was 0.612.\n",
      "fold 3: best val_balacc=0.612, val_F1=0.471\n",
      "[trial 14 | fold 4 | ep 000] loss=2.7075  val_balacc=0.500  val_F1=0.000\n",
      "[trial 14 | fold 4 | ep 010] loss=1.4023  val_balacc=0.500  val_F1=0.541\n",
      "[trial 14 | fold 4 | ep 020] loss=0.7773  val_balacc=0.500  val_F1=0.541\n",
      "[trial 14 | fold 4 | ep 030] loss=0.7788  val_balacc=0.500  val_F1=0.541\n",
      "[trial 14 | fold 4 | ep 040] loss=0.7980  val_balacc=0.500  val_F1=0.541\n",
      "[trial 14 | fold 4 | ep 050] loss=0.7491  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 58 (fold 4). Best BA was 0.526.\n",
      "fold 4: best val_balacc=0.526, val_F1=0.500\n",
      "TRIAL 14: mean bal-acc=0.5960.037, mean F1=0.4660.069\n",
      "\n",
      "--- Trial 15/50 ---\n",
      "Config: {'hidden': 32, 'layers': 2, 'heads': 6, 'dropout': 0.348, 'lr': np.float64(0.001432574347760321), 'wd': np.float64(0.0007794496489750811), 'readout': 'att'}\n",
      "[trial 15 | fold 0 | ep 000] loss=0.9934  val_balacc=0.667  val_F1=0.556\n",
      "[trial 15 | fold 0 | ep 010] loss=0.6960  val_balacc=0.500  val_F1=0.000\n",
      "[trial 15 | fold 0 | ep 020] loss=0.6896  val_balacc=0.500  val_F1=0.000\n",
      "[trial 15 | fold 0 | ep 030] loss=0.6923  val_balacc=0.500  val_F1=0.000\n",
      "[trial 15 | fold 0 | ep 040] loss=0.6902  val_balacc=0.500  val_F1=0.000\n",
      "[trial 15 | fold 0 | ep 050] loss=0.6869  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 0). Best BA was 0.667.\n",
      "fold 0: best val_balacc=0.667, val_F1=0.556\n",
      "[trial 15 | fold 1 | ep 000] loss=0.7084  val_balacc=0.500  val_F1=0.500\n",
      "[trial 15 | fold 1 | ep 010] loss=0.6848  val_balacc=0.500  val_F1=0.500\n",
      "[trial 15 | fold 1 | ep 020] loss=0.6916  val_balacc=0.500  val_F1=0.500\n",
      "[trial 15 | fold 1 | ep 030] loss=0.6922  val_balacc=0.500  val_F1=0.500\n",
      "[trial 15 | fold 1 | ep 040] loss=0.7021  val_balacc=0.500  val_F1=0.500\n",
      "[trial 15 | fold 1 | ep 050] loss=0.6857  val_balacc=0.500  val_F1=0.500\n",
      "Early stop @ epoch 50 (fold 1). Best BA was 0.500.\n",
      "fold 1: best val_balacc=0.500, val_F1=0.500\n",
      "[trial 15 | fold 2 | ep 000] loss=0.8843  val_balacc=0.500  val_F1=0.541\n",
      "[trial 15 | fold 2 | ep 010] loss=0.7030  val_balacc=0.409  val_F1=0.438\n",
      "[trial 15 | fold 2 | ep 020] loss=0.6938  val_balacc=0.500  val_F1=0.541\n",
      "[trial 15 | fold 2 | ep 030] loss=0.6940  val_balacc=0.500  val_F1=0.541\n",
      "[trial 15 | fold 2 | ep 040] loss=0.6926  val_balacc=0.500  val_F1=0.541\n",
      "[trial 15 | fold 2 | ep 050] loss=0.6896  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 50 (fold 2). Best BA was 0.500.\n",
      "fold 2: best val_balacc=0.500, val_F1=0.541\n",
      "[trial 15 | fold 3 | ep 000] loss=1.1313  val_balacc=0.500  val_F1=0.541\n",
      "[trial 15 | fold 3 | ep 010] loss=0.6938  val_balacc=0.500  val_F1=0.541\n",
      "[trial 15 | fold 3 | ep 020] loss=0.6895  val_balacc=0.494  val_F1=0.381\n",
      "[trial 15 | fold 3 | ep 030] loss=0.6946  val_balacc=0.500  val_F1=0.541\n",
      "[trial 15 | fold 3 | ep 040] loss=0.6906  val_balacc=0.500  val_F1=0.541\n",
      "[trial 15 | fold 3 | ep 050] loss=0.6889  val_balacc=0.500  val_F1=0.541\n",
      "[trial 15 | fold 3 | ep 060] loss=0.6871  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 62 (fold 3). Best BA was 0.538.\n",
      "fold 3: best val_balacc=0.538, val_F1=0.545\n",
      "[trial 15 | fold 4 | ep 000] loss=1.2313  val_balacc=0.500  val_F1=0.541\n",
      "[trial 15 | fold 4 | ep 010] loss=0.6936  val_balacc=0.500  val_F1=0.541\n",
      "[trial 15 | fold 4 | ep 020] loss=0.6931  val_balacc=0.441  val_F1=0.000\n",
      "[trial 15 | fold 4 | ep 030] loss=0.6897  val_balacc=0.491  val_F1=0.154\n",
      "[trial 15 | fold 4 | ep 040] loss=0.6974  val_balacc=0.491  val_F1=0.154\n",
      "[trial 15 | fold 4 | ep 050] loss=0.6968  val_balacc=0.441  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 4). Best BA was 0.500.\n",
      "fold 4: best val_balacc=0.500, val_F1=0.541\n",
      "TRIAL 15: mean bal-acc=0.5410.065, mean F1=0.5360.019\n",
      "\n",
      "--- Trial 16/50 ---\n",
      "Config: {'hidden': 256, 'layers': 4, 'heads': 4, 'dropout': 0.459, 'lr': np.float64(0.004465118975867086), 'wd': np.float64(0.00263347626684356), 'readout': 'mean'}\n",
      "[trial 16 | fold 0 | ep 000] loss=22.1466  val_balacc=0.500  val_F1=0.000\n",
      "[trial 16 | fold 0 | ep 010] loss=0.6967  val_balacc=0.500  val_F1=0.000\n",
      "[trial 16 | fold 0 | ep 020] loss=0.6947  val_balacc=0.500  val_F1=0.000\n",
      "[trial 16 | fold 0 | ep 030] loss=0.6940  val_balacc=0.500  val_F1=0.000\n",
      "[trial 16 | fold 0 | ep 040] loss=0.6881  val_balacc=0.500  val_F1=0.000\n",
      "[trial 16 | fold 0 | ep 050] loss=0.6930  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 0). Best BA was 0.500.\n",
      "fold 0: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 16 | fold 1 | ep 000] loss=101.1529  val_balacc=0.500  val_F1=0.000\n",
      "[trial 16 | fold 1 | ep 010] loss=0.6944  val_balacc=0.500  val_F1=0.000\n",
      "[trial 16 | fold 1 | ep 020] loss=0.6948  val_balacc=0.500  val_F1=0.000\n",
      "[trial 16 | fold 1 | ep 030] loss=0.6948  val_balacc=0.500  val_F1=0.000\n",
      "[trial 16 | fold 1 | ep 040] loss=0.6917  val_balacc=0.500  val_F1=0.000\n",
      "[trial 16 | fold 1 | ep 050] loss=0.6933  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 1). Best BA was 0.500.\n",
      "fold 1: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 16 | fold 2 | ep 000] loss=22.4559  val_balacc=0.500  val_F1=0.541\n",
      "[trial 16 | fold 2 | ep 010] loss=0.6929  val_balacc=0.500  val_F1=0.000\n",
      "[trial 16 | fold 2 | ep 020] loss=0.6944  val_balacc=0.500  val_F1=0.000\n",
      "[trial 16 | fold 2 | ep 030] loss=0.6930  val_balacc=0.500  val_F1=0.000\n",
      "[trial 16 | fold 2 | ep 040] loss=0.6939  val_balacc=0.500  val_F1=0.000\n",
      "[trial 16 | fold 2 | ep 050] loss=0.6949  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 2). Best BA was 0.500.\n",
      "fold 2: best val_balacc=0.500, val_F1=0.541\n",
      "[trial 16 | fold 3 | ep 000] loss=99.3747  val_balacc=0.500  val_F1=0.000\n",
      "[trial 16 | fold 3 | ep 010] loss=0.6917  val_balacc=0.500  val_F1=0.000\n",
      "[trial 16 | fold 3 | ep 020] loss=0.6921  val_balacc=0.500  val_F1=0.000\n",
      "[trial 16 | fold 3 | ep 030] loss=0.6884  val_balacc=0.500  val_F1=0.000\n",
      "[trial 16 | fold 3 | ep 040] loss=0.6964  val_balacc=0.500  val_F1=0.000\n",
      "[trial 16 | fold 3 | ep 050] loss=0.6923  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 3). Best BA was 0.500.\n",
      "fold 3: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 16 | fold 4 | ep 000] loss=77.7697  val_balacc=0.500  val_F1=0.000\n",
      "[trial 16 | fold 4 | ep 010] loss=0.6925  val_balacc=0.500  val_F1=0.000\n",
      "[trial 16 | fold 4 | ep 020] loss=0.6939  val_balacc=0.500  val_F1=0.000\n",
      "[trial 16 | fold 4 | ep 030] loss=0.6958  val_balacc=0.500  val_F1=0.000\n",
      "[trial 16 | fold 4 | ep 040] loss=0.6947  val_balacc=0.500  val_F1=0.000\n",
      "[trial 16 | fold 4 | ep 050] loss=0.6941  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 4). Best BA was 0.500.\n",
      "fold 4: best val_balacc=0.500, val_F1=0.000\n",
      "TRIAL 16: mean bal-acc=0.5000.000, mean F1=0.1080.216\n",
      "\n",
      "--- Trial 17/50 ---\n",
      "Config: {'hidden': 32, 'layers': 4, 'heads': 6, 'dropout': 0.525, 'lr': np.float64(0.002022209713518744), 'wd': np.float64(0.00023950851077991634), 'readout': 'att'}\n",
      "[trial 17 | fold 0 | ep 000] loss=50.6289  val_balacc=0.500  val_F1=0.000\n",
      "[trial 17 | fold 0 | ep 010] loss=15.1507  val_balacc=0.500  val_F1=0.526\n",
      "[trial 17 | fold 0 | ep 020] loss=7.1843  val_balacc=0.517  val_F1=0.267\n",
      "[trial 17 | fold 0 | ep 030] loss=4.3001  val_balacc=0.500  val_F1=0.000\n",
      "[trial 17 | fold 0 | ep 040] loss=3.9355  val_balacc=0.500  val_F1=0.526\n",
      "[trial 17 | fold 0 | ep 050] loss=3.0803  val_balacc=0.500  val_F1=0.526\n",
      "[trial 17 | fold 0 | ep 060] loss=3.2658  val_balacc=0.500  val_F1=0.526\n",
      "[trial 17 | fold 0 | ep 070] loss=2.7844  val_balacc=0.500  val_F1=0.526\n",
      "Early stop @ epoch 75 (fold 0). Best BA was 0.683.\n",
      "fold 0: best val_balacc=0.683, val_F1=0.609\n",
      "[trial 17 | fold 1 | ep 000] loss=48.1405  val_balacc=0.500  val_F1=0.000\n",
      "[trial 17 | fold 1 | ep 010] loss=16.3879  val_balacc=0.500  val_F1=0.000\n",
      "[trial 17 | fold 1 | ep 020] loss=13.1295  val_balacc=0.500  val_F1=0.000\n",
      "[trial 17 | fold 1 | ep 030] loss=9.1535  val_balacc=0.500  val_F1=0.000\n",
      "[trial 17 | fold 1 | ep 040] loss=10.2923  val_balacc=0.500  val_F1=0.000\n",
      "[trial 17 | fold 1 | ep 050] loss=8.9663  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 1). Best BA was 0.500.\n",
      "fold 1: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 17 | fold 2 | ep 000] loss=47.9284  val_balacc=0.500  val_F1=0.541\n",
      "[trial 17 | fold 2 | ep 010] loss=18.1018  val_balacc=0.500  val_F1=0.000\n",
      "[trial 17 | fold 2 | ep 020] loss=12.7443  val_balacc=0.409  val_F1=0.438\n",
      "[trial 17 | fold 2 | ep 030] loss=10.1671  val_balacc=0.500  val_F1=0.541\n",
      "[trial 17 | fold 2 | ep 040] loss=12.5779  val_balacc=0.500  val_F1=0.541\n",
      "[trial 17 | fold 2 | ep 050] loss=11.8673  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 50 (fold 2). Best BA was 0.500.\n",
      "fold 2: best val_balacc=0.500, val_F1=0.541\n",
      "[trial 17 | fold 3 | ep 000] loss=47.8295  val_balacc=0.500  val_F1=0.541\n",
      "[trial 17 | fold 3 | ep 010] loss=17.1176  val_balacc=0.500  val_F1=0.000\n",
      "[trial 17 | fold 3 | ep 020] loss=11.3469  val_balacc=0.412  val_F1=0.000\n",
      "[trial 17 | fold 3 | ep 030] loss=7.1724  val_balacc=0.500  val_F1=0.541\n",
      "[trial 17 | fold 3 | ep 040] loss=7.7418  val_balacc=0.500  val_F1=0.541\n",
      "[trial 17 | fold 3 | ep 050] loss=6.1188  val_balacc=0.500  val_F1=0.541\n",
      "[trial 17 | fold 3 | ep 060] loss=6.0821  val_balacc=0.500  val_F1=0.541\n",
      "[trial 17 | fold 3 | ep 070] loss=6.9361  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 72 (fold 3). Best BA was 0.503.\n",
      "fold 3: best val_balacc=0.503, val_F1=0.333\n",
      "[trial 17 | fold 4 | ep 000] loss=60.6864  val_balacc=0.500  val_F1=0.000\n",
      "[trial 17 | fold 4 | ep 010] loss=26.7734  val_balacc=0.500  val_F1=0.541\n",
      "[trial 17 | fold 4 | ep 020] loss=17.9289  val_balacc=0.500  val_F1=0.541\n",
      "[trial 17 | fold 4 | ep 030] loss=12.6001  val_balacc=0.500  val_F1=0.541\n",
      "[trial 17 | fold 4 | ep 040] loss=13.0697  val_balacc=0.500  val_F1=0.541\n",
      "[trial 17 | fold 4 | ep 050] loss=13.9649  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 50 (fold 4). Best BA was 0.500.\n",
      "fold 4: best val_balacc=0.500, val_F1=0.000\n",
      "TRIAL 17: mean bal-acc=0.5370.073, mean F1=0.2970.259\n",
      "\n",
      "--- Trial 18/50 ---\n",
      "Config: {'hidden': 256, 'layers': 2, 'heads': 4, 'dropout': 0.301, 'lr': np.float64(0.0016858260193540373), 'wd': np.float64(0.0013750906912709815), 'readout': 's2s'}\n",
      "[trial 18 | fold 0 | ep 000] loss=1.0953  val_balacc=0.500  val_F1=0.000\n",
      "[trial 18 | fold 0 | ep 010] loss=0.6959  val_balacc=0.500  val_F1=0.000\n",
      "[trial 18 | fold 0 | ep 020] loss=0.6933  val_balacc=0.500  val_F1=0.000\n",
      "[trial 18 | fold 0 | ep 030] loss=0.6937  val_balacc=0.500  val_F1=0.000\n",
      "[trial 18 | fold 0 | ep 040] loss=0.6927  val_balacc=0.500  val_F1=0.000\n",
      "[trial 18 | fold 0 | ep 050] loss=0.6937  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 0). Best BA was 0.500.\n",
      "fold 0: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 18 | fold 1 | ep 000] loss=0.9640  val_balacc=0.500  val_F1=0.500\n",
      "[trial 18 | fold 1 | ep 010] loss=0.7041  val_balacc=0.500  val_F1=0.500\n",
      "[trial 18 | fold 1 | ep 020] loss=0.6930  val_balacc=0.500  val_F1=0.000\n",
      "[trial 18 | fold 1 | ep 030] loss=0.6985  val_balacc=0.500  val_F1=0.000\n",
      "[trial 18 | fold 1 | ep 040] loss=0.6954  val_balacc=0.500  val_F1=0.000\n",
      "[trial 18 | fold 1 | ep 050] loss=0.7035  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 1). Best BA was 0.500.\n",
      "fold 1: best val_balacc=0.500, val_F1=0.500\n",
      "[trial 18 | fold 2 | ep 000] loss=1.0952  val_balacc=0.500  val_F1=0.541\n",
      "[trial 18 | fold 2 | ep 010] loss=0.7009  val_balacc=0.500  val_F1=0.541\n",
      "[trial 18 | fold 2 | ep 020] loss=0.6922  val_balacc=0.388  val_F1=0.400\n",
      "[trial 18 | fold 2 | ep 030] loss=0.6914  val_balacc=0.485  val_F1=0.417\n",
      "[trial 18 | fold 2 | ep 040] loss=0.6950  val_balacc=0.435  val_F1=0.348\n",
      "[trial 18 | fold 2 | ep 050] loss=0.6923  val_balacc=0.435  val_F1=0.348\n",
      "[trial 18 | fold 2 | ep 060] loss=0.6910  val_balacc=0.435  val_F1=0.348\n",
      "Early stop @ epoch 62 (fold 2). Best BA was 0.541.\n",
      "fold 2: best val_balacc=0.541, val_F1=0.286\n",
      "[trial 18 | fold 3 | ep 000] loss=8.8634  val_balacc=0.415  val_F1=0.286\n",
      "[trial 18 | fold 3 | ep 010] loss=0.7522  val_balacc=0.500  val_F1=0.541\n",
      "[trial 18 | fold 3 | ep 020] loss=0.6866  val_balacc=0.432  val_F1=0.133\n",
      "[trial 18 | fold 3 | ep 030] loss=0.6775  val_balacc=0.424  val_F1=0.222\n",
      "[trial 18 | fold 3 | ep 040] loss=0.6873  val_balacc=0.403  val_F1=0.125\n",
      "[trial 18 | fold 3 | ep 050] loss=0.6914  val_balacc=0.403  val_F1=0.125\n",
      "[trial 18 | fold 3 | ep 060] loss=0.6874  val_balacc=0.403  val_F1=0.125\n",
      "Early stop @ epoch 66 (fold 3). Best BA was 0.550.\n",
      "fold 3: best val_balacc=0.550, val_F1=0.182\n",
      "[trial 18 | fold 4 | ep 000] loss=2.4018  val_balacc=0.500  val_F1=0.541\n",
      "[trial 18 | fold 4 | ep 010] loss=0.7166  val_balacc=0.500  val_F1=0.000\n",
      "[trial 18 | fold 4 | ep 020] loss=0.6996  val_balacc=0.500  val_F1=0.000\n",
      "[trial 18 | fold 4 | ep 030] loss=0.7044  val_balacc=0.500  val_F1=0.000\n",
      "[trial 18 | fold 4 | ep 040] loss=0.6915  val_balacc=0.500  val_F1=0.000\n",
      "[trial 18 | fold 4 | ep 050] loss=0.7138  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 4). Best BA was 0.500.\n",
      "fold 4: best val_balacc=0.500, val_F1=0.541\n",
      "TRIAL 18: mean bal-acc=0.5180.023, mean F1=0.3020.201\n",
      "\n",
      "--- Trial 19/50 ---\n",
      "Config: {'hidden': 64, 'layers': 4, 'heads': 2, 'dropout': 0.561, 'lr': np.float64(0.00032140212853702275), 'wd': np.float64(0.0009481324138302528), 'readout': 's2s'}\n",
      "[trial 19 | fold 0 | ep 000] loss=119.3187  val_balacc=0.500  val_F1=0.000\n",
      "[trial 19 | fold 0 | ep 010] loss=58.3487  val_balacc=0.500  val_F1=0.526\n",
      "[trial 19 | fold 0 | ep 020] loss=44.1899  val_balacc=0.478  val_F1=0.500\n",
      "[trial 19 | fold 0 | ep 030] loss=60.2097  val_balacc=0.528  val_F1=0.541\n",
      "[trial 19 | fold 0 | ep 040] loss=55.8680  val_balacc=0.528  val_F1=0.541\n",
      "[trial 19 | fold 0 | ep 050] loss=51.9149  val_balacc=0.500  val_F1=0.526\n",
      "[trial 19 | fold 0 | ep 060] loss=50.2091  val_balacc=0.500  val_F1=0.526\n",
      "Early stop @ epoch 62 (fold 0). Best BA was 0.611.\n",
      "fold 0: best val_balacc=0.611, val_F1=0.588\n",
      "[trial 19 | fold 1 | ep 000] loss=92.0839  val_balacc=0.389  val_F1=0.357\n",
      "[trial 19 | fold 1 | ep 010] loss=61.2641  val_balacc=0.611  val_F1=0.471\n",
      "[trial 19 | fold 1 | ep 020] loss=55.8090  val_balacc=0.611  val_F1=0.364\n",
      "[trial 19 | fold 1 | ep 030] loss=62.9352  val_balacc=0.500  val_F1=0.000\n",
      "[trial 19 | fold 1 | ep 040] loss=49.8727  val_balacc=0.500  val_F1=0.000\n",
      "[trial 19 | fold 1 | ep 050] loss=62.2165  val_balacc=0.500  val_F1=0.000\n",
      "[trial 19 | fold 1 | ep 060] loss=46.3988  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 66 (fold 1). Best BA was 0.694.\n",
      "fold 1: best val_balacc=0.694, val_F1=0.588\n",
      "[trial 19 | fold 2 | ep 000] loss=115.9804  val_balacc=0.471  val_F1=0.000\n",
      "[trial 19 | fold 2 | ep 010] loss=75.2844  val_balacc=0.500  val_F1=0.000\n",
      "[trial 19 | fold 2 | ep 020] loss=50.6524  val_balacc=0.500  val_F1=0.000\n",
      "[trial 19 | fold 2 | ep 030] loss=36.1828  val_balacc=0.550  val_F1=0.182\n",
      "[trial 19 | fold 2 | ep 040] loss=37.6125  val_balacc=0.500  val_F1=0.541\n",
      "[trial 19 | fold 2 | ep 050] loss=33.5646  val_balacc=0.500  val_F1=0.541\n",
      "[trial 19 | fold 2 | ep 060] loss=34.2605  val_balacc=0.500  val_F1=0.541\n",
      "[trial 19 | fold 2 | ep 070] loss=55.2933  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 71 (fold 2). Best BA was 0.568.\n",
      "fold 2: best val_balacc=0.568, val_F1=0.562\n",
      "[trial 19 | fold 3 | ep 000] loss=94.3307  val_balacc=0.500  val_F1=0.000\n",
      "[trial 19 | fold 3 | ep 010] loss=58.4692  val_balacc=0.424  val_F1=0.222\n",
      "[trial 19 | fold 3 | ep 020] loss=43.6546  val_balacc=0.297  val_F1=0.240\n",
      "[trial 19 | fold 3 | ep 030] loss=54.1123  val_balacc=0.444  val_F1=0.300\n",
      "[trial 19 | fold 3 | ep 040] loss=41.4275  val_balacc=0.403  val_F1=0.125\n",
      "[trial 19 | fold 3 | ep 050] loss=40.4180  val_balacc=0.435  val_F1=0.348\n",
      "[trial 19 | fold 3 | ep 060] loss=39.6351  val_balacc=0.435  val_F1=0.348\n",
      "Early stop @ epoch 69 (fold 3). Best BA was 0.529.\n",
      "fold 3: best val_balacc=0.529, val_F1=0.556\n",
      "[trial 19 | fold 4 | ep 000] loss=76.9582  val_balacc=0.500  val_F1=0.000\n",
      "[trial 19 | fold 4 | ep 010] loss=89.7042  val_balacc=0.500  val_F1=0.000\n",
      "[trial 19 | fold 4 | ep 020] loss=43.1408  val_balacc=0.500  val_F1=0.000\n",
      "[trial 19 | fold 4 | ep 030] loss=46.0154  val_balacc=0.500  val_F1=0.000\n",
      "[trial 19 | fold 4 | ep 040] loss=47.6232  val_balacc=0.500  val_F1=0.000\n",
      "[trial 19 | fold 4 | ep 050] loss=59.8526  val_balacc=0.500  val_F1=0.000\n",
      "[trial 19 | fold 4 | ep 060] loss=41.0647  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 62 (fold 4). Best BA was 0.600.\n",
      "fold 4: best val_balacc=0.600, val_F1=0.333\n",
      "TRIAL 19: mean bal-acc=0.6010.055, mean F1=0.5260.097\n",
      "\n",
      "--- Trial 20/50 ---\n",
      "Config: {'hidden': 64, 'layers': 2, 'heads': 4, 'dropout': 0.529, 'lr': np.float64(0.0008248762379386861), 'wd': np.float64(0.001803943626270354), 'readout': 's2s'}\n",
      "[trial 20 | fold 0 | ep 000] loss=2.6012  val_balacc=0.500  val_F1=0.526\n",
      "[trial 20 | fold 0 | ep 010] loss=1.9795  val_balacc=0.567  val_F1=0.533\n",
      "[trial 20 | fold 0 | ep 020] loss=1.3845  val_balacc=0.456  val_F1=0.471\n",
      "[trial 20 | fold 0 | ep 030] loss=1.0438  val_balacc=0.500  val_F1=0.526\n",
      "[trial 20 | fold 0 | ep 040] loss=0.8769  val_balacc=0.500  val_F1=0.526\n",
      "[trial 20 | fold 0 | ep 050] loss=0.9805  val_balacc=0.500  val_F1=0.526\n",
      "[trial 20 | fold 0 | ep 060] loss=0.9028  val_balacc=0.500  val_F1=0.526\n",
      "[trial 20 | fold 0 | ep 070] loss=0.8982  val_balacc=0.500  val_F1=0.526\n",
      "Early stop @ epoch 75 (fold 0). Best BA was 0.656.\n",
      "fold 0: best val_balacc=0.656, val_F1=0.583\n",
      "[trial 20 | fold 1 | ep 000] loss=2.3607  val_balacc=0.500  val_F1=0.500\n",
      "[trial 20 | fold 1 | ep 010] loss=1.7673  val_balacc=0.472  val_F1=0.400\n",
      "[trial 20 | fold 1 | ep 020] loss=1.2861  val_balacc=0.500  val_F1=0.500\n",
      "[trial 20 | fold 1 | ep 030] loss=1.4096  val_balacc=0.500  val_F1=0.500\n",
      "[trial 20 | fold 1 | ep 040] loss=1.0404  val_balacc=0.528  val_F1=0.514\n",
      "[trial 20 | fold 1 | ep 050] loss=1.0500  val_balacc=0.528  val_F1=0.514\n",
      "Early stop @ epoch 59 (fold 1). Best BA was 0.556.\n",
      "fold 1: best val_balacc=0.556, val_F1=0.500\n",
      "[trial 20 | fold 2 | ep 000] loss=2.3747  val_balacc=0.500  val_F1=0.541\n",
      "[trial 20 | fold 2 | ep 010] loss=1.7676  val_balacc=0.500  val_F1=0.541\n",
      "[trial 20 | fold 2 | ep 020] loss=1.6292  val_balacc=0.500  val_F1=0.541\n",
      "[trial 20 | fold 2 | ep 030] loss=1.3395  val_balacc=0.500  val_F1=0.541\n",
      "[trial 20 | fold 2 | ep 040] loss=1.4808  val_balacc=0.500  val_F1=0.541\n",
      "[trial 20 | fold 2 | ep 050] loss=1.3084  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 53 (fold 2). Best BA was 0.568.\n",
      "fold 2: best val_balacc=0.568, val_F1=0.562\n",
      "[trial 20 | fold 3 | ep 000] loss=2.9174  val_balacc=0.497  val_F1=0.483\n",
      "[trial 20 | fold 3 | ep 010] loss=1.5204  val_balacc=0.500  val_F1=0.541\n",
      "[trial 20 | fold 3 | ep 020] loss=1.2240  val_balacc=0.500  val_F1=0.541\n",
      "[trial 20 | fold 3 | ep 030] loss=1.0790  val_balacc=0.500  val_F1=0.541\n",
      "[trial 20 | fold 3 | ep 040] loss=0.9932  val_balacc=0.500  val_F1=0.541\n",
      "[trial 20 | fold 3 | ep 050] loss=1.0492  val_balacc=0.500  val_F1=0.541\n",
      "[trial 20 | fold 3 | ep 060] loss=1.0639  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 61 (fold 3). Best BA was 0.529.\n",
      "fold 3: best val_balacc=0.529, val_F1=0.556\n",
      "[trial 20 | fold 4 | ep 000] loss=2.0402  val_balacc=0.562  val_F1=0.375\n",
      "[trial 20 | fold 4 | ep 010] loss=2.1642  val_balacc=0.476  val_F1=0.444\n",
      "[trial 20 | fold 4 | ep 020] loss=1.5683  val_balacc=0.526  val_F1=0.500\n",
      "[trial 20 | fold 4 | ep 030] loss=1.1454  val_balacc=0.568  val_F1=0.562\n",
      "[trial 20 | fold 4 | ep 040] loss=0.9673  val_balacc=0.518  val_F1=0.516\n",
      "[trial 20 | fold 4 | ep 050] loss=0.8840  val_balacc=0.618  val_F1=0.606\n",
      "Early stop @ epoch 56 (fold 4). Best BA was 0.685.\n",
      "fold 4: best val_balacc=0.685, val_F1=0.643\n",
      "TRIAL 20: mean bal-acc=0.5990.061, mean F1=0.5690.046\n",
      "\n",
      "--- Trial 21/50 ---\n",
      "Config: {'hidden': 32, 'layers': 4, 'heads': 4, 'dropout': 0.447, 'lr': np.float64(0.0001549030288800107), 'wd': np.float64(0.00026603056615492367), 'readout': 'att'}\n",
      "[trial 21 | fold 0 | ep 000] loss=13.7348  val_balacc=0.500  val_F1=0.526\n",
      "[trial 21 | fold 0 | ep 010] loss=13.4509  val_balacc=0.572  val_F1=0.308\n",
      "[trial 21 | fold 0 | ep 020] loss=11.6682  val_balacc=0.500  val_F1=0.526\n",
      "[trial 21 | fold 0 | ep 030] loss=10.7584  val_balacc=0.544  val_F1=0.286\n",
      "[trial 21 | fold 0 | ep 040] loss=10.3915  val_balacc=0.500  val_F1=0.526\n",
      "[trial 21 | fold 0 | ep 050] loss=10.7278  val_balacc=0.617  val_F1=0.581\n",
      "[trial 21 | fold 0 | ep 060] loss=10.4918  val_balacc=0.589  val_F1=0.562\n",
      "Early stop @ epoch 63 (fold 0). Best BA was 0.739.\n",
      "fold 0: best val_balacc=0.739, val_F1=0.667\n",
      "[trial 21 | fold 1 | ep 000] loss=17.3637  val_balacc=0.500  val_F1=0.500\n",
      "[trial 21 | fold 1 | ep 010] loss=10.6236  val_balacc=0.500  val_F1=0.500\n",
      "[trial 21 | fold 1 | ep 020] loss=10.4758  val_balacc=0.500  val_F1=0.500\n",
      "[trial 21 | fold 1 | ep 030] loss=16.3362  val_balacc=0.500  val_F1=0.500\n",
      "[trial 21 | fold 1 | ep 040] loss=14.4109  val_balacc=0.500  val_F1=0.500\n",
      "[trial 21 | fold 1 | ep 050] loss=11.7165  val_balacc=0.500  val_F1=0.500\n",
      "Early stop @ epoch 50 (fold 1). Best BA was 0.500.\n",
      "fold 1: best val_balacc=0.500, val_F1=0.500\n",
      "[trial 21 | fold 2 | ep 000] loss=23.9296  val_balacc=0.500  val_F1=0.541\n",
      "[trial 21 | fold 2 | ep 010] loss=17.7612  val_balacc=0.459  val_F1=0.485\n",
      "[trial 21 | fold 2 | ep 020] loss=14.5806  val_balacc=0.459  val_F1=0.485\n",
      "[trial 21 | fold 2 | ep 030] loss=13.8715  val_balacc=0.429  val_F1=0.471\n",
      "[trial 21 | fold 2 | ep 040] loss=13.1801  val_balacc=0.479  val_F1=0.514\n",
      "[trial 21 | fold 2 | ep 050] loss=15.8402  val_balacc=0.479  val_F1=0.514\n",
      "Early stop @ epoch 59 (fold 2). Best BA was 0.529.\n",
      "fold 2: best val_balacc=0.529, val_F1=0.556\n",
      "[trial 21 | fold 3 | ep 000] loss=14.0276  val_balacc=0.500  val_F1=0.541\n",
      "[trial 21 | fold 3 | ep 010] loss=17.4418  val_balacc=0.500  val_F1=0.541\n",
      "[trial 21 | fold 3 | ep 020] loss=13.9193  val_balacc=0.500  val_F1=0.541\n",
      "[trial 21 | fold 3 | ep 030] loss=10.4221  val_balacc=0.476  val_F1=0.444\n",
      "[trial 21 | fold 3 | ep 040] loss=13.2693  val_balacc=0.515  val_F1=0.435\n",
      "[trial 21 | fold 3 | ep 050] loss=13.2455  val_balacc=0.506  val_F1=0.462\n",
      "[trial 21 | fold 3 | ep 060] loss=12.4442  val_balacc=0.476  val_F1=0.444\n",
      "Early stop @ epoch 61 (fold 3). Best BA was 0.626.\n",
      "fold 3: best val_balacc=0.626, val_F1=0.600\n",
      "[trial 21 | fold 4 | ep 000] loss=21.0804  val_balacc=0.500  val_F1=0.541\n",
      "[trial 21 | fold 4 | ep 010] loss=9.5478  val_balacc=0.500  val_F1=0.541\n",
      "[trial 21 | fold 4 | ep 020] loss=11.7450  val_balacc=0.500  val_F1=0.541\n",
      "[trial 21 | fold 4 | ep 030] loss=12.2948  val_balacc=0.500  val_F1=0.541\n",
      "[trial 21 | fold 4 | ep 040] loss=13.8564  val_balacc=0.500  val_F1=0.541\n",
      "[trial 21 | fold 4 | ep 050] loss=13.8018  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 50 (fold 4). Best BA was 0.500.\n",
      "fold 4: best val_balacc=0.500, val_F1=0.541\n",
      "TRIAL 21: mean bal-acc=0.5790.092, mean F1=0.5730.057\n",
      "\n",
      "--- Trial 22/50 ---\n",
      "Config: {'hidden': 64, 'layers': 2, 'heads': 2, 'dropout': 0.563, 'lr': np.float64(0.004062924042858131), 'wd': np.float64(7.417830553723407e-05), 'readout': 'att'}\n",
      "[trial 22 | fold 0 | ep 000] loss=6.5931  val_balacc=0.500  val_F1=0.526\n",
      "[trial 22 | fold 0 | ep 010] loss=1.1041  val_balacc=0.283  val_F1=0.276\n",
      "[trial 22 | fold 0 | ep 020] loss=0.8835  val_balacc=0.500  val_F1=0.000\n",
      "[trial 22 | fold 0 | ep 030] loss=0.8244  val_balacc=0.500  val_F1=0.000\n",
      "[trial 22 | fold 0 | ep 040] loss=0.7892  val_balacc=0.500  val_F1=0.000\n",
      "[trial 22 | fold 0 | ep 050] loss=0.7960  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 54 (fold 0). Best BA was 0.678.\n",
      "fold 0: best val_balacc=0.678, val_F1=0.615\n",
      "[trial 22 | fold 1 | ep 000] loss=4.6708  val_balacc=0.500  val_F1=0.500\n",
      "[trial 22 | fold 1 | ep 010] loss=1.3122  val_balacc=0.500  val_F1=0.500\n",
      "[trial 22 | fold 1 | ep 020] loss=0.9325  val_balacc=0.500  val_F1=0.500\n",
      "[trial 22 | fold 1 | ep 030] loss=0.8190  val_balacc=0.500  val_F1=0.500\n",
      "[trial 22 | fold 1 | ep 040] loss=0.9745  val_balacc=0.500  val_F1=0.500\n",
      "[trial 22 | fold 1 | ep 050] loss=0.7733  val_balacc=0.500  val_F1=0.500\n",
      "Early stop @ epoch 50 (fold 1). Best BA was 0.500.\n",
      "fold 1: best val_balacc=0.500, val_F1=0.500\n",
      "[trial 22 | fold 2 | ep 000] loss=6.2505  val_balacc=0.500  val_F1=0.541\n",
      "[trial 22 | fold 2 | ep 010] loss=1.6873  val_balacc=0.500  val_F1=0.541\n",
      "[trial 22 | fold 2 | ep 020] loss=0.9556  val_balacc=0.500  val_F1=0.541\n",
      "[trial 22 | fold 2 | ep 030] loss=0.8761  val_balacc=0.500  val_F1=0.541\n",
      "[trial 22 | fold 2 | ep 040] loss=0.8620  val_balacc=0.500  val_F1=0.541\n",
      "[trial 22 | fold 2 | ep 050] loss=0.7917  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 51 (fold 2). Best BA was 0.571.\n",
      "fold 2: best val_balacc=0.571, val_F1=0.308\n",
      "[trial 22 | fold 3 | ep 000] loss=3.8910  val_balacc=0.500  val_F1=0.000\n",
      "[trial 22 | fold 3 | ep 010] loss=0.7626  val_balacc=0.500  val_F1=0.541\n",
      "[trial 22 | fold 3 | ep 020] loss=0.7307  val_balacc=0.468  val_F1=0.467\n",
      "[trial 22 | fold 3 | ep 030] loss=0.6765  val_balacc=0.465  val_F1=0.364\n",
      "[trial 22 | fold 3 | ep 040] loss=0.7519  val_balacc=0.382  val_F1=0.000\n",
      "[trial 22 | fold 3 | ep 050] loss=0.6966  val_balacc=0.412  val_F1=0.000\n",
      "[trial 22 | fold 3 | ep 060] loss=0.6851  val_balacc=0.412  val_F1=0.000\n",
      "[trial 22 | fold 3 | ep 070] loss=0.6932  val_balacc=0.412  val_F1=0.000\n",
      "[trial 22 | fold 3 | ep 080] loss=0.6925  val_balacc=0.412  val_F1=0.000\n",
      "Early stop @ epoch 84 (fold 3). Best BA was 0.524.\n",
      "fold 3: best val_balacc=0.524, val_F1=0.400\n",
      "[trial 22 | fold 4 | ep 000] loss=7.2369  val_balacc=0.500  val_F1=0.541\n",
      "[trial 22 | fold 4 | ep 010] loss=1.0876  val_balacc=0.500  val_F1=0.541\n",
      "[trial 22 | fold 4 | ep 020] loss=0.6906  val_balacc=0.509  val_F1=0.529\n",
      "[trial 22 | fold 4 | ep 030] loss=0.8684  val_balacc=0.500  val_F1=0.541\n",
      "[trial 22 | fold 4 | ep 040] loss=0.8070  val_balacc=0.500  val_F1=0.541\n",
      "[trial 22 | fold 4 | ep 050] loss=0.8196  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 51 (fold 4). Best BA was 0.535.\n",
      "fold 4: best val_balacc=0.535, val_F1=0.480\n",
      "TRIAL 22: mean bal-acc=0.5610.062, mean F1=0.4610.103\n",
      "\n",
      "--- Trial 23/50 ---\n",
      "Config: {'hidden': 32, 'layers': 4, 'heads': 2, 'dropout': 0.339, 'lr': np.float64(0.0006419341852095082), 'wd': np.float64(0.0006288936234836103), 'readout': 'att'}\n",
      "[trial 23 | fold 0 | ep 000] loss=6.7510  val_balacc=0.500  val_F1=0.000\n",
      "[trial 23 | fold 0 | ep 010] loss=3.7227  val_balacc=0.500  val_F1=0.526\n",
      "[trial 23 | fold 0 | ep 020] loss=2.9579  val_balacc=0.500  val_F1=0.000\n",
      "[trial 23 | fold 0 | ep 030] loss=2.2171  val_balacc=0.500  val_F1=0.000\n",
      "[trial 23 | fold 0 | ep 040] loss=2.9448  val_balacc=0.500  val_F1=0.000\n",
      "[trial 23 | fold 0 | ep 050] loss=2.2713  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 57 (fold 0). Best BA was 0.767.\n",
      "fold 0: best val_balacc=0.767, val_F1=0.700\n",
      "[trial 23 | fold 1 | ep 000] loss=8.8273  val_balacc=0.528  val_F1=0.500\n",
      "[trial 23 | fold 1 | ep 010] loss=5.9925  val_balacc=0.528  val_F1=0.514\n",
      "[trial 23 | fold 1 | ep 020] loss=9.8899  val_balacc=0.472  val_F1=0.000\n",
      "[trial 23 | fold 1 | ep 030] loss=4.9975  val_balacc=0.389  val_F1=0.320\n",
      "[trial 23 | fold 1 | ep 040] loss=6.0379  val_balacc=0.389  val_F1=0.320\n",
      "[trial 23 | fold 1 | ep 050] loss=4.1334  val_balacc=0.389  val_F1=0.320\n",
      "Early stop @ epoch 52 (fold 1). Best BA was 0.611.\n",
      "fold 1: best val_balacc=0.611, val_F1=0.500\n",
      "[trial 23 | fold 2 | ep 000] loss=8.4726  val_balacc=0.500  val_F1=0.000\n",
      "[trial 23 | fold 2 | ep 010] loss=3.5171  val_balacc=0.500  val_F1=0.000\n",
      "[trial 23 | fold 2 | ep 020] loss=3.9967  val_balacc=0.500  val_F1=0.000\n",
      "[trial 23 | fold 2 | ep 030] loss=2.6109  val_balacc=0.500  val_F1=0.000\n",
      "[trial 23 | fold 2 | ep 040] loss=2.8823  val_balacc=0.500  val_F1=0.000\n",
      "[trial 23 | fold 2 | ep 050] loss=2.7374  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 2). Best BA was 0.500.\n",
      "fold 2: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 23 | fold 3 | ep 000] loss=8.9015  val_balacc=0.468  val_F1=0.467\n",
      "[trial 23 | fold 3 | ep 010] loss=5.1546  val_balacc=0.500  val_F1=0.541\n",
      "[trial 23 | fold 3 | ep 020] loss=3.9754  val_balacc=0.382  val_F1=0.000\n",
      "[trial 23 | fold 3 | ep 030] loss=3.9078  val_balacc=0.509  val_F1=0.529\n",
      "[trial 23 | fold 3 | ep 040] loss=4.0011  val_balacc=0.553  val_F1=0.421\n",
      "[trial 23 | fold 3 | ep 050] loss=5.1101  val_balacc=0.553  val_F1=0.421\n",
      "[trial 23 | fold 3 | ep 060] loss=4.5161  val_balacc=0.524  val_F1=0.400\n",
      "[trial 23 | fold 3 | ep 070] loss=4.1910  val_balacc=0.524  val_F1=0.400\n",
      "[trial 23 | fold 3 | ep 080] loss=4.3644  val_balacc=0.524  val_F1=0.400\n",
      "Early stop @ epoch 87 (fold 3). Best BA was 0.553.\n",
      "fold 3: best val_balacc=0.553, val_F1=0.421\n",
      "[trial 23 | fold 4 | ep 000] loss=9.6853  val_balacc=0.538  val_F1=0.545\n",
      "[trial 23 | fold 4 | ep 010] loss=5.9343  val_balacc=0.479  val_F1=0.514\n",
      "[trial 23 | fold 4 | ep 020] loss=2.6281  val_balacc=0.500  val_F1=0.541\n",
      "[trial 23 | fold 4 | ep 030] loss=2.5416  val_balacc=0.500  val_F1=0.541\n",
      "[trial 23 | fold 4 | ep 040] loss=2.8989  val_balacc=0.500  val_F1=0.541\n",
      "[trial 23 | fold 4 | ep 050] loss=2.4210  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 58 (fold 4). Best BA was 0.550.\n",
      "fold 4: best val_balacc=0.550, val_F1=0.182\n",
      "TRIAL 23: mean bal-acc=0.5960.092, mean F1=0.3610.245\n",
      "\n",
      "--- Trial 24/50 ---\n",
      "Config: {'hidden': 256, 'layers': 2, 'heads': 6, 'dropout': 0.527, 'lr': np.float64(0.0014861560503550784), 'wd': np.float64(0.0013331160280455957), 'readout': 'att'}\n",
      "[trial 24 | fold 0 | ep 000] loss=10.2478  val_balacc=0.500  val_F1=0.526\n",
      "[trial 24 | fold 0 | ep 010] loss=1.7644  val_balacc=0.500  val_F1=0.000\n",
      "[trial 24 | fold 0 | ep 020] loss=1.1488  val_balacc=0.500  val_F1=0.000\n",
      "[trial 24 | fold 0 | ep 030] loss=0.9847  val_balacc=0.500  val_F1=0.000\n",
      "[trial 24 | fold 0 | ep 040] loss=0.7969  val_balacc=0.500  val_F1=0.000\n",
      "[trial 24 | fold 0 | ep 050] loss=0.8417  val_balacc=0.500  val_F1=0.000\n",
      "[trial 24 | fold 0 | ep 060] loss=0.9143  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 66 (fold 0). Best BA was 0.544.\n",
      "fold 0: best val_balacc=0.544, val_F1=0.500\n",
      "[trial 24 | fold 1 | ep 000] loss=12.8586  val_balacc=0.500  val_F1=0.500\n",
      "[trial 24 | fold 1 | ep 010] loss=0.7016  val_balacc=0.500  val_F1=0.500\n",
      "[trial 24 | fold 1 | ep 020] loss=0.6964  val_balacc=0.500  val_F1=0.500\n",
      "[trial 24 | fold 1 | ep 030] loss=0.6869  val_balacc=0.500  val_F1=0.500\n",
      "[trial 24 | fold 1 | ep 040] loss=0.6968  val_balacc=0.500  val_F1=0.500\n",
      "[trial 24 | fold 1 | ep 050] loss=0.6929  val_balacc=0.500  val_F1=0.500\n",
      "Early stop @ epoch 55 (fold 1). Best BA was 0.528.\n",
      "fold 1: best val_balacc=0.528, val_F1=0.435\n",
      "[trial 24 | fold 2 | ep 000] loss=4.7676  val_balacc=0.500  val_F1=0.000\n",
      "[trial 24 | fold 2 | ep 010] loss=0.6921  val_balacc=0.500  val_F1=0.541\n",
      "[trial 24 | fold 2 | ep 020] loss=0.6902  val_balacc=0.500  val_F1=0.541\n",
      "[trial 24 | fold 2 | ep 030] loss=0.6936  val_balacc=0.500  val_F1=0.541\n",
      "[trial 24 | fold 2 | ep 040] loss=0.6952  val_balacc=0.500  val_F1=0.541\n",
      "[trial 24 | fold 2 | ep 050] loss=0.6923  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 50 (fold 2). Best BA was 0.500.\n",
      "fold 2: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 24 | fold 3 | ep 000] loss=3.0679  val_balacc=0.500  val_F1=0.000\n",
      "[trial 24 | fold 3 | ep 010] loss=0.6895  val_balacc=0.500  val_F1=0.000\n",
      "[trial 24 | fold 3 | ep 020] loss=0.6869  val_balacc=0.500  val_F1=0.000\n",
      "[trial 24 | fold 3 | ep 030] loss=0.6940  val_balacc=0.500  val_F1=0.000\n",
      "[trial 24 | fold 3 | ep 040] loss=0.6977  val_balacc=0.500  val_F1=0.000\n",
      "[trial 24 | fold 3 | ep 050] loss=0.6869  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 3). Best BA was 0.500.\n",
      "fold 3: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 24 | fold 4 | ep 000] loss=7.8469  val_balacc=0.500  val_F1=0.000\n",
      "[trial 24 | fold 4 | ep 010] loss=1.4896  val_balacc=0.500  val_F1=0.000\n",
      "[trial 24 | fold 4 | ep 020] loss=0.7646  val_balacc=0.500  val_F1=0.000\n",
      "[trial 24 | fold 4 | ep 030] loss=0.8294  val_balacc=0.500  val_F1=0.000\n",
      "[trial 24 | fold 4 | ep 040] loss=0.8067  val_balacc=0.500  val_F1=0.000\n",
      "[trial 24 | fold 4 | ep 050] loss=0.7898  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 53 (fold 4). Best BA was 0.647.\n",
      "fold 4: best val_balacc=0.647, val_F1=0.625\n",
      "TRIAL 24: mean bal-acc=0.5440.054, mean F1=0.3120.262\n",
      "\n",
      "--- Trial 25/50 ---\n",
      "Config: {'hidden': 128, 'layers': 3, 'heads': 6, 'dropout': 0.435, 'lr': np.float64(0.0002637449795755048), 'wd': np.float64(6.71462552397177e-05), 'readout': 'mean'}\n",
      "[trial 25 | fold 0 | ep 000] loss=1.2572  val_balacc=0.500  val_F1=0.000\n",
      "[trial 25 | fold 0 | ep 010] loss=0.7161  val_balacc=0.500  val_F1=0.526\n",
      "[trial 25 | fold 0 | ep 020] loss=0.7266  val_balacc=0.500  val_F1=0.526\n",
      "[trial 25 | fold 0 | ep 030] loss=0.6997  val_balacc=0.500  val_F1=0.526\n",
      "[trial 25 | fold 0 | ep 040] loss=0.7043  val_balacc=0.500  val_F1=0.526\n",
      "[trial 25 | fold 0 | ep 050] loss=0.7374  val_balacc=0.500  val_F1=0.526\n",
      "Early stop @ epoch 51 (fold 0). Best BA was 0.650.\n",
      "fold 0: best val_balacc=0.650, val_F1=0.462\n",
      "[trial 25 | fold 1 | ep 000] loss=2.6807  val_balacc=0.500  val_F1=0.000\n",
      "[trial 25 | fold 1 | ep 010] loss=0.7258  val_balacc=0.500  val_F1=0.500\n",
      "[trial 25 | fold 1 | ep 020] loss=0.7377  val_balacc=0.500  val_F1=0.500\n",
      "[trial 25 | fold 1 | ep 030] loss=0.7093  val_balacc=0.500  val_F1=0.500\n",
      "[trial 25 | fold 1 | ep 040] loss=0.7211  val_balacc=0.500  val_F1=0.500\n",
      "[trial 25 | fold 1 | ep 050] loss=0.7436  val_balacc=0.500  val_F1=0.500\n",
      "Early stop @ epoch 50 (fold 1). Best BA was 0.500.\n",
      "fold 1: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 25 | fold 2 | ep 000] loss=2.1213  val_balacc=0.500  val_F1=0.000\n",
      "[trial 25 | fold 2 | ep 010] loss=0.7274  val_balacc=0.500  val_F1=0.541\n",
      "[trial 25 | fold 2 | ep 020] loss=0.7138  val_balacc=0.500  val_F1=0.541\n",
      "[trial 25 | fold 2 | ep 030] loss=0.6975  val_balacc=0.500  val_F1=0.541\n",
      "[trial 25 | fold 2 | ep 040] loss=0.7046  val_balacc=0.500  val_F1=0.541\n",
      "[trial 25 | fold 2 | ep 050] loss=0.7064  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 52 (fold 2). Best BA was 0.529.\n",
      "fold 2: best val_balacc=0.529, val_F1=0.556\n",
      "[trial 25 | fold 3 | ep 000] loss=1.0480  val_balacc=0.500  val_F1=0.541\n",
      "[trial 25 | fold 3 | ep 010] loss=0.8130  val_balacc=0.500  val_F1=0.000\n",
      "[trial 25 | fold 3 | ep 020] loss=0.7297  val_balacc=0.500  val_F1=0.000\n",
      "[trial 25 | fold 3 | ep 030] loss=0.7109  val_balacc=0.500  val_F1=0.000\n",
      "[trial 25 | fold 3 | ep 040] loss=0.6976  val_balacc=0.500  val_F1=0.000\n",
      "[trial 25 | fold 3 | ep 050] loss=0.7025  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 3). Best BA was 0.500.\n",
      "fold 3: best val_balacc=0.500, val_F1=0.541\n",
      "[trial 25 | fold 4 | ep 000] loss=1.6670  val_balacc=0.500  val_F1=0.541\n",
      "[trial 25 | fold 4 | ep 010] loss=0.7473  val_balacc=0.500  val_F1=0.541\n",
      "[trial 25 | fold 4 | ep 020] loss=0.7279  val_balacc=0.500  val_F1=0.541\n",
      "[trial 25 | fold 4 | ep 030] loss=0.7009  val_balacc=0.500  val_F1=0.541\n",
      "[trial 25 | fold 4 | ep 040] loss=0.7158  val_balacc=0.500  val_F1=0.541\n",
      "[trial 25 | fold 4 | ep 050] loss=0.7321  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 50 (fold 4). Best BA was 0.500.\n",
      "fold 4: best val_balacc=0.500, val_F1=0.541\n",
      "TRIAL 25: mean bal-acc=0.5360.058, mean F1=0.4200.212\n",
      "\n",
      "--- Trial 26/50 ---\n",
      "Config: {'hidden': 64, 'layers': 4, 'heads': 2, 'dropout': 0.302, 'lr': np.float64(0.0015944119328070443), 'wd': np.float64(6.557186530342078e-05), 'readout': 'mean'}\n",
      "[trial 26 | fold 0 | ep 000] loss=1.9890  val_balacc=0.500  val_F1=0.000\n",
      "[trial 26 | fold 0 | ep 010] loss=0.7024  val_balacc=0.500  val_F1=0.000\n",
      "[trial 26 | fold 0 | ep 020] loss=0.7007  val_balacc=0.500  val_F1=0.526\n",
      "[trial 26 | fold 0 | ep 030] loss=0.6954  val_balacc=0.500  val_F1=0.526\n",
      "[trial 26 | fold 0 | ep 040] loss=0.6917  val_balacc=0.500  val_F1=0.526\n",
      "[trial 26 | fold 0 | ep 050] loss=0.7029  val_balacc=0.500  val_F1=0.526\n",
      "Early stop @ epoch 59 (fold 0). Best BA was 0.728.\n",
      "fold 0: best val_balacc=0.728, val_F1=0.667\n",
      "[trial 26 | fold 1 | ep 000] loss=1.4030  val_balacc=0.500  val_F1=0.000\n",
      "[trial 26 | fold 1 | ep 010] loss=0.6978  val_balacc=0.500  val_F1=0.000\n",
      "[trial 26 | fold 1 | ep 020] loss=0.7135  val_balacc=0.500  val_F1=0.000\n",
      "[trial 26 | fold 1 | ep 030] loss=0.7049  val_balacc=0.500  val_F1=0.000\n",
      "[trial 26 | fold 1 | ep 040] loss=0.6935  val_balacc=0.500  val_F1=0.000\n",
      "[trial 26 | fold 1 | ep 050] loss=0.7058  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 53 (fold 1). Best BA was 0.694.\n",
      "fold 1: best val_balacc=0.694, val_F1=0.588\n",
      "[trial 26 | fold 2 | ep 000] loss=1.6582  val_balacc=0.529  val_F1=0.556\n",
      "[trial 26 | fold 2 | ep 010] loss=0.6973  val_balacc=0.500  val_F1=0.541\n",
      "[trial 26 | fold 2 | ep 020] loss=0.7064  val_balacc=0.500  val_F1=0.541\n",
      "[trial 26 | fold 2 | ep 030] loss=0.6824  val_balacc=0.500  val_F1=0.541\n",
      "[trial 26 | fold 2 | ep 040] loss=0.7029  val_balacc=0.500  val_F1=0.541\n",
      "[trial 26 | fold 2 | ep 050] loss=0.6953  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 50 (fold 2). Best BA was 0.529.\n",
      "fold 2: best val_balacc=0.529, val_F1=0.556\n",
      "[trial 26 | fold 3 | ep 000] loss=1.8291  val_balacc=0.500  val_F1=0.000\n",
      "[trial 26 | fold 3 | ep 010] loss=0.7018  val_balacc=0.500  val_F1=0.541\n",
      "[trial 26 | fold 3 | ep 020] loss=0.7265  val_balacc=0.500  val_F1=0.541\n",
      "[trial 26 | fold 3 | ep 030] loss=0.6930  val_balacc=0.500  val_F1=0.541\n",
      "[trial 26 | fold 3 | ep 040] loss=0.6792  val_balacc=0.500  val_F1=0.541\n",
      "[trial 26 | fold 3 | ep 050] loss=0.6887  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 50 (fold 3). Best BA was 0.500.\n",
      "fold 3: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 26 | fold 4 | ep 000] loss=2.2757  val_balacc=0.500  val_F1=0.000\n",
      "[trial 26 | fold 4 | ep 010] loss=0.6896  val_balacc=0.500  val_F1=0.541\n",
      "[trial 26 | fold 4 | ep 020] loss=0.6873  val_balacc=0.500  val_F1=0.541\n",
      "[trial 26 | fold 4 | ep 030] loss=0.6830  val_balacc=0.500  val_F1=0.541\n",
      "[trial 26 | fold 4 | ep 040] loss=0.6958  val_balacc=0.500  val_F1=0.541\n",
      "[trial 26 | fold 4 | ep 050] loss=0.6799  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 58 (fold 4). Best BA was 0.518.\n",
      "fold 4: best val_balacc=0.518, val_F1=0.516\n",
      "TRIAL 26: mean bal-acc=0.5940.097, mean F1=0.4650.238\n",
      "\n",
      "--- Trial 27/50 ---\n",
      "Config: {'hidden': 32, 'layers': 3, 'heads': 2, 'dropout': 0.454, 'lr': np.float64(0.00029725181725389125), 'wd': np.float64(0.0004675187172203805), 'readout': 's2s'}\n",
      "[trial 27 | fold 0 | ep 000] loss=7.3995  val_balacc=0.500  val_F1=0.526\n",
      "[trial 27 | fold 0 | ep 010] loss=5.4587  val_balacc=0.500  val_F1=0.526\n",
      "[trial 27 | fold 0 | ep 020] loss=5.8028  val_balacc=0.533  val_F1=0.529\n",
      "[trial 27 | fold 0 | ep 030] loss=5.9604  val_balacc=0.500  val_F1=0.526\n",
      "[trial 27 | fold 0 | ep 040] loss=5.0959  val_balacc=0.500  val_F1=0.526\n",
      "[trial 27 | fold 0 | ep 050] loss=3.2503  val_balacc=0.500  val_F1=0.526\n",
      "[trial 27 | fold 0 | ep 060] loss=4.7613  val_balacc=0.500  val_F1=0.526\n",
      "[trial 27 | fold 0 | ep 070] loss=4.8193  val_balacc=0.500  val_F1=0.526\n",
      "Early stop @ epoch 73 (fold 0). Best BA was 0.767.\n",
      "fold 0: best val_balacc=0.767, val_F1=0.700\n",
      "[trial 27 | fold 1 | ep 000] loss=4.1146  val_balacc=0.500  val_F1=0.000\n",
      "[trial 27 | fold 1 | ep 010] loss=4.2352  val_balacc=0.528  val_F1=0.514\n",
      "[trial 27 | fold 1 | ep 020] loss=4.0010  val_balacc=0.528  val_F1=0.514\n",
      "[trial 27 | fold 1 | ep 030] loss=4.5897  val_balacc=0.528  val_F1=0.514\n",
      "[trial 27 | fold 1 | ep 040] loss=3.6439  val_balacc=0.500  val_F1=0.500\n",
      "[trial 27 | fold 1 | ep 050] loss=4.1584  val_balacc=0.500  val_F1=0.500\n",
      "Early stop @ epoch 55 (fold 1). Best BA was 0.611.\n",
      "fold 1: best val_balacc=0.611, val_F1=0.364\n",
      "[trial 27 | fold 2 | ep 000] loss=8.4942  val_balacc=0.715  val_F1=0.667\n",
      "[trial 27 | fold 2 | ep 010] loss=6.0284  val_balacc=0.526  val_F1=0.500\n",
      "[trial 27 | fold 2 | ep 020] loss=6.5706  val_balacc=0.588  val_F1=0.588\n",
      "[trial 27 | fold 2 | ep 030] loss=4.9322  val_balacc=0.529  val_F1=0.556\n",
      "[trial 27 | fold 2 | ep 040] loss=5.4114  val_balacc=0.529  val_F1=0.556\n",
      "[trial 27 | fold 2 | ep 050] loss=4.0283  val_balacc=0.529  val_F1=0.556\n",
      "Early stop @ epoch 50 (fold 2). Best BA was 0.715.\n",
      "fold 2: best val_balacc=0.715, val_F1=0.667\n",
      "[trial 27 | fold 3 | ep 000] loss=6.6112  val_balacc=0.500  val_F1=0.541\n",
      "[trial 27 | fold 3 | ep 010] loss=4.8742  val_balacc=0.500  val_F1=0.000\n",
      "[trial 27 | fold 3 | ep 020] loss=5.8280  val_balacc=0.453  val_F1=0.235\n",
      "[trial 27 | fold 3 | ep 030] loss=5.1536  val_balacc=0.453  val_F1=0.235\n",
      "[trial 27 | fold 3 | ep 040] loss=5.1662  val_balacc=0.482  val_F1=0.250\n",
      "[trial 27 | fold 3 | ep 050] loss=4.5999  val_balacc=0.482  val_F1=0.250\n",
      "Early stop @ epoch 50 (fold 3). Best BA was 0.500.\n",
      "fold 3: best val_balacc=0.500, val_F1=0.541\n",
      "[trial 27 | fold 4 | ep 000] loss=8.3194  val_balacc=0.453  val_F1=0.235\n",
      "[trial 27 | fold 4 | ep 010] loss=4.4934  val_balacc=0.500  val_F1=0.000\n",
      "[trial 27 | fold 4 | ep 020] loss=4.3407  val_balacc=0.500  val_F1=0.000\n",
      "[trial 27 | fold 4 | ep 030] loss=4.0141  val_balacc=0.500  val_F1=0.000\n",
      "[trial 27 | fold 4 | ep 040] loss=4.8326  val_balacc=0.500  val_F1=0.000\n",
      "[trial 27 | fold 4 | ep 050] loss=5.0630  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 51 (fold 4). Best BA was 0.550.\n",
      "fold 4: best val_balacc=0.550, val_F1=0.182\n",
      "TRIAL 27: mean bal-acc=0.6280.099, mean F1=0.4910.194\n",
      "\n",
      "--- Trial 28/50 ---\n",
      "Config: {'hidden': 64, 'layers': 4, 'heads': 6, 'dropout': 0.473, 'lr': np.float64(0.00025872894996959887), 'wd': np.float64(0.00044148493672360387), 'readout': 'att'}\n",
      "[trial 28 | fold 0 | ep 000] loss=14.7966  val_balacc=0.500  val_F1=0.526\n",
      "[trial 28 | fold 0 | ep 010] loss=14.1726  val_balacc=0.500  val_F1=0.526\n",
      "[trial 28 | fold 0 | ep 020] loss=13.9201  val_balacc=0.517  val_F1=0.267\n",
      "[trial 28 | fold 0 | ep 030] loss=6.9938  val_balacc=0.517  val_F1=0.267\n",
      "[trial 28 | fold 0 | ep 040] loss=12.2264  val_balacc=0.517  val_F1=0.267\n",
      "[trial 28 | fold 0 | ep 050] loss=10.4631  val_balacc=0.517  val_F1=0.267\n",
      "Early stop @ epoch 51 (fold 0). Best BA was 0.617.\n",
      "fold 0: best val_balacc=0.617, val_F1=0.471\n",
      "[trial 28 | fold 1 | ep 000] loss=20.9800  val_balacc=0.500  val_F1=0.000\n",
      "[trial 28 | fold 1 | ep 010] loss=9.5650  val_balacc=0.500  val_F1=0.500\n",
      "[trial 28 | fold 1 | ep 020] loss=9.2813  val_balacc=0.528  val_F1=0.435\n",
      "[trial 28 | fold 1 | ep 030] loss=7.7240  val_balacc=0.500  val_F1=0.000\n",
      "[trial 28 | fold 1 | ep 040] loss=8.4651  val_balacc=0.500  val_F1=0.000\n",
      "[trial 28 | fold 1 | ep 050] loss=10.4678  val_balacc=0.500  val_F1=0.000\n",
      "[trial 28 | fold 1 | ep 060] loss=11.7195  val_balacc=0.500  val_F1=0.000\n",
      "[trial 28 | fold 1 | ep 070] loss=10.9137  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 74 (fold 1). Best BA was 0.611.\n",
      "fold 1: best val_balacc=0.611, val_F1=0.364\n",
      "[trial 28 | fold 2 | ep 000] loss=15.7749  val_balacc=0.500  val_F1=0.000\n",
      "[trial 28 | fold 2 | ep 010] loss=12.8804  val_balacc=0.500  val_F1=0.541\n",
      "[trial 28 | fold 2 | ep 020] loss=11.5784  val_balacc=0.500  val_F1=0.000\n",
      "[trial 28 | fold 2 | ep 030] loss=5.8791  val_balacc=0.653  val_F1=0.571\n",
      "[trial 28 | fold 2 | ep 040] loss=6.7078  val_balacc=0.459  val_F1=0.485\n",
      "[trial 28 | fold 2 | ep 050] loss=5.2393  val_balacc=0.529  val_F1=0.556\n",
      "[trial 28 | fold 2 | ep 060] loss=8.5359  val_balacc=0.529  val_F1=0.556\n",
      "[trial 28 | fold 2 | ep 070] loss=5.6898  val_balacc=0.529  val_F1=0.556\n",
      "[trial 28 | fold 2 | ep 080] loss=6.1920  val_balacc=0.529  val_F1=0.556\n",
      "Early stop @ epoch 80 (fold 2). Best BA was 0.653.\n",
      "fold 2: best val_balacc=0.653, val_F1=0.571\n",
      "[trial 28 | fold 3 | ep 000] loss=13.4839  val_balacc=0.500  val_F1=0.000\n",
      "[trial 28 | fold 3 | ep 010] loss=10.8191  val_balacc=0.500  val_F1=0.000\n",
      "[trial 28 | fold 3 | ep 020] loss=9.8063  val_balacc=0.500  val_F1=0.000\n",
      "[trial 28 | fold 3 | ep 030] loss=9.7437  val_balacc=0.500  val_F1=0.000\n",
      "[trial 28 | fold 3 | ep 040] loss=7.3682  val_balacc=0.500  val_F1=0.000\n",
      "[trial 28 | fold 3 | ep 050] loss=9.0502  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 54 (fold 3). Best BA was 0.588.\n",
      "fold 3: best val_balacc=0.588, val_F1=0.588\n",
      "[trial 28 | fold 4 | ep 000] loss=17.3666  val_balacc=0.500  val_F1=0.000\n",
      "[trial 28 | fold 4 | ep 010] loss=11.6373  val_balacc=0.500  val_F1=0.541\n",
      "[trial 28 | fold 4 | ep 020] loss=14.3243  val_balacc=0.500  val_F1=0.000\n",
      "[trial 28 | fold 4 | ep 030] loss=11.9394  val_balacc=0.500  val_F1=0.000\n",
      "[trial 28 | fold 4 | ep 040] loss=10.1004  val_balacc=0.500  val_F1=0.000\n",
      "[trial 28 | fold 4 | ep 050] loss=13.3343  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 52 (fold 4). Best BA was 0.571.\n",
      "fold 4: best val_balacc=0.571, val_F1=0.308\n",
      "TRIAL 28: mean bal-acc=0.6080.028, mean F1=0.4600.111\n",
      "\n",
      "--- Trial 29/50 ---\n",
      "Config: {'hidden': 64, 'layers': 2, 'heads': 2, 'dropout': 0.498, 'lr': np.float64(0.00039989036308632383), 'wd': np.float64(0.00033202788305131017), 'readout': 's2s'}\n",
      "[trial 29 | fold 0 | ep 000] loss=2.4045  val_balacc=0.500  val_F1=0.000\n",
      "[trial 29 | fold 0 | ep 010] loss=1.6989  val_balacc=0.472  val_F1=0.000\n",
      "[trial 29 | fold 0 | ep 020] loss=1.4220  val_balacc=0.411  val_F1=0.125\n",
      "[trial 29 | fold 0 | ep 030] loss=1.3172  val_balacc=0.500  val_F1=0.526\n",
      "[trial 29 | fold 0 | ep 040] loss=1.2284  val_balacc=0.528  val_F1=0.541\n",
      "[trial 29 | fold 0 | ep 050] loss=1.0593  val_balacc=0.556  val_F1=0.556\n",
      "[trial 29 | fold 0 | ep 060] loss=1.2959  val_balacc=0.528  val_F1=0.541\n",
      "[trial 29 | fold 0 | ep 070] loss=1.1322  val_balacc=0.528  val_F1=0.541\n",
      "Early stop @ epoch 73 (fold 0). Best BA was 0.644.\n",
      "fold 0: best val_balacc=0.644, val_F1=0.600\n",
      "[trial 29 | fold 1 | ep 000] loss=1.9961  val_balacc=0.500  val_F1=0.500\n",
      "[trial 29 | fold 1 | ep 010] loss=1.6295  val_balacc=0.500  val_F1=0.500\n",
      "[trial 29 | fold 1 | ep 020] loss=2.0167  val_balacc=0.500  val_F1=0.500\n",
      "[trial 29 | fold 1 | ep 030] loss=1.8546  val_balacc=0.500  val_F1=0.500\n",
      "[trial 29 | fold 1 | ep 040] loss=1.3449  val_balacc=0.500  val_F1=0.500\n",
      "[trial 29 | fold 1 | ep 050] loss=1.5613  val_balacc=0.500  val_F1=0.500\n",
      "Early stop @ epoch 50 (fold 1). Best BA was 0.500.\n",
      "fold 1: best val_balacc=0.500, val_F1=0.500\n",
      "[trial 29 | fold 2 | ep 000] loss=2.4264  val_balacc=0.500  val_F1=0.000\n",
      "[trial 29 | fold 2 | ep 010] loss=1.8147  val_balacc=0.429  val_F1=0.471\n",
      "[trial 29 | fold 2 | ep 020] loss=1.5253  val_balacc=0.500  val_F1=0.000\n",
      "[trial 29 | fold 2 | ep 030] loss=1.2322  val_balacc=0.500  val_F1=0.000\n",
      "[trial 29 | fold 2 | ep 040] loss=1.2514  val_balacc=0.471  val_F1=0.000\n",
      "[trial 29 | fold 2 | ep 050] loss=1.1814  val_balacc=0.418  val_F1=0.414\n",
      "[trial 29 | fold 2 | ep 060] loss=0.9105  val_balacc=0.418  val_F1=0.414\n",
      "Early stop @ epoch 69 (fold 2). Best BA was 0.538.\n",
      "fold 2: best val_balacc=0.538, val_F1=0.545\n",
      "[trial 29 | fold 3 | ep 000] loss=2.8044  val_balacc=0.500  val_F1=0.000\n",
      "[trial 29 | fold 3 | ep 010] loss=1.6411  val_balacc=0.588  val_F1=0.588\n",
      "[trial 29 | fold 3 | ep 020] loss=1.3910  val_balacc=0.500  val_F1=0.541\n",
      "[trial 29 | fold 3 | ep 030] loss=1.5917  val_balacc=0.500  val_F1=0.541\n",
      "[trial 29 | fold 3 | ep 040] loss=1.4179  val_balacc=0.500  val_F1=0.541\n",
      "[trial 29 | fold 3 | ep 050] loss=1.1785  val_balacc=0.500  val_F1=0.541\n",
      "[trial 29 | fold 3 | ep 060] loss=1.2139  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 60 (fold 3). Best BA was 0.588.\n",
      "fold 3: best val_balacc=0.588, val_F1=0.588\n",
      "[trial 29 | fold 4 | ep 000] loss=2.5437  val_balacc=0.397  val_F1=0.370\n",
      "[trial 29 | fold 4 | ep 010] loss=2.0631  val_balacc=0.550  val_F1=0.182\n",
      "[trial 29 | fold 4 | ep 020] loss=2.0102  val_balacc=0.485  val_F1=0.417\n",
      "[trial 29 | fold 4 | ep 030] loss=1.6418  val_balacc=0.453  val_F1=0.235\n",
      "[trial 29 | fold 4 | ep 040] loss=1.3229  val_balacc=0.453  val_F1=0.235\n",
      "[trial 29 | fold 4 | ep 050] loss=1.2510  val_balacc=0.453  val_F1=0.235\n",
      "Early stop @ epoch 55 (fold 4). Best BA was 0.624.\n",
      "fold 4: best val_balacc=0.624, val_F1=0.545\n",
      "TRIAL 29: mean bal-acc=0.5790.053, mean F1=0.5560.036\n",
      "\n",
      "--- Trial 30/50 ---\n",
      "Config: {'hidden': 32, 'layers': 4, 'heads': 6, 'dropout': 0.595, 'lr': np.float64(0.00014696333914151086), 'wd': np.float64(0.00031931003584901674), 'readout': 'att'}\n",
      "[trial 30 | fold 0 | ep 000] loss=116.8298  val_balacc=0.589  val_F1=0.444\n",
      "[trial 30 | fold 0 | ep 010] loss=123.9957  val_balacc=0.600  val_F1=0.333\n",
      "[trial 30 | fold 0 | ep 020] loss=134.9754  val_balacc=0.550  val_F1=0.182\n",
      "[trial 30 | fold 0 | ep 030] loss=145.8751  val_balacc=0.550  val_F1=0.182\n",
      "[trial 30 | fold 0 | ep 040] loss=99.9701  val_balacc=0.550  val_F1=0.182\n",
      "[trial 30 | fold 0 | ep 050] loss=123.6186  val_balacc=0.550  val_F1=0.182\n",
      "Early stop @ epoch 51 (fold 0). Best BA was 0.600.\n",
      "fold 0: best val_balacc=0.600, val_F1=0.333\n",
      "[trial 30 | fold 1 | ep 000] loss=154.4452  val_balacc=0.444  val_F1=0.143\n",
      "[trial 30 | fold 1 | ep 010] loss=112.7369  val_balacc=0.472  val_F1=0.000\n",
      "[trial 30 | fold 1 | ep 020] loss=105.8488  val_balacc=0.472  val_F1=0.000\n",
      "[trial 30 | fold 1 | ep 030] loss=104.6807  val_balacc=0.472  val_F1=0.000\n",
      "[trial 30 | fold 1 | ep 040] loss=146.5286  val_balacc=0.472  val_F1=0.000\n",
      "[trial 30 | fold 1 | ep 050] loss=159.1070  val_balacc=0.472  val_F1=0.000\n",
      "Early stop @ epoch 52 (fold 1). Best BA was 0.500.\n",
      "fold 1: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 30 | fold 2 | ep 000] loss=154.7672  val_balacc=0.500  val_F1=0.000\n",
      "[trial 30 | fold 2 | ep 010] loss=192.3803  val_balacc=0.500  val_F1=0.000\n",
      "[trial 30 | fold 2 | ep 020] loss=125.8852  val_balacc=0.500  val_F1=0.000\n",
      "[trial 30 | fold 2 | ep 030] loss=122.1705  val_balacc=0.500  val_F1=0.000\n",
      "[trial 30 | fold 2 | ep 040] loss=135.2989  val_balacc=0.500  val_F1=0.000\n",
      "[trial 30 | fold 2 | ep 050] loss=164.8173  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 2). Best BA was 0.500.\n",
      "fold 2: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 30 | fold 3 | ep 000] loss=131.0858  val_balacc=0.441  val_F1=0.000\n",
      "[trial 30 | fold 3 | ep 010] loss=126.4424  val_balacc=0.559  val_F1=0.571\n",
      "[trial 30 | fold 3 | ep 020] loss=100.2688  val_balacc=0.500  val_F1=0.000\n",
      "[trial 30 | fold 3 | ep 030] loss=118.1244  val_balacc=0.500  val_F1=0.000\n",
      "[trial 30 | fold 3 | ep 040] loss=102.4472  val_balacc=0.500  val_F1=0.000\n",
      "[trial 30 | fold 3 | ep 050] loss=96.1559  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 51 (fold 3). Best BA was 0.618.\n",
      "fold 3: best val_balacc=0.618, val_F1=0.606\n",
      "[trial 30 | fold 4 | ep 000] loss=115.2477  val_balacc=0.500  val_F1=0.541\n",
      "[trial 30 | fold 4 | ep 010] loss=109.1619  val_balacc=0.529  val_F1=0.556\n",
      "[trial 30 | fold 4 | ep 020] loss=130.2243  val_balacc=0.500  val_F1=0.541\n",
      "[trial 30 | fold 4 | ep 030] loss=118.8967  val_balacc=0.500  val_F1=0.541\n",
      "[trial 30 | fold 4 | ep 040] loss=91.0022  val_balacc=0.500  val_F1=0.541\n",
      "[trial 30 | fold 4 | ep 050] loss=144.5642  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 54 (fold 4). Best BA was 0.635.\n",
      "fold 4: best val_balacc=0.635, val_F1=0.593\n",
      "TRIAL 30: mean bal-acc=0.5710.059, mean F1=0.3060.268\n",
      "\n",
      "--- Trial 31/50 ---\n",
      "Config: {'hidden': 32, 'layers': 2, 'heads': 2, 'dropout': 0.357, 'lr': np.float64(0.0005783369779697876), 'wd': np.float64(0.00034892596157393477), 'readout': 'att'}\n",
      "[trial 31 | fold 0 | ep 000] loss=1.8130  val_balacc=0.500  val_F1=0.526\n",
      "[trial 31 | fold 0 | ep 010] loss=0.8828  val_balacc=0.500  val_F1=0.526\n",
      "[trial 31 | fold 0 | ep 020] loss=0.7719  val_balacc=0.500  val_F1=0.526\n",
      "[trial 31 | fold 0 | ep 030] loss=0.8175  val_balacc=0.500  val_F1=0.526\n",
      "[trial 31 | fold 0 | ep 040] loss=0.8840  val_balacc=0.500  val_F1=0.526\n",
      "[trial 31 | fold 0 | ep 050] loss=0.6958  val_balacc=0.500  val_F1=0.526\n",
      "Early stop @ epoch 52 (fold 0). Best BA was 0.656.\n",
      "fold 0: best val_balacc=0.656, val_F1=0.583\n",
      "[trial 31 | fold 1 | ep 000] loss=0.9745  val_balacc=0.500  val_F1=0.000\n",
      "[trial 31 | fold 1 | ep 010] loss=0.8107  val_balacc=0.500  val_F1=0.000\n",
      "[trial 31 | fold 1 | ep 020] loss=0.7371  val_balacc=0.500  val_F1=0.000\n",
      "[trial 31 | fold 1 | ep 030] loss=0.7550  val_balacc=0.500  val_F1=0.000\n",
      "[trial 31 | fold 1 | ep 040] loss=0.7228  val_balacc=0.500  val_F1=0.000\n",
      "[trial 31 | fold 1 | ep 050] loss=0.7436  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 1). Best BA was 0.500.\n",
      "fold 1: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 31 | fold 2 | ep 000] loss=1.9243  val_balacc=0.500  val_F1=0.000\n",
      "[trial 31 | fold 2 | ep 010] loss=0.9641  val_balacc=0.500  val_F1=0.000\n",
      "[trial 31 | fold 2 | ep 020] loss=0.7235  val_balacc=0.500  val_F1=0.000\n",
      "[trial 31 | fold 2 | ep 030] loss=0.6960  val_balacc=0.500  val_F1=0.000\n",
      "[trial 31 | fold 2 | ep 040] loss=0.7231  val_balacc=0.500  val_F1=0.000\n",
      "[trial 31 | fold 2 | ep 050] loss=0.6954  val_balacc=0.500  val_F1=0.000\n",
      "[trial 31 | fold 2 | ep 060] loss=0.7045  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 63 (fold 2). Best BA was 0.621.\n",
      "fold 2: best val_balacc=0.621, val_F1=0.429\n",
      "[trial 31 | fold 3 | ep 000] loss=1.4307  val_balacc=0.500  val_F1=0.000\n",
      "[trial 31 | fold 3 | ep 010] loss=0.7569  val_balacc=0.541  val_F1=0.286\n",
      "[trial 31 | fold 3 | ep 020] loss=0.7175  val_balacc=0.500  val_F1=0.541\n",
      "[trial 31 | fold 3 | ep 030] loss=0.7430  val_balacc=0.500  val_F1=0.541\n",
      "[trial 31 | fold 3 | ep 040] loss=0.6989  val_balacc=0.500  val_F1=0.541\n",
      "[trial 31 | fold 3 | ep 050] loss=0.6998  val_balacc=0.500  val_F1=0.541\n",
      "[trial 31 | fold 3 | ep 060] loss=0.6897  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 62 (fold 3). Best BA was 0.612.\n",
      "fold 3: best val_balacc=0.612, val_F1=0.471\n",
      "[trial 31 | fold 4 | ep 000] loss=1.0391  val_balacc=0.500  val_F1=0.000\n",
      "[trial 31 | fold 4 | ep 010] loss=0.7066  val_balacc=0.500  val_F1=0.000\n",
      "[trial 31 | fold 4 | ep 020] loss=0.7108  val_balacc=0.500  val_F1=0.000\n",
      "[trial 31 | fold 4 | ep 030] loss=0.6984  val_balacc=0.500  val_F1=0.000\n",
      "[trial 31 | fold 4 | ep 040] loss=0.7129  val_balacc=0.500  val_F1=0.000\n",
      "[trial 31 | fold 4 | ep 050] loss=0.7138  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 4). Best BA was 0.500.\n",
      "fold 4: best val_balacc=0.500, val_F1=0.000\n",
      "TRIAL 31: mean bal-acc=0.5780.065, mean F1=0.2960.247\n",
      "\n",
      "--- Trial 32/50 ---\n",
      "Config: {'hidden': 256, 'layers': 2, 'heads': 2, 'dropout': 0.433, 'lr': np.float64(0.0029067407041487), 'wd': np.float64(0.00063040642278312), 'readout': 'mean'}\n",
      "[trial 32 | fold 0 | ep 000] loss=1.4264  val_balacc=0.500  val_F1=0.526\n",
      "[trial 32 | fold 0 | ep 010] loss=0.6911  val_balacc=0.500  val_F1=0.526\n",
      "[trial 32 | fold 0 | ep 020] loss=0.6946  val_balacc=0.433  val_F1=0.438\n",
      "[trial 32 | fold 0 | ep 030] loss=0.6858  val_balacc=0.672  val_F1=0.533\n",
      "[trial 32 | fold 0 | ep 040] loss=0.6799  val_balacc=0.600  val_F1=0.333\n",
      "[trial 32 | fold 0 | ep 050] loss=0.6872  val_balacc=0.650  val_F1=0.462\n",
      "[trial 32 | fold 0 | ep 060] loss=0.6817  val_balacc=0.650  val_F1=0.462\n",
      "[trial 32 | fold 0 | ep 070] loss=0.6886  val_balacc=0.650  val_F1=0.462\n",
      "[trial 32 | fold 0 | ep 080] loss=0.6637  val_balacc=0.650  val_F1=0.462\n",
      "Early stop @ epoch 82 (fold 0). Best BA was 0.700.\n",
      "fold 0: best val_balacc=0.700, val_F1=0.571\n",
      "[trial 32 | fold 1 | ep 000] loss=2.5496  val_balacc=0.500  val_F1=0.000\n",
      "[trial 32 | fold 1 | ep 010] loss=0.6936  val_balacc=0.500  val_F1=0.500\n",
      "[trial 32 | fold 1 | ep 020] loss=0.6902  val_balacc=0.500  val_F1=0.500\n",
      "[trial 32 | fold 1 | ep 030] loss=0.6906  val_balacc=0.500  val_F1=0.500\n",
      "[trial 32 | fold 1 | ep 040] loss=0.6948  val_balacc=0.500  val_F1=0.500\n",
      "[trial 32 | fold 1 | ep 050] loss=0.6915  val_balacc=0.500  val_F1=0.500\n",
      "Early stop @ epoch 50 (fold 1). Best BA was 0.500.\n",
      "fold 1: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 32 | fold 2 | ep 000] loss=2.0871  val_balacc=0.500  val_F1=0.000\n",
      "[trial 32 | fold 2 | ep 010] loss=0.6920  val_balacc=0.500  val_F1=0.000\n",
      "[trial 32 | fold 2 | ep 020] loss=0.6841  val_balacc=0.491  val_F1=0.154\n",
      "[trial 32 | fold 2 | ep 030] loss=0.6791  val_balacc=0.403  val_F1=0.125\n",
      "[trial 32 | fold 2 | ep 040] loss=0.6842  val_balacc=0.453  val_F1=0.235\n",
      "[trial 32 | fold 2 | ep 050] loss=0.6755  val_balacc=0.453  val_F1=0.235\n",
      "[trial 32 | fold 2 | ep 060] loss=0.6744  val_balacc=0.453  val_F1=0.235\n",
      "Early stop @ epoch 62 (fold 2). Best BA was 0.706.\n",
      "fold 2: best val_balacc=0.706, val_F1=0.667\n",
      "[trial 32 | fold 3 | ep 000] loss=1.8263  val_balacc=0.500  val_F1=0.000\n",
      "[trial 32 | fold 3 | ep 010] loss=0.6922  val_balacc=0.500  val_F1=0.000\n",
      "[trial 32 | fold 3 | ep 020] loss=0.6935  val_balacc=0.500  val_F1=0.000\n",
      "[trial 32 | fold 3 | ep 030] loss=0.6877  val_balacc=0.500  val_F1=0.000\n",
      "[trial 32 | fold 3 | ep 040] loss=0.6872  val_balacc=0.500  val_F1=0.000\n",
      "[trial 32 | fold 3 | ep 050] loss=0.6916  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 3). Best BA was 0.500.\n",
      "fold 3: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 32 | fold 4 | ep 000] loss=3.1445  val_balacc=0.500  val_F1=0.541\n",
      "[trial 32 | fold 4 | ep 010] loss=0.6931  val_balacc=0.500  val_F1=0.000\n",
      "[trial 32 | fold 4 | ep 020] loss=0.6915  val_balacc=0.500  val_F1=0.000\n",
      "[trial 32 | fold 4 | ep 030] loss=0.6841  val_balacc=0.500  val_F1=0.000\n",
      "[trial 32 | fold 4 | ep 040] loss=0.6868  val_balacc=0.500  val_F1=0.000\n",
      "[trial 32 | fold 4 | ep 050] loss=0.6866  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 4). Best BA was 0.500.\n",
      "fold 4: best val_balacc=0.500, val_F1=0.541\n",
      "TRIAL 32: mean bal-acc=0.5810.099, mean F1=0.3560.293\n",
      "\n",
      "--- Trial 33/50 ---\n",
      "Config: {'hidden': 32, 'layers': 2, 'heads': 2, 'dropout': 0.35, 'lr': np.float64(0.0006684819805521592), 'wd': np.float64(0.00013380261545986396), 'readout': 'att'}\n",
      "[trial 33 | fold 0 | ep 000] loss=0.8377  val_balacc=0.500  val_F1=0.000\n",
      "[trial 33 | fold 0 | ep 010] loss=0.7609  val_balacc=0.500  val_F1=0.000\n",
      "[trial 33 | fold 0 | ep 020] loss=0.6894  val_balacc=0.500  val_F1=0.000\n",
      "[trial 33 | fold 0 | ep 030] loss=0.7088  val_balacc=0.500  val_F1=0.000\n",
      "[trial 33 | fold 0 | ep 040] loss=0.6922  val_balacc=0.500  val_F1=0.000\n",
      "[trial 33 | fold 0 | ep 050] loss=0.6963  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 58 (fold 0). Best BA was 0.550.\n",
      "fold 0: best val_balacc=0.550, val_F1=0.182\n",
      "[trial 33 | fold 1 | ep 000] loss=1.2080  val_balacc=0.500  val_F1=0.000\n",
      "[trial 33 | fold 1 | ep 010] loss=0.7077  val_balacc=0.500  val_F1=0.000\n",
      "[trial 33 | fold 1 | ep 020] loss=0.7049  val_balacc=0.528  val_F1=0.514\n",
      "[trial 33 | fold 1 | ep 030] loss=0.6885  val_balacc=0.528  val_F1=0.514\n",
      "[trial 33 | fold 1 | ep 040] loss=0.6872  val_balacc=0.500  val_F1=0.485\n",
      "[trial 33 | fold 1 | ep 050] loss=0.7077  val_balacc=0.500  val_F1=0.485\n",
      "[trial 33 | fold 1 | ep 060] loss=0.6910  val_balacc=0.500  val_F1=0.485\n",
      "Early stop @ epoch 63 (fold 1). Best BA was 0.639.\n",
      "fold 1: best val_balacc=0.639, val_F1=0.560\n",
      "[trial 33 | fold 2 | ep 000] loss=1.4695  val_balacc=0.500  val_F1=0.000\n",
      "[trial 33 | fold 2 | ep 010] loss=0.8132  val_balacc=0.500  val_F1=0.000\n",
      "[trial 33 | fold 2 | ep 020] loss=0.7867  val_balacc=0.500  val_F1=0.000\n",
      "[trial 33 | fold 2 | ep 030] loss=0.7902  val_balacc=0.500  val_F1=0.000\n",
      "[trial 33 | fold 2 | ep 040] loss=0.7895  val_balacc=0.500  val_F1=0.000\n",
      "[trial 33 | fold 2 | ep 050] loss=0.7869  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 2). Best BA was 0.500.\n",
      "fold 2: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 33 | fold 3 | ep 000] loss=0.9998  val_balacc=0.500  val_F1=0.541\n",
      "[trial 33 | fold 3 | ep 010] loss=0.9645  val_balacc=0.500  val_F1=0.541\n",
      "[trial 33 | fold 3 | ep 020] loss=0.7232  val_balacc=0.500  val_F1=0.541\n",
      "[trial 33 | fold 3 | ep 030] loss=0.7046  val_balacc=0.500  val_F1=0.541\n",
      "[trial 33 | fold 3 | ep 040] loss=0.6722  val_balacc=0.500  val_F1=0.541\n",
      "[trial 33 | fold 3 | ep 050] loss=0.8293  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 50 (fold 3). Best BA was 0.500.\n",
      "fold 3: best val_balacc=0.500, val_F1=0.541\n",
      "[trial 33 | fold 4 | ep 000] loss=1.4259  val_balacc=0.500  val_F1=0.000\n",
      "[trial 33 | fold 4 | ep 010] loss=0.9168  val_balacc=0.500  val_F1=0.000\n",
      "[trial 33 | fold 4 | ep 020] loss=0.7535  val_balacc=0.500  val_F1=0.000\n",
      "[trial 33 | fold 4 | ep 030] loss=0.8327  val_balacc=0.500  val_F1=0.000\n",
      "[trial 33 | fold 4 | ep 040] loss=0.9287  val_balacc=0.500  val_F1=0.000\n",
      "[trial 33 | fold 4 | ep 050] loss=0.7834  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 4). Best BA was 0.500.\n",
      "fold 4: best val_balacc=0.500, val_F1=0.000\n",
      "TRIAL 33: mean bal-acc=0.5380.054, mean F1=0.2560.249\n",
      "\n",
      "--- Trial 34/50 ---\n",
      "Config: {'hidden': 32, 'layers': 2, 'heads': 4, 'dropout': 0.301, 'lr': np.float64(0.00046059037176646915), 'wd': np.float64(0.003564561823242047), 'readout': 'att'}\n",
      "[trial 34 | fold 0 | ep 000] loss=0.7775  val_balacc=0.500  val_F1=0.000\n",
      "[trial 34 | fold 0 | ep 010] loss=0.7056  val_balacc=0.500  val_F1=0.000\n",
      "[trial 34 | fold 0 | ep 020] loss=0.6994  val_balacc=0.500  val_F1=0.000\n",
      "[trial 34 | fold 0 | ep 030] loss=0.7047  val_balacc=0.500  val_F1=0.000\n",
      "[trial 34 | fold 0 | ep 040] loss=0.6910  val_balacc=0.500  val_F1=0.000\n",
      "[trial 34 | fold 0 | ep 050] loss=0.6995  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 0). Best BA was 0.500.\n",
      "fold 0: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 34 | fold 1 | ep 000] loss=0.9428  val_balacc=0.528  val_F1=0.514\n",
      "[trial 34 | fold 1 | ep 010] loss=0.6628  val_balacc=0.500  val_F1=0.500\n",
      "[trial 34 | fold 1 | ep 020] loss=0.6923  val_balacc=0.500  val_F1=0.500\n",
      "[trial 34 | fold 1 | ep 030] loss=0.6896  val_balacc=0.500  val_F1=0.500\n",
      "[trial 34 | fold 1 | ep 040] loss=0.6940  val_balacc=0.500  val_F1=0.500\n",
      "[trial 34 | fold 1 | ep 050] loss=0.7765  val_balacc=0.500  val_F1=0.500\n",
      "Early stop @ epoch 56 (fold 1). Best BA was 0.583.\n",
      "fold 1: best val_balacc=0.583, val_F1=0.519\n",
      "[trial 34 | fold 2 | ep 000] loss=0.6996  val_balacc=0.482  val_F1=0.250\n",
      "[trial 34 | fold 2 | ep 010] loss=0.7044  val_balacc=0.388  val_F1=0.400\n",
      "[trial 34 | fold 2 | ep 020] loss=0.6926  val_balacc=0.456  val_F1=0.400\n",
      "[trial 34 | fold 2 | ep 030] loss=0.6926  val_balacc=0.406  val_F1=0.333\n",
      "[trial 34 | fold 2 | ep 040] loss=0.6959  val_balacc=0.456  val_F1=0.400\n",
      "[trial 34 | fold 2 | ep 050] loss=0.6924  val_balacc=0.456  val_F1=0.400\n",
      "Early stop @ epoch 51 (fold 2). Best BA was 0.500.\n",
      "fold 2: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 34 | fold 3 | ep 000] loss=0.8588  val_balacc=0.524  val_F1=0.400\n",
      "[trial 34 | fold 3 | ep 010] loss=0.7308  val_balacc=0.412  val_F1=0.000\n",
      "[trial 34 | fold 3 | ep 020] loss=0.6997  val_balacc=0.412  val_F1=0.000\n",
      "[trial 34 | fold 3 | ep 030] loss=0.6995  val_balacc=0.368  val_F1=0.357\n",
      "[trial 34 | fold 3 | ep 040] loss=0.6956  val_balacc=0.426  val_F1=0.385\n",
      "[trial 34 | fold 3 | ep 050] loss=0.6971  val_balacc=0.456  val_F1=0.400\n",
      "[trial 34 | fold 3 | ep 060] loss=0.6940  val_balacc=0.485  val_F1=0.417\n",
      "Early stop @ epoch 62 (fold 3). Best BA was 0.559.\n",
      "fold 3: best val_balacc=0.559, val_F1=0.571\n",
      "[trial 34 | fold 4 | ep 000] loss=0.9279  val_balacc=0.500  val_F1=0.541\n",
      "[trial 34 | fold 4 | ep 010] loss=0.6751  val_balacc=0.500  val_F1=0.541\n",
      "[trial 34 | fold 4 | ep 020] loss=0.6925  val_balacc=0.500  val_F1=0.541\n",
      "[trial 34 | fold 4 | ep 030] loss=0.7008  val_balacc=0.500  val_F1=0.541\n",
      "[trial 34 | fold 4 | ep 040] loss=0.6952  val_balacc=0.500  val_F1=0.541\n",
      "[trial 34 | fold 4 | ep 050] loss=0.6958  val_balacc=0.500  val_F1=0.541\n",
      "[trial 34 | fold 4 | ep 060] loss=0.6920  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 64 (fold 4). Best BA was 0.529.\n",
      "fold 4: best val_balacc=0.529, val_F1=0.556\n",
      "TRIAL 34: mean bal-acc=0.5340.033, mean F1=0.3290.269\n",
      "\n",
      "--- Trial 35/50 ---\n",
      "Config: {'hidden': 128, 'layers': 3, 'heads': 6, 'dropout': 0.587, 'lr': np.float64(0.004911264127408121), 'wd': np.float64(0.0006464078935523594), 'readout': 's2s'}\n",
      "[trial 35 | fold 0 | ep 000] loss=208.5891  val_balacc=0.428  val_F1=0.457\n",
      "[trial 35 | fold 0 | ep 010] loss=3.1856  val_balacc=0.500  val_F1=0.000\n",
      "[trial 35 | fold 0 | ep 020] loss=0.9088  val_balacc=0.500  val_F1=0.000\n",
      "[trial 35 | fold 0 | ep 030] loss=0.7895  val_balacc=0.522  val_F1=0.167\n",
      "[trial 35 | fold 0 | ep 040] loss=0.9141  val_balacc=0.500  val_F1=0.000\n",
      "[trial 35 | fold 0 | ep 050] loss=0.8450  val_balacc=0.500  val_F1=0.000\n",
      "[trial 35 | fold 0 | ep 060] loss=0.6857  val_balacc=0.500  val_F1=0.000\n",
      "[trial 35 | fold 0 | ep 070] loss=0.7548  val_balacc=0.500  val_F1=0.000\n",
      "[trial 35 | fold 0 | ep 080] loss=0.7980  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 80 (fold 0). Best BA was 0.522.\n",
      "fold 0: best val_balacc=0.522, val_F1=0.167\n",
      "[trial 35 | fold 1 | ep 000] loss=198.8759  val_balacc=0.500  val_F1=0.000\n",
      "[trial 35 | fold 1 | ep 010] loss=35.6003  val_balacc=0.500  val_F1=0.000\n",
      "[trial 35 | fold 1 | ep 020] loss=26.0944  val_balacc=0.500  val_F1=0.000\n",
      "[trial 35 | fold 1 | ep 030] loss=21.5324  val_balacc=0.500  val_F1=0.000\n",
      "[trial 35 | fold 1 | ep 040] loss=15.9273  val_balacc=0.500  val_F1=0.000\n",
      "[trial 35 | fold 1 | ep 050] loss=21.2642  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 1). Best BA was 0.500.\n",
      "fold 1: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 35 | fold 2 | ep 000] loss=94.3696  val_balacc=0.500  val_F1=0.541\n",
      "[trial 35 | fold 2 | ep 010] loss=59.1660  val_balacc=0.500  val_F1=0.000\n",
      "[trial 35 | fold 2 | ep 020] loss=37.6058  val_balacc=0.500  val_F1=0.000\n",
      "[trial 35 | fold 2 | ep 030] loss=21.6331  val_balacc=0.500  val_F1=0.000\n",
      "[trial 35 | fold 2 | ep 040] loss=25.0065  val_balacc=0.500  val_F1=0.000\n",
      "[trial 35 | fold 2 | ep 050] loss=24.6475  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 2). Best BA was 0.500.\n",
      "fold 2: best val_balacc=0.500, val_F1=0.541\n",
      "[trial 35 | fold 3 | ep 000] loss=245.6569  val_balacc=0.500  val_F1=0.541\n",
      "[trial 35 | fold 3 | ep 010] loss=15.0398  val_balacc=0.500  val_F1=0.000\n",
      "[trial 35 | fold 3 | ep 020] loss=5.6323  val_balacc=0.500  val_F1=0.000\n",
      "[trial 35 | fold 3 | ep 030] loss=4.3779  val_balacc=0.500  val_F1=0.000\n",
      "[trial 35 | fold 3 | ep 040] loss=4.2374  val_balacc=0.500  val_F1=0.000\n",
      "[trial 35 | fold 3 | ep 050] loss=4.1753  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 3). Best BA was 0.500.\n",
      "fold 3: best val_balacc=0.500, val_F1=0.541\n",
      "[trial 35 | fold 4 | ep 000] loss=216.5695  val_balacc=0.500  val_F1=0.541\n",
      "[trial 35 | fold 4 | ep 010] loss=32.3375  val_balacc=0.500  val_F1=0.000\n",
      "[trial 35 | fold 4 | ep 020] loss=34.1465  val_balacc=0.500  val_F1=0.541\n",
      "[trial 35 | fold 4 | ep 030] loss=17.5007  val_balacc=0.471  val_F1=0.000\n",
      "[trial 35 | fold 4 | ep 040] loss=16.5547  val_balacc=0.553  val_F1=0.421\n",
      "[trial 35 | fold 4 | ep 050] loss=15.8654  val_balacc=0.459  val_F1=0.485\n",
      "[trial 35 | fold 4 | ep 060] loss=8.8619  val_balacc=0.526  val_F1=0.500\n",
      "[trial 35 | fold 4 | ep 070] loss=10.6688  val_balacc=0.526  val_F1=0.500\n",
      "Early stop @ epoch 72 (fold 4). Best BA was 0.635.\n",
      "fold 4: best val_balacc=0.635, val_F1=0.593\n",
      "TRIAL 35: mean bal-acc=0.5320.053, mean F1=0.3680.239\n",
      "\n",
      "--- Trial 36/50 ---\n",
      "Config: {'hidden': 256, 'layers': 2, 'heads': 2, 'dropout': 0.389, 'lr': np.float64(0.004423929057831093), 'wd': np.float64(0.0007199968327247923), 'readout': 's2s'}\n",
      "[trial 36 | fold 0 | ep 000] loss=15.7603  val_balacc=0.500  val_F1=0.526\n",
      "[trial 36 | fold 0 | ep 010] loss=0.7820  val_balacc=0.500  val_F1=0.000\n",
      "[trial 36 | fold 0 | ep 020] loss=0.7391  val_balacc=0.500  val_F1=0.526\n",
      "[trial 36 | fold 0 | ep 030] loss=0.7199  val_balacc=0.472  val_F1=0.000\n",
      "[trial 36 | fold 0 | ep 040] loss=0.7104  val_balacc=0.500  val_F1=0.526\n",
      "[trial 36 | fold 0 | ep 050] loss=0.6935  val_balacc=0.500  val_F1=0.526\n",
      "[trial 36 | fold 0 | ep 060] loss=0.6971  val_balacc=0.500  val_F1=0.526\n",
      "[trial 36 | fold 0 | ep 070] loss=0.6918  val_balacc=0.500  val_F1=0.526\n",
      "[trial 36 | fold 0 | ep 080] loss=0.7038  val_balacc=0.500  val_F1=0.526\n",
      "Early stop @ epoch 87 (fold 0). Best BA was 0.689.\n",
      "fold 0: best val_balacc=0.689, val_F1=0.600\n",
      "[trial 36 | fold 1 | ep 000] loss=13.2888  val_balacc=0.500  val_F1=0.000\n",
      "[trial 36 | fold 1 | ep 010] loss=0.8070  val_balacc=0.500  val_F1=0.500\n",
      "[trial 36 | fold 1 | ep 020] loss=0.7277  val_balacc=0.500  val_F1=0.000\n",
      "[trial 36 | fold 1 | ep 030] loss=0.6776  val_balacc=0.500  val_F1=0.000\n",
      "[trial 36 | fold 1 | ep 040] loss=0.6910  val_balacc=0.500  val_F1=0.000\n",
      "[trial 36 | fold 1 | ep 050] loss=0.6973  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 55 (fold 1). Best BA was 0.639.\n",
      "fold 1: best val_balacc=0.639, val_F1=0.571\n",
      "[trial 36 | fold 2 | ep 000] loss=15.3264  val_balacc=0.500  val_F1=0.541\n",
      "[trial 36 | fold 2 | ep 010] loss=0.7361  val_balacc=0.500  val_F1=0.541\n",
      "[trial 36 | fold 2 | ep 020] loss=0.7692  val_balacc=0.500  val_F1=0.000\n",
      "[trial 36 | fold 2 | ep 030] loss=0.6915  val_balacc=0.500  val_F1=0.000\n",
      "[trial 36 | fold 2 | ep 040] loss=0.7021  val_balacc=0.403  val_F1=0.125\n",
      "[trial 36 | fold 2 | ep 050] loss=0.7056  val_balacc=0.491  val_F1=0.154\n",
      "Early stop @ epoch 59 (fold 2). Best BA was 0.529.\n",
      "fold 2: best val_balacc=0.529, val_F1=0.556\n",
      "[trial 36 | fold 3 | ep 000] loss=16.0218  val_balacc=0.500  val_F1=0.000\n",
      "[trial 36 | fold 3 | ep 010] loss=0.7895  val_balacc=0.500  val_F1=0.000\n",
      "[trial 36 | fold 3 | ep 020] loss=0.7137  val_balacc=0.524  val_F1=0.400\n",
      "[trial 36 | fold 3 | ep 030] loss=0.6818  val_balacc=0.500  val_F1=0.000\n",
      "[trial 36 | fold 3 | ep 040] loss=0.6962  val_balacc=0.471  val_F1=0.000\n",
      "[trial 36 | fold 3 | ep 050] loss=0.6870  val_balacc=0.585  val_F1=0.538\n",
      "[trial 36 | fold 3 | ep 060] loss=0.6961  val_balacc=0.565  val_F1=0.500\n",
      "[trial 36 | fold 3 | ep 070] loss=0.7115  val_balacc=0.615  val_F1=0.560\n",
      "[trial 36 | fold 3 | ep 080] loss=0.7111  val_balacc=0.615  val_F1=0.560\n",
      "[trial 36 | fold 3 | ep 090] loss=0.7088  val_balacc=0.615  val_F1=0.560\n",
      "[trial 36 | fold 3 | ep 100] loss=0.7033  val_balacc=0.615  val_F1=0.560\n",
      "Early stop @ epoch 105 (fold 3). Best BA was 0.685.\n",
      "fold 3: best val_balacc=0.685, val_F1=0.643\n",
      "[trial 36 | fold 4 | ep 000] loss=10.6265  val_balacc=0.500  val_F1=0.000\n",
      "[trial 36 | fold 4 | ep 010] loss=0.6986  val_balacc=0.500  val_F1=0.541\n",
      "[trial 36 | fold 4 | ep 020] loss=0.6936  val_balacc=0.500  val_F1=0.541\n",
      "[trial 36 | fold 4 | ep 030] loss=0.6898  val_balacc=0.500  val_F1=0.541\n",
      "[trial 36 | fold 4 | ep 040] loss=0.6918  val_balacc=0.500  val_F1=0.541\n",
      "[trial 36 | fold 4 | ep 050] loss=0.6972  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 50 (fold 4). Best BA was 0.500.\n",
      "fold 4: best val_balacc=0.500, val_F1=0.000\n",
      "TRIAL 36: mean bal-acc=0.6080.079, mean F1=0.4740.239\n",
      "\n",
      "--- Trial 37/50 ---\n",
      "Config: {'hidden': 32, 'layers': 4, 'heads': 4, 'dropout': 0.317, 'lr': np.float64(0.0009828793486963993), 'wd': np.float64(0.0005066065145603329), 'readout': 's2s'}\n",
      "[trial 37 | fold 0 | ep 000] loss=5.2851  val_balacc=0.500  val_F1=0.526\n",
      "[trial 37 | fold 0 | ep 010] loss=1.9343  val_balacc=0.533  val_F1=0.529\n",
      "[trial 37 | fold 0 | ep 020] loss=1.3730  val_balacc=0.500  val_F1=0.526\n",
      "[trial 37 | fold 0 | ep 030] loss=1.1388  val_balacc=0.500  val_F1=0.526\n",
      "[trial 37 | fold 0 | ep 040] loss=1.3634  val_balacc=0.500  val_F1=0.526\n",
      "[trial 37 | fold 0 | ep 050] loss=1.5072  val_balacc=0.500  val_F1=0.526\n",
      "Early stop @ epoch 59 (fold 0). Best BA was 0.650.\n",
      "fold 0: best val_balacc=0.650, val_F1=0.593\n",
      "[trial 37 | fold 1 | ep 000] loss=5.0156  val_balacc=0.500  val_F1=0.000\n",
      "[trial 37 | fold 1 | ep 010] loss=2.6694  val_balacc=0.500  val_F1=0.500\n",
      "[trial 37 | fold 1 | ep 020] loss=1.4952  val_balacc=0.444  val_F1=0.348\n",
      "[trial 37 | fold 1 | ep 030] loss=1.1590  val_balacc=0.528  val_F1=0.514\n",
      "[trial 37 | fold 1 | ep 040] loss=0.9069  val_balacc=0.528  val_F1=0.514\n",
      "[trial 37 | fold 1 | ep 050] loss=1.3432  val_balacc=0.528  val_F1=0.514\n",
      "Early stop @ epoch 56 (fold 1). Best BA was 0.639.\n",
      "fold 1: best val_balacc=0.639, val_F1=0.571\n",
      "[trial 37 | fold 2 | ep 000] loss=5.0447  val_balacc=0.500  val_F1=0.000\n",
      "[trial 37 | fold 2 | ep 010] loss=3.5168  val_balacc=0.535  val_F1=0.480\n",
      "[trial 37 | fold 2 | ep 020] loss=1.5157  val_balacc=0.556  val_F1=0.519\n",
      "[trial 37 | fold 2 | ep 030] loss=1.3166  val_balacc=0.488  val_F1=0.500\n",
      "[trial 37 | fold 2 | ep 040] loss=1.3462  val_balacc=0.479  val_F1=0.514\n",
      "[trial 37 | fold 2 | ep 050] loss=1.1286  val_balacc=0.479  val_F1=0.514\n",
      "Early stop @ epoch 57 (fold 2). Best BA was 0.685.\n",
      "fold 2: best val_balacc=0.685, val_F1=0.643\n",
      "[trial 37 | fold 3 | ep 000] loss=3.7282  val_balacc=0.606  val_F1=0.571\n",
      "[trial 37 | fold 3 | ep 010] loss=1.6545  val_balacc=0.500  val_F1=0.000\n",
      "[trial 37 | fold 3 | ep 020] loss=1.4200  val_balacc=0.500  val_F1=0.541\n",
      "[trial 37 | fold 3 | ep 030] loss=1.2601  val_balacc=0.471  val_F1=0.000\n",
      "[trial 37 | fold 3 | ep 040] loss=0.9863  val_balacc=0.506  val_F1=0.462\n",
      "[trial 37 | fold 3 | ep 050] loss=1.0620  val_balacc=0.465  val_F1=0.364\n",
      "[trial 37 | fold 3 | ep 060] loss=0.9696  val_balacc=0.465  val_F1=0.364\n",
      "Early stop @ epoch 61 (fold 3). Best BA was 0.674.\n",
      "fold 3: best val_balacc=0.674, val_F1=0.609\n",
      "[trial 37 | fold 4 | ep 000] loss=5.0298  val_balacc=0.400  val_F1=0.457\n",
      "[trial 37 | fold 4 | ep 010] loss=1.9372  val_balacc=0.500  val_F1=0.000\n",
      "[trial 37 | fold 4 | ep 020] loss=1.2762  val_balacc=0.585  val_F1=0.538\n",
      "[trial 37 | fold 4 | ep 030] loss=0.9220  val_balacc=0.500  val_F1=0.000\n",
      "[trial 37 | fold 4 | ep 040] loss=0.8577  val_balacc=0.441  val_F1=0.000\n",
      "[trial 37 | fold 4 | ep 050] loss=1.0761  val_balacc=0.462  val_F1=0.143\n",
      "[trial 37 | fold 4 | ep 060] loss=0.9418  val_balacc=0.491  val_F1=0.154\n",
      "Early stop @ epoch 65 (fold 4). Best BA was 0.674.\n",
      "fold 4: best val_balacc=0.674, val_F1=0.609\n",
      "TRIAL 37: mean bal-acc=0.6640.017, mean F1=0.6050.023\n",
      "\n",
      "--- Trial 38/50 ---\n",
      "Config: {'hidden': 64, 'layers': 2, 'heads': 6, 'dropout': 0.324, 'lr': np.float64(0.00020687644030934044), 'wd': np.float64(0.0007745335190802323), 'readout': 's2s'}\n",
      "[trial 38 | fold 0 | ep 000] loss=0.8109  val_balacc=0.500  val_F1=0.526\n",
      "[trial 38 | fold 0 | ep 010] loss=0.8078  val_balacc=0.500  val_F1=0.526\n",
      "[trial 38 | fold 0 | ep 020] loss=0.7117  val_balacc=0.500  val_F1=0.526\n",
      "[trial 38 | fold 0 | ep 030] loss=0.6920  val_balacc=0.500  val_F1=0.526\n",
      "[trial 38 | fold 0 | ep 040] loss=0.7377  val_balacc=0.500  val_F1=0.526\n",
      "[trial 38 | fold 0 | ep 050] loss=0.7491  val_balacc=0.500  val_F1=0.526\n",
      "Early stop @ epoch 51 (fold 0). Best BA was 0.594.\n",
      "fold 0: best val_balacc=0.594, val_F1=0.400\n",
      "[trial 38 | fold 1 | ep 000] loss=0.8695  val_balacc=0.500  val_F1=0.500\n",
      "[trial 38 | fold 1 | ep 010] loss=0.6958  val_balacc=0.500  val_F1=0.500\n",
      "[trial 38 | fold 1 | ep 020] loss=0.6930  val_balacc=0.500  val_F1=0.000\n",
      "[trial 38 | fold 1 | ep 030] loss=0.6932  val_balacc=0.500  val_F1=0.000\n",
      "[trial 38 | fold 1 | ep 040] loss=0.6949  val_balacc=0.472  val_F1=0.000\n",
      "[trial 38 | fold 1 | ep 050] loss=0.6953  val_balacc=0.472  val_F1=0.000\n",
      "Early stop @ epoch 51 (fold 1). Best BA was 0.528.\n",
      "fold 1: best val_balacc=0.528, val_F1=0.514\n",
      "[trial 38 | fold 2 | ep 000] loss=0.7070  val_balacc=0.500  val_F1=0.000\n",
      "[trial 38 | fold 2 | ep 010] loss=0.6916  val_balacc=0.388  val_F1=0.400\n",
      "[trial 38 | fold 2 | ep 020] loss=0.6902  val_balacc=0.409  val_F1=0.438\n",
      "[trial 38 | fold 2 | ep 030] loss=0.6880  val_balacc=0.388  val_F1=0.400\n",
      "[trial 38 | fold 2 | ep 040] loss=0.6920  val_balacc=0.409  val_F1=0.438\n",
      "[trial 38 | fold 2 | ep 050] loss=0.6928  val_balacc=0.409  val_F1=0.438\n",
      "Early stop @ epoch 50 (fold 2). Best BA was 0.500.\n",
      "fold 2: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 38 | fold 3 | ep 000] loss=0.8323  val_balacc=0.500  val_F1=0.541\n",
      "[trial 38 | fold 3 | ep 010] loss=0.6924  val_balacc=0.500  val_F1=0.000\n",
      "[trial 38 | fold 3 | ep 020] loss=0.6951  val_balacc=0.500  val_F1=0.541\n",
      "[trial 38 | fold 3 | ep 030] loss=0.6915  val_balacc=0.432  val_F1=0.133\n",
      "[trial 38 | fold 3 | ep 040] loss=0.6927  val_balacc=0.471  val_F1=0.000\n",
      "[trial 38 | fold 3 | ep 050] loss=0.6902  val_balacc=0.412  val_F1=0.000\n",
      "[trial 38 | fold 3 | ep 060] loss=0.6878  val_balacc=0.412  val_F1=0.000\n",
      "Early stop @ epoch 61 (fold 3). Best BA was 0.559.\n",
      "fold 3: best val_balacc=0.559, val_F1=0.571\n",
      "[trial 38 | fold 4 | ep 000] loss=0.9266  val_balacc=0.500  val_F1=0.541\n",
      "[trial 38 | fold 4 | ep 010] loss=0.8315  val_balacc=0.394  val_F1=0.211\n",
      "[trial 38 | fold 4 | ep 020] loss=0.7262  val_balacc=0.500  val_F1=0.541\n",
      "[trial 38 | fold 4 | ep 030] loss=0.8331  val_balacc=0.500  val_F1=0.541\n",
      "[trial 38 | fold 4 | ep 040] loss=0.7371  val_balacc=0.500  val_F1=0.541\n",
      "[trial 38 | fold 4 | ep 050] loss=0.7627  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 53 (fold 4). Best BA was 0.676.\n",
      "fold 4: best val_balacc=0.676, val_F1=0.645\n",
      "TRIAL 38: mean bal-acc=0.5720.061, mean F1=0.4260.228\n",
      "\n",
      "--- Trial 39/50 ---\n",
      "Config: {'hidden': 64, 'layers': 3, 'heads': 2, 'dropout': 0.582, 'lr': np.float64(0.00092867003810773), 'wd': np.float64(0.0007191311801704242), 'readout': 'mean'}\n",
      "[trial 39 | fold 0 | ep 000] loss=3.5602  val_balacc=0.500  val_F1=0.000\n",
      "[trial 39 | fold 0 | ep 010] loss=0.7313  val_balacc=0.500  val_F1=0.000\n",
      "[trial 39 | fold 0 | ep 020] loss=0.7330  val_balacc=0.500  val_F1=0.000\n",
      "[trial 39 | fold 0 | ep 030] loss=0.7344  val_balacc=0.500  val_F1=0.000\n",
      "[trial 39 | fold 0 | ep 040] loss=0.6880  val_balacc=0.500  val_F1=0.000\n",
      "[trial 39 | fold 0 | ep 050] loss=0.6851  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 52 (fold 0). Best BA was 0.572.\n",
      "fold 0: best val_balacc=0.572, val_F1=0.308\n",
      "[trial 39 | fold 1 | ep 000] loss=3.1942  val_balacc=0.500  val_F1=0.000\n",
      "[trial 39 | fold 1 | ep 010] loss=0.8624  val_balacc=0.500  val_F1=0.000\n",
      "[trial 39 | fold 1 | ep 020] loss=0.6945  val_balacc=0.500  val_F1=0.000\n",
      "[trial 39 | fold 1 | ep 030] loss=0.7563  val_balacc=0.500  val_F1=0.000\n",
      "[trial 39 | fold 1 | ep 040] loss=0.7404  val_balacc=0.500  val_F1=0.000\n",
      "[trial 39 | fold 1 | ep 050] loss=0.7162  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 1). Best BA was 0.500.\n",
      "fold 1: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 39 | fold 2 | ep 000] loss=2.4990  val_balacc=0.474  val_F1=0.316\n",
      "[trial 39 | fold 2 | ep 010] loss=0.7098  val_balacc=0.368  val_F1=0.357\n",
      "[trial 39 | fold 2 | ep 020] loss=0.7495  val_balacc=0.500  val_F1=0.541\n",
      "[trial 39 | fold 2 | ep 030] loss=0.7111  val_balacc=0.500  val_F1=0.541\n",
      "[trial 39 | fold 2 | ep 040] loss=0.6999  val_balacc=0.500  val_F1=0.541\n",
      "[trial 39 | fold 2 | ep 050] loss=0.7020  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 51 (fold 2). Best BA was 0.500.\n",
      "fold 2: best val_balacc=0.500, val_F1=0.541\n",
      "[trial 39 | fold 3 | ep 000] loss=2.6270  val_balacc=0.500  val_F1=0.000\n",
      "[trial 39 | fold 3 | ep 010] loss=0.7635  val_balacc=0.500  val_F1=0.000\n",
      "[trial 39 | fold 3 | ep 020] loss=0.7120  val_balacc=0.500  val_F1=0.000\n",
      "[trial 39 | fold 3 | ep 030] loss=0.7034  val_balacc=0.500  val_F1=0.000\n",
      "[trial 39 | fold 3 | ep 040] loss=0.7072  val_balacc=0.500  val_F1=0.000\n",
      "[trial 39 | fold 3 | ep 050] loss=0.7066  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 3). Best BA was 0.500.\n",
      "fold 3: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 39 | fold 4 | ep 000] loss=5.0067  val_balacc=0.500  val_F1=0.000\n",
      "[trial 39 | fold 4 | ep 010] loss=0.8802  val_balacc=0.521  val_F1=0.167\n",
      "[trial 39 | fold 4 | ep 020] loss=0.7388  val_balacc=0.500  val_F1=0.000\n",
      "[trial 39 | fold 4 | ep 030] loss=0.7044  val_balacc=0.594  val_F1=0.522\n",
      "[trial 39 | fold 4 | ep 040] loss=0.7166  val_balacc=0.594  val_F1=0.522\n",
      "[trial 39 | fold 4 | ep 050] loss=0.7006  val_balacc=0.553  val_F1=0.421\n",
      "[trial 39 | fold 4 | ep 060] loss=0.7048  val_balacc=0.653  val_F1=0.571\n",
      "[trial 39 | fold 4 | ep 070] loss=0.6983  val_balacc=0.653  val_F1=0.571\n",
      "[trial 39 | fold 4 | ep 080] loss=0.6863  val_balacc=0.653  val_F1=0.571\n",
      "[trial 39 | fold 4 | ep 090] loss=0.6745  val_balacc=0.653  val_F1=0.571\n",
      "Early stop @ epoch 94 (fold 4). Best BA was 0.653.\n",
      "fold 4: best val_balacc=0.653, val_F1=0.571\n",
      "TRIAL 39: mean bal-acc=0.5450.061, mean F1=0.2840.249\n",
      "\n",
      "--- Trial 40/50 ---\n",
      "Config: {'hidden': 32, 'layers': 3, 'heads': 6, 'dropout': 0.475, 'lr': np.float64(0.0007730225312700235), 'wd': np.float64(0.0037015406217027567), 'readout': 'mean'}\n",
      "[trial 40 | fold 0 | ep 000] loss=1.0636  val_balacc=0.472  val_F1=0.000\n",
      "[trial 40 | fold 0 | ep 010] loss=0.6940  val_balacc=0.500  val_F1=0.000\n",
      "[trial 40 | fold 0 | ep 020] loss=0.7121  val_balacc=0.500  val_F1=0.000\n",
      "[trial 40 | fold 0 | ep 030] loss=0.7041  val_balacc=0.500  val_F1=0.000\n",
      "[trial 40 | fold 0 | ep 040] loss=0.7046  val_balacc=0.500  val_F1=0.000\n",
      "[trial 40 | fold 0 | ep 050] loss=0.6847  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 52 (fold 0). Best BA was 0.500.\n",
      "fold 0: best val_balacc=0.500, val_F1=0.526\n",
      "[trial 40 | fold 1 | ep 000] loss=1.2275  val_balacc=0.500  val_F1=0.000\n",
      "[trial 40 | fold 1 | ep 010] loss=0.7404  val_balacc=0.500  val_F1=0.000\n",
      "[trial 40 | fold 1 | ep 020] loss=0.7167  val_balacc=0.500  val_F1=0.500\n",
      "[trial 40 | fold 1 | ep 030] loss=0.7120  val_balacc=0.500  val_F1=0.500\n",
      "[trial 40 | fold 1 | ep 040] loss=0.7099  val_balacc=0.500  val_F1=0.500\n",
      "[trial 40 | fold 1 | ep 050] loss=0.7086  val_balacc=0.500  val_F1=0.500\n",
      "Early stop @ epoch 50 (fold 1). Best BA was 0.500.\n",
      "fold 1: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 40 | fold 2 | ep 000] loss=1.4566  val_balacc=0.500  val_F1=0.541\n",
      "[trial 40 | fold 2 | ep 010] loss=0.6813  val_balacc=0.500  val_F1=0.000\n",
      "[trial 40 | fold 2 | ep 020] loss=0.7149  val_balacc=0.500  val_F1=0.000\n",
      "[trial 40 | fold 2 | ep 030] loss=0.7027  val_balacc=0.500  val_F1=0.000\n",
      "[trial 40 | fold 2 | ep 040] loss=0.6981  val_balacc=0.500  val_F1=0.000\n",
      "[trial 40 | fold 2 | ep 050] loss=0.7087  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 2). Best BA was 0.500.\n",
      "fold 2: best val_balacc=0.500, val_F1=0.541\n",
      "[trial 40 | fold 3 | ep 000] loss=1.7435  val_balacc=0.500  val_F1=0.541\n",
      "[trial 40 | fold 3 | ep 010] loss=0.7350  val_balacc=0.500  val_F1=0.000\n",
      "[trial 40 | fold 3 | ep 020] loss=0.7005  val_balacc=0.500  val_F1=0.000\n",
      "[trial 40 | fold 3 | ep 030] loss=0.7178  val_balacc=0.500  val_F1=0.000\n",
      "[trial 40 | fold 3 | ep 040] loss=0.7139  val_balacc=0.500  val_F1=0.000\n",
      "[trial 40 | fold 3 | ep 050] loss=0.7076  val_balacc=0.500  val_F1=0.000\n",
      "[trial 40 | fold 3 | ep 060] loss=0.7402  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 62 (fold 3). Best BA was 0.512.\n",
      "fold 3: best val_balacc=0.512, val_F1=0.267\n",
      "[trial 40 | fold 4 | ep 000] loss=1.8372  val_balacc=0.500  val_F1=0.000\n",
      "[trial 40 | fold 4 | ep 010] loss=0.7656  val_balacc=0.500  val_F1=0.541\n",
      "[trial 40 | fold 4 | ep 020] loss=0.6840  val_balacc=0.500  val_F1=0.541\n",
      "[trial 40 | fold 4 | ep 030] loss=0.7109  val_balacc=0.500  val_F1=0.541\n",
      "[trial 40 | fold 4 | ep 040] loss=0.6970  val_balacc=0.500  val_F1=0.541\n",
      "[trial 40 | fold 4 | ep 050] loss=0.7030  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 52 (fold 4). Best BA was 0.506.\n",
      "fold 4: best val_balacc=0.506, val_F1=0.462\n",
      "TRIAL 40: mean bal-acc=0.5040.005, mean F1=0.3590.204\n",
      "\n",
      "--- Trial 41/50 ---\n",
      "Config: {'hidden': 128, 'layers': 2, 'heads': 4, 'dropout': 0.419, 'lr': np.float64(0.0013841448494780932), 'wd': np.float64(0.00019905090841741991), 'readout': 'att'}\n",
      "[trial 41 | fold 0 | ep 000] loss=1.5078  val_balacc=0.500  val_F1=0.000\n",
      "[trial 41 | fold 0 | ep 010] loss=0.7081  val_balacc=0.500  val_F1=0.526\n",
      "[trial 41 | fold 0 | ep 020] loss=0.6915  val_balacc=0.500  val_F1=0.526\n",
      "[trial 41 | fold 0 | ep 030] loss=0.7004  val_balacc=0.500  val_F1=0.526\n",
      "[trial 41 | fold 0 | ep 040] loss=0.6932  val_balacc=0.500  val_F1=0.526\n",
      "[trial 41 | fold 0 | ep 050] loss=0.6973  val_balacc=0.500  val_F1=0.526\n",
      "Early stop @ epoch 50 (fold 0). Best BA was 0.500.\n",
      "fold 0: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 41 | fold 1 | ep 000] loss=1.0343  val_balacc=0.500  val_F1=0.500\n",
      "[trial 41 | fold 1 | ep 010] loss=0.7052  val_balacc=0.500  val_F1=0.000\n",
      "[trial 41 | fold 1 | ep 020] loss=0.6947  val_balacc=0.500  val_F1=0.000\n",
      "[trial 41 | fold 1 | ep 030] loss=0.6917  val_balacc=0.500  val_F1=0.000\n",
      "[trial 41 | fold 1 | ep 040] loss=0.6905  val_balacc=0.500  val_F1=0.000\n",
      "[trial 41 | fold 1 | ep 050] loss=0.6875  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 1). Best BA was 0.500.\n",
      "fold 1: best val_balacc=0.500, val_F1=0.500\n",
      "[trial 41 | fold 2 | ep 000] loss=1.1840  val_balacc=0.500  val_F1=0.000\n",
      "[trial 41 | fold 2 | ep 010] loss=0.6990  val_balacc=0.500  val_F1=0.541\n",
      "[trial 41 | fold 2 | ep 020] loss=0.6895  val_balacc=0.429  val_F1=0.471\n",
      "[trial 41 | fold 2 | ep 030] loss=0.6969  val_balacc=0.500  val_F1=0.541\n",
      "[trial 41 | fold 2 | ep 040] loss=0.6904  val_balacc=0.500  val_F1=0.541\n",
      "[trial 41 | fold 2 | ep 050] loss=0.6902  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 50 (fold 2). Best BA was 0.500.\n",
      "fold 2: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 41 | fold 3 | ep 000] loss=1.4589  val_balacc=0.500  val_F1=0.000\n",
      "[trial 41 | fold 3 | ep 010] loss=0.6934  val_balacc=0.500  val_F1=0.000\n",
      "[trial 41 | fold 3 | ep 020] loss=0.7020  val_balacc=0.500  val_F1=0.000\n",
      "[trial 41 | fold 3 | ep 030] loss=0.6854  val_balacc=0.500  val_F1=0.000\n",
      "[trial 41 | fold 3 | ep 040] loss=0.6860  val_balacc=0.500  val_F1=0.000\n",
      "[trial 41 | fold 3 | ep 050] loss=0.6860  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 56 (fold 3). Best BA was 0.568.\n",
      "fold 3: best val_balacc=0.568, val_F1=0.562\n",
      "[trial 41 | fold 4 | ep 000] loss=1.1686  val_balacc=0.500  val_F1=0.000\n",
      "[trial 41 | fold 4 | ep 010] loss=0.6985  val_balacc=0.500  val_F1=0.541\n",
      "[trial 41 | fold 4 | ep 020] loss=0.6907  val_balacc=0.500  val_F1=0.541\n",
      "[trial 41 | fold 4 | ep 030] loss=0.6915  val_balacc=0.500  val_F1=0.541\n",
      "[trial 41 | fold 4 | ep 040] loss=0.6974  val_balacc=0.500  val_F1=0.541\n",
      "[trial 41 | fold 4 | ep 050] loss=0.6920  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 50 (fold 4). Best BA was 0.500.\n",
      "fold 4: best val_balacc=0.500, val_F1=0.000\n",
      "TRIAL 41: mean bal-acc=0.5140.027, mean F1=0.2120.261\n",
      "\n",
      "--- Trial 42/50 ---\n",
      "Config: {'hidden': 32, 'layers': 2, 'heads': 4, 'dropout': 0.486, 'lr': np.float64(0.0009047098455470519), 'wd': np.float64(7.923781699209354e-05), 'readout': 's2s'}\n",
      "[trial 42 | fold 0 | ep 000] loss=1.5932  val_balacc=0.528  val_F1=0.435\n",
      "[trial 42 | fold 0 | ep 010] loss=1.6113  val_balacc=0.628  val_F1=0.560\n",
      "[trial 42 | fold 0 | ep 020] loss=1.2927  val_balacc=0.450  val_F1=0.486\n",
      "[trial 42 | fold 0 | ep 030] loss=1.0190  val_balacc=0.500  val_F1=0.526\n",
      "[trial 42 | fold 0 | ep 040] loss=1.1912  val_balacc=0.500  val_F1=0.526\n",
      "[trial 42 | fold 0 | ep 050] loss=0.8329  val_balacc=0.500  val_F1=0.526\n",
      "Early stop @ epoch 55 (fold 0). Best BA was 0.739.\n",
      "fold 0: best val_balacc=0.739, val_F1=0.667\n",
      "[trial 42 | fold 1 | ep 000] loss=1.5519  val_balacc=0.500  val_F1=0.000\n",
      "[trial 42 | fold 1 | ep 010] loss=1.4785  val_balacc=0.500  val_F1=0.500\n",
      "[trial 42 | fold 1 | ep 020] loss=1.0768  val_balacc=0.500  val_F1=0.500\n",
      "[trial 42 | fold 1 | ep 030] loss=1.0832  val_balacc=0.500  val_F1=0.500\n",
      "[trial 42 | fold 1 | ep 040] loss=1.2957  val_balacc=0.500  val_F1=0.500\n",
      "[trial 42 | fold 1 | ep 050] loss=1.2558  val_balacc=0.500  val_F1=0.500\n",
      "Early stop @ epoch 54 (fold 1). Best BA was 0.639.\n",
      "fold 1: best val_balacc=0.639, val_F1=0.560\n",
      "[trial 42 | fold 2 | ep 000] loss=1.7817  val_balacc=0.541  val_F1=0.286\n",
      "[trial 42 | fold 2 | ep 010] loss=1.3945  val_balacc=0.529  val_F1=0.556\n",
      "[trial 42 | fold 2 | ep 020] loss=1.0249  val_balacc=0.500  val_F1=0.541\n",
      "[trial 42 | fold 2 | ep 030] loss=1.0515  val_balacc=0.500  val_F1=0.541\n",
      "[trial 42 | fold 2 | ep 040] loss=1.1930  val_balacc=0.500  val_F1=0.541\n",
      "[trial 42 | fold 2 | ep 050] loss=1.1541  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 50 (fold 2). Best BA was 0.541.\n",
      "fold 2: best val_balacc=0.541, val_F1=0.286\n",
      "[trial 42 | fold 3 | ep 000] loss=2.4841  val_balacc=0.500  val_F1=0.541\n",
      "[trial 42 | fold 3 | ep 010] loss=2.0172  val_balacc=0.500  val_F1=0.541\n",
      "[trial 42 | fold 3 | ep 020] loss=1.1056  val_balacc=0.544  val_F1=0.455\n",
      "[trial 42 | fold 3 | ep 030] loss=0.9411  val_balacc=0.568  val_F1=0.562\n",
      "[trial 42 | fold 3 | ep 040] loss=0.9902  val_balacc=0.626  val_F1=0.600\n",
      "[trial 42 | fold 3 | ep 050] loss=0.9669  val_balacc=0.618  val_F1=0.606\n",
      "[trial 42 | fold 3 | ep 060] loss=0.8746  val_balacc=0.618  val_F1=0.606\n",
      "[trial 42 | fold 3 | ep 070] loss=1.2508  val_balacc=0.647  val_F1=0.625\n",
      "[trial 42 | fold 3 | ep 080] loss=0.9983  val_balacc=0.647  val_F1=0.625\n",
      "[trial 42 | fold 3 | ep 090] loss=0.9947  val_balacc=0.647  val_F1=0.625\n",
      "[trial 42 | fold 3 | ep 100] loss=0.8780  val_balacc=0.647  val_F1=0.625\n",
      "Early stop @ epoch 109 (fold 3). Best BA was 0.647.\n",
      "fold 3: best val_balacc=0.647, val_F1=0.625\n",
      "[trial 42 | fold 4 | ep 000] loss=2.3456  val_balacc=0.500  val_F1=0.000\n",
      "[trial 42 | fold 4 | ep 010] loss=2.3429  val_balacc=0.500  val_F1=0.541\n",
      "[trial 42 | fold 4 | ep 020] loss=0.9176  val_balacc=0.471  val_F1=0.000\n",
      "[trial 42 | fold 4 | ep 030] loss=0.8136  val_balacc=0.500  val_F1=0.541\n",
      "[trial 42 | fold 4 | ep 040] loss=0.8714  val_balacc=0.500  val_F1=0.541\n",
      "[trial 42 | fold 4 | ep 050] loss=0.8036  val_balacc=0.500  val_F1=0.541\n",
      "[trial 42 | fold 4 | ep 060] loss=0.7918  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 68 (fold 4). Best BA was 0.582.\n",
      "fold 4: best val_balacc=0.582, val_F1=0.444\n",
      "TRIAL 42: mean bal-acc=0.6300.067, mean F1=0.5160.138\n",
      "\n",
      "--- Trial 43/50 ---\n",
      "Config: {'hidden': 64, 'layers': 4, 'heads': 4, 'dropout': 0.34, 'lr': np.float64(0.00039170177706669695), 'wd': np.float64(6.863610539538903e-05), 'readout': 'mean'}\n",
      "[trial 43 | fold 0 | ep 000] loss=1.7772  val_balacc=0.500  val_F1=0.526\n",
      "[trial 43 | fold 0 | ep 010] loss=0.7585  val_balacc=0.500  val_F1=0.526\n",
      "[trial 43 | fold 0 | ep 020] loss=0.7533  val_balacc=0.500  val_F1=0.526\n",
      "[trial 43 | fold 0 | ep 030] loss=0.7227  val_balacc=0.500  val_F1=0.526\n",
      "[trial 43 | fold 0 | ep 040] loss=0.7109  val_balacc=0.500  val_F1=0.526\n",
      "[trial 43 | fold 0 | ep 050] loss=0.7332  val_balacc=0.500  val_F1=0.526\n",
      "Early stop @ epoch 50 (fold 0). Best BA was 0.500.\n",
      "fold 0: best val_balacc=0.500, val_F1=0.526\n",
      "[trial 43 | fold 1 | ep 000] loss=1.4592  val_balacc=0.500  val_F1=0.000\n",
      "[trial 43 | fold 1 | ep 010] loss=0.8728  val_balacc=0.528  val_F1=0.182\n",
      "[trial 43 | fold 1 | ep 020] loss=0.7229  val_balacc=0.500  val_F1=0.500\n",
      "[trial 43 | fold 1 | ep 030] loss=0.7132  val_balacc=0.500  val_F1=0.500\n",
      "[trial 43 | fold 1 | ep 040] loss=0.7174  val_balacc=0.500  val_F1=0.500\n",
      "[trial 43 | fold 1 | ep 050] loss=0.7432  val_balacc=0.500  val_F1=0.500\n",
      "Early stop @ epoch 55 (fold 1). Best BA was 0.583.\n",
      "fold 1: best val_balacc=0.583, val_F1=0.333\n",
      "[trial 43 | fold 2 | ep 000] loss=1.6161  val_balacc=0.500  val_F1=0.541\n",
      "[trial 43 | fold 2 | ep 010] loss=0.8198  val_balacc=0.500  val_F1=0.541\n",
      "[trial 43 | fold 2 | ep 020] loss=0.7249  val_balacc=0.450  val_F1=0.500\n",
      "[trial 43 | fold 2 | ep 030] loss=0.7225  val_balacc=0.500  val_F1=0.541\n",
      "[trial 43 | fold 2 | ep 040] loss=0.7297  val_balacc=0.500  val_F1=0.541\n",
      "[trial 43 | fold 2 | ep 050] loss=0.7087  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 50 (fold 2). Best BA was 0.500.\n",
      "fold 2: best val_balacc=0.500, val_F1=0.541\n",
      "[trial 43 | fold 3 | ep 000] loss=1.1397  val_balacc=0.500  val_F1=0.000\n",
      "[trial 43 | fold 3 | ep 010] loss=0.7518  val_balacc=0.500  val_F1=0.541\n",
      "[trial 43 | fold 3 | ep 020] loss=0.7423  val_balacc=0.500  val_F1=0.541\n",
      "[trial 43 | fold 3 | ep 030] loss=0.7046  val_balacc=0.500  val_F1=0.541\n",
      "[trial 43 | fold 3 | ep 040] loss=0.7156  val_balacc=0.500  val_F1=0.541\n",
      "[trial 43 | fold 3 | ep 050] loss=0.7014  val_balacc=0.500  val_F1=0.541\n",
      "[trial 43 | fold 3 | ep 060] loss=0.6910  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 67 (fold 3). Best BA was 0.626.\n",
      "fold 3: best val_balacc=0.626, val_F1=0.600\n",
      "[trial 43 | fold 4 | ep 000] loss=2.0368  val_balacc=0.500  val_F1=0.541\n",
      "[trial 43 | fold 4 | ep 010] loss=0.7296  val_balacc=0.500  val_F1=0.541\n",
      "[trial 43 | fold 4 | ep 020] loss=0.7211  val_balacc=0.432  val_F1=0.133\n",
      "[trial 43 | fold 4 | ep 030] loss=0.7207  val_balacc=0.450  val_F1=0.500\n",
      "[trial 43 | fold 4 | ep 040] loss=0.6956  val_balacc=0.500  val_F1=0.541\n",
      "[trial 43 | fold 4 | ep 050] loss=0.7335  val_balacc=0.450  val_F1=0.500\n",
      "[trial 43 | fold 4 | ep 060] loss=0.7005  val_balacc=0.450  val_F1=0.500\n",
      "Early stop @ epoch 65 (fold 4). Best BA was 0.521.\n",
      "fold 4: best val_balacc=0.521, val_F1=0.167\n",
      "TRIAL 43: mean bal-acc=0.5460.050, mean F1=0.4330.161\n",
      "\n",
      "--- Trial 44/50 ---\n",
      "Config: {'hidden': 128, 'layers': 3, 'heads': 2, 'dropout': 0.431, 'lr': np.float64(0.0008373577104687902), 'wd': np.float64(0.00020137441430723998), 'readout': 's2s'}\n",
      "[trial 44 | fold 0 | ep 000] loss=4.6559  val_balacc=0.500  val_F1=0.000\n",
      "[trial 44 | fold 0 | ep 010] loss=4.1206  val_balacc=0.567  val_F1=0.533\n",
      "[trial 44 | fold 0 | ep 020] loss=1.3389  val_balacc=0.506  val_F1=0.514\n",
      "[trial 44 | fold 0 | ep 030] loss=1.2998  val_balacc=0.500  val_F1=0.526\n",
      "[trial 44 | fold 0 | ep 040] loss=0.9830  val_balacc=0.500  val_F1=0.526\n",
      "[trial 44 | fold 0 | ep 050] loss=1.0995  val_balacc=0.500  val_F1=0.526\n",
      "[trial 44 | fold 0 | ep 060] loss=1.0831  val_balacc=0.500  val_F1=0.526\n",
      "Early stop @ epoch 63 (fold 0). Best BA was 0.617.\n",
      "fold 0: best val_balacc=0.617, val_F1=0.471\n",
      "[trial 44 | fold 1 | ep 000] loss=3.7712  val_balacc=0.500  val_F1=0.000\n",
      "[trial 44 | fold 1 | ep 010] loss=2.8826  val_balacc=0.500  val_F1=0.000\n",
      "[trial 44 | fold 1 | ep 020] loss=1.2720  val_balacc=0.500  val_F1=0.000\n",
      "[trial 44 | fold 1 | ep 030] loss=1.3338  val_balacc=0.556  val_F1=0.480\n",
      "[trial 44 | fold 1 | ep 040] loss=1.2650  val_balacc=0.361  val_F1=0.261\n",
      "[trial 44 | fold 1 | ep 050] loss=1.0679  val_balacc=0.389  val_F1=0.273\n",
      "[trial 44 | fold 1 | ep 060] loss=0.8370  val_balacc=0.361  val_F1=0.261\n",
      "Early stop @ epoch 63 (fold 1). Best BA was 0.556.\n",
      "fold 1: best val_balacc=0.556, val_F1=0.500\n",
      "[trial 44 | fold 2 | ep 000] loss=4.3474  val_balacc=0.500  val_F1=0.541\n",
      "[trial 44 | fold 2 | ep 010] loss=2.9352  val_balacc=0.500  val_F1=0.000\n",
      "[trial 44 | fold 2 | ep 020] loss=1.9083  val_balacc=0.529  val_F1=0.556\n",
      "[trial 44 | fold 2 | ep 030] loss=0.9452  val_balacc=0.529  val_F1=0.556\n",
      "[trial 44 | fold 2 | ep 040] loss=0.9597  val_balacc=0.500  val_F1=0.541\n",
      "[trial 44 | fold 2 | ep 050] loss=0.8602  val_balacc=0.500  val_F1=0.541\n",
      "[trial 44 | fold 2 | ep 060] loss=0.8822  val_balacc=0.500  val_F1=0.541\n",
      "[trial 44 | fold 2 | ep 070] loss=0.8411  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 72 (fold 2). Best BA was 0.606.\n",
      "fold 2: best val_balacc=0.606, val_F1=0.571\n",
      "[trial 44 | fold 3 | ep 000] loss=4.9423  val_balacc=0.500  val_F1=0.541\n",
      "[trial 44 | fold 3 | ep 010] loss=3.6913  val_balacc=0.488  val_F1=0.500\n",
      "[trial 44 | fold 3 | ep 020] loss=1.9438  val_balacc=0.450  val_F1=0.500\n",
      "[trial 44 | fold 3 | ep 030] loss=1.2871  val_balacc=0.500  val_F1=0.541\n",
      "[trial 44 | fold 3 | ep 040] loss=1.2112  val_balacc=0.500  val_F1=0.541\n",
      "[trial 44 | fold 3 | ep 050] loss=1.2599  val_balacc=0.500  val_F1=0.541\n",
      "[trial 44 | fold 3 | ep 060] loss=1.0927  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 62 (fold 3). Best BA was 0.568.\n",
      "fold 3: best val_balacc=0.568, val_F1=0.562\n",
      "[trial 44 | fold 4 | ep 000] loss=3.6825  val_balacc=0.500  val_F1=0.541\n",
      "[trial 44 | fold 4 | ep 010] loss=2.4693  val_balacc=0.500  val_F1=0.541\n",
      "[trial 44 | fold 4 | ep 020] loss=1.4667  val_balacc=0.500  val_F1=0.000\n",
      "[trial 44 | fold 4 | ep 030] loss=1.5846  val_balacc=0.500  val_F1=0.000\n",
      "[trial 44 | fold 4 | ep 040] loss=1.4000  val_balacc=0.500  val_F1=0.000\n",
      "[trial 44 | fold 4 | ep 050] loss=1.2402  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 54 (fold 4). Best BA was 0.647.\n",
      "fold 4: best val_balacc=0.647, val_F1=0.625\n",
      "TRIAL 44: mean bal-acc=0.5990.033, mean F1=0.5460.055\n",
      "\n",
      "--- Trial 45/50 ---\n",
      "Config: {'hidden': 32, 'layers': 4, 'heads': 6, 'dropout': 0.39, 'lr': np.float64(0.001339915860734354), 'wd': np.float64(0.0037742443378558335), 'readout': 'mean'}\n",
      "[trial 45 | fold 0 | ep 000] loss=2.0688  val_balacc=0.500  val_F1=0.000\n",
      "[trial 45 | fold 0 | ep 010] loss=0.7398  val_balacc=0.500  val_F1=0.526\n",
      "[trial 45 | fold 0 | ep 020] loss=0.6785  val_balacc=0.500  val_F1=0.526\n",
      "[trial 45 | fold 0 | ep 030] loss=0.7224  val_balacc=0.500  val_F1=0.526\n",
      "[trial 45 | fold 0 | ep 040] loss=0.6841  val_balacc=0.500  val_F1=0.526\n",
      "[trial 45 | fold 0 | ep 050] loss=0.7164  val_balacc=0.500  val_F1=0.526\n",
      "Early stop @ epoch 50 (fold 0). Best BA was 0.500.\n",
      "fold 0: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 45 | fold 1 | ep 000] loss=4.2145  val_balacc=0.500  val_F1=0.500\n",
      "[trial 45 | fold 1 | ep 010] loss=0.7312  val_balacc=0.528  val_F1=0.400\n",
      "[trial 45 | fold 1 | ep 020] loss=0.6817  val_balacc=0.500  val_F1=0.500\n",
      "[trial 45 | fold 1 | ep 030] loss=0.7113  val_balacc=0.500  val_F1=0.500\n",
      "[trial 45 | fold 1 | ep 040] loss=0.6920  val_balacc=0.500  val_F1=0.500\n",
      "[trial 45 | fold 1 | ep 050] loss=0.6933  val_balacc=0.500  val_F1=0.500\n",
      "Early stop @ epoch 53 (fold 1). Best BA was 0.528.\n",
      "fold 1: best val_balacc=0.528, val_F1=0.514\n",
      "[trial 45 | fold 2 | ep 000] loss=4.0860  val_balacc=0.529  val_F1=0.556\n",
      "[trial 45 | fold 2 | ep 010] loss=0.7170  val_balacc=0.529  val_F1=0.556\n",
      "[trial 45 | fold 2 | ep 020] loss=0.7374  val_balacc=0.500  val_F1=0.541\n",
      "[trial 45 | fold 2 | ep 030] loss=0.7122  val_balacc=0.500  val_F1=0.541\n",
      "[trial 45 | fold 2 | ep 040] loss=0.7273  val_balacc=0.500  val_F1=0.541\n",
      "[trial 45 | fold 2 | ep 050] loss=0.7015  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 50 (fold 2). Best BA was 0.529.\n",
      "fold 2: best val_balacc=0.529, val_F1=0.556\n",
      "[trial 45 | fold 3 | ep 000] loss=2.9227  val_balacc=0.500  val_F1=0.541\n",
      "[trial 45 | fold 3 | ep 010] loss=0.7064  val_balacc=0.500  val_F1=0.541\n",
      "[trial 45 | fold 3 | ep 020] loss=0.7015  val_balacc=0.500  val_F1=0.541\n",
      "[trial 45 | fold 3 | ep 030] loss=0.7006  val_balacc=0.500  val_F1=0.541\n",
      "[trial 45 | fold 3 | ep 040] loss=0.6763  val_balacc=0.500  val_F1=0.541\n",
      "[trial 45 | fold 3 | ep 050] loss=0.7227  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 57 (fold 3). Best BA was 0.535.\n",
      "fold 3: best val_balacc=0.535, val_F1=0.480\n",
      "[trial 45 | fold 4 | ep 000] loss=2.1562  val_balacc=0.500  val_F1=0.000\n",
      "[trial 45 | fold 4 | ep 010] loss=0.7652  val_balacc=0.500  val_F1=0.541\n",
      "[trial 45 | fold 4 | ep 020] loss=0.6859  val_balacc=0.500  val_F1=0.541\n",
      "[trial 45 | fold 4 | ep 030] loss=0.7152  val_balacc=0.500  val_F1=0.541\n",
      "[trial 45 | fold 4 | ep 040] loss=0.7088  val_balacc=0.500  val_F1=0.541\n",
      "[trial 45 | fold 4 | ep 050] loss=0.6704  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 50 (fold 4). Best BA was 0.500.\n",
      "fold 4: best val_balacc=0.500, val_F1=0.000\n",
      "TRIAL 45: mean bal-acc=0.5180.015, mean F1=0.3100.254\n",
      "\n",
      "--- Trial 46/50 ---\n",
      "Config: {'hidden': 128, 'layers': 2, 'heads': 2, 'dropout': 0.523, 'lr': np.float64(0.00018369669213515538), 'wd': np.float64(0.00018301325583249922), 'readout': 'mean'}\n",
      "[trial 46 | fold 0 | ep 000] loss=0.8028  val_balacc=0.500  val_F1=0.000\n",
      "[trial 46 | fold 0 | ep 010] loss=0.6715  val_balacc=0.500  val_F1=0.000\n",
      "[trial 46 | fold 0 | ep 020] loss=0.6850  val_balacc=0.500  val_F1=0.000\n",
      "[trial 46 | fold 0 | ep 030] loss=0.7388  val_balacc=0.500  val_F1=0.000\n",
      "[trial 46 | fold 0 | ep 040] loss=0.6857  val_balacc=0.500  val_F1=0.000\n",
      "[trial 46 | fold 0 | ep 050] loss=0.7387  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 0). Best BA was 0.500.\n",
      "fold 0: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 46 | fold 1 | ep 000] loss=0.9161  val_balacc=0.500  val_F1=0.500\n",
      "[trial 46 | fold 1 | ep 010] loss=0.7069  val_balacc=0.500  val_F1=0.000\n",
      "[trial 46 | fold 1 | ep 020] loss=0.7308  val_balacc=0.500  val_F1=0.000\n",
      "[trial 46 | fold 1 | ep 030] loss=0.7075  val_balacc=0.500  val_F1=0.000\n",
      "[trial 46 | fold 1 | ep 040] loss=0.7168  val_balacc=0.500  val_F1=0.000\n",
      "[trial 46 | fold 1 | ep 050] loss=0.7043  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 1). Best BA was 0.500.\n",
      "fold 1: best val_balacc=0.500, val_F1=0.500\n",
      "[trial 46 | fold 2 | ep 000] loss=0.8323  val_balacc=0.500  val_F1=0.000\n",
      "[trial 46 | fold 2 | ep 010] loss=0.7324  val_balacc=0.529  val_F1=0.556\n",
      "[trial 46 | fold 2 | ep 020] loss=0.7149  val_balacc=0.500  val_F1=0.000\n",
      "[trial 46 | fold 2 | ep 030] loss=0.7257  val_balacc=0.500  val_F1=0.000\n",
      "[trial 46 | fold 2 | ep 040] loss=0.7159  val_balacc=0.521  val_F1=0.167\n",
      "[trial 46 | fold 2 | ep 050] loss=0.7260  val_balacc=0.521  val_F1=0.167\n",
      "[trial 46 | fold 2 | ep 060] loss=0.7292  val_balacc=0.521  val_F1=0.167\n",
      "[trial 46 | fold 2 | ep 070] loss=0.7052  val_balacc=0.521  val_F1=0.167\n",
      "[trial 46 | fold 2 | ep 080] loss=0.7013  val_balacc=0.521  val_F1=0.167\n",
      "Early stop @ epoch 84 (fold 2). Best BA was 0.597.\n",
      "fold 2: best val_balacc=0.597, val_F1=0.581\n",
      "[trial 46 | fold 3 | ep 000] loss=0.8317  val_balacc=0.500  val_F1=0.000\n",
      "[trial 46 | fold 3 | ep 010] loss=0.7395  val_balacc=0.500  val_F1=0.000\n",
      "[trial 46 | fold 3 | ep 020] loss=0.7504  val_balacc=0.500  val_F1=0.000\n",
      "[trial 46 | fold 3 | ep 030] loss=0.7033  val_balacc=0.500  val_F1=0.000\n",
      "[trial 46 | fold 3 | ep 040] loss=0.7043  val_balacc=0.500  val_F1=0.000\n",
      "[trial 46 | fold 3 | ep 050] loss=0.6527  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 3). Best BA was 0.500.\n",
      "fold 3: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 46 | fold 4 | ep 000] loss=0.9138  val_balacc=0.500  val_F1=0.000\n",
      "[trial 46 | fold 4 | ep 010] loss=0.7455  val_balacc=0.500  val_F1=0.000\n",
      "[trial 46 | fold 4 | ep 020] loss=0.7100  val_balacc=0.565  val_F1=0.500\n",
      "[trial 46 | fold 4 | ep 030] loss=0.7176  val_balacc=0.500  val_F1=0.541\n",
      "[trial 46 | fold 4 | ep 040] loss=0.7073  val_balacc=0.418  val_F1=0.414\n",
      "[trial 46 | fold 4 | ep 050] loss=0.6932  val_balacc=0.479  val_F1=0.514\n",
      "[trial 46 | fold 4 | ep 060] loss=0.7108  val_balacc=0.479  val_F1=0.514\n",
      "Early stop @ epoch 61 (fold 4). Best BA was 0.571.\n",
      "fold 4: best val_balacc=0.571, val_F1=0.308\n",
      "TRIAL 46: mean bal-acc=0.5340.042, mean F1=0.2780.243\n",
      "\n",
      "--- Trial 47/50 ---\n",
      "Config: {'hidden': 128, 'layers': 2, 'heads': 6, 'dropout': 0.49, 'lr': np.float64(0.00028086700970397695), 'wd': np.float64(0.00047427879592892993), 'readout': 'mean'}\n",
      "[trial 47 | fold 0 | ep 000] loss=0.8770  val_balacc=0.500  val_F1=0.526\n",
      "[trial 47 | fold 0 | ep 010] loss=0.7795  val_balacc=0.500  val_F1=0.526\n",
      "[trial 47 | fold 0 | ep 020] loss=0.7436  val_balacc=0.500  val_F1=0.000\n",
      "[trial 47 | fold 0 | ep 030] loss=0.7191  val_balacc=0.528  val_F1=0.541\n",
      "[trial 47 | fold 0 | ep 040] loss=0.6995  val_balacc=0.528  val_F1=0.541\n",
      "[trial 47 | fold 0 | ep 050] loss=0.7108  val_balacc=0.528  val_F1=0.541\n",
      "Early stop @ epoch 59 (fold 0). Best BA was 0.528.\n",
      "fold 0: best val_balacc=0.528, val_F1=0.541\n",
      "[trial 47 | fold 1 | ep 000] loss=0.8144  val_balacc=0.500  val_F1=0.500\n",
      "[trial 47 | fold 1 | ep 010] loss=0.7178  val_balacc=0.528  val_F1=0.483\n",
      "[trial 47 | fold 1 | ep 020] loss=0.7082  val_balacc=0.556  val_F1=0.200\n",
      "[trial 47 | fold 1 | ep 030] loss=0.7039  val_balacc=0.556  val_F1=0.421\n",
      "[trial 47 | fold 1 | ep 040] loss=0.6805  val_balacc=0.556  val_F1=0.421\n",
      "[trial 47 | fold 1 | ep 050] loss=0.6690  val_balacc=0.556  val_F1=0.421\n",
      "Early stop @ epoch 56 (fold 1). Best BA was 0.667.\n",
      "fold 1: best val_balacc=0.667, val_F1=0.556\n",
      "[trial 47 | fold 2 | ep 000] loss=0.8873  val_balacc=0.500  val_F1=0.541\n",
      "[trial 47 | fold 2 | ep 010] loss=0.7466  val_balacc=0.529  val_F1=0.556\n",
      "[trial 47 | fold 2 | ep 020] loss=0.7050  val_balacc=0.529  val_F1=0.556\n",
      "[trial 47 | fold 2 | ep 030] loss=0.6904  val_balacc=0.529  val_F1=0.556\n",
      "[trial 47 | fold 2 | ep 040] loss=0.7184  val_balacc=0.529  val_F1=0.556\n",
      "[trial 47 | fold 2 | ep 050] loss=0.7218  val_balacc=0.529  val_F1=0.556\n",
      "[trial 47 | fold 2 | ep 060] loss=0.7240  val_balacc=0.529  val_F1=0.556\n",
      "Early stop @ epoch 63 (fold 2). Best BA was 0.682.\n",
      "fold 2: best val_balacc=0.682, val_F1=0.600\n",
      "[trial 47 | fold 3 | ep 000] loss=0.8757  val_balacc=0.500  val_F1=0.541\n",
      "[trial 47 | fold 3 | ep 010] loss=0.7534  val_balacc=0.500  val_F1=0.000\n",
      "[trial 47 | fold 3 | ep 020] loss=0.7130  val_balacc=0.479  val_F1=0.514\n",
      "[trial 47 | fold 3 | ep 030] loss=0.6810  val_balacc=0.500  val_F1=0.000\n",
      "[trial 47 | fold 3 | ep 040] loss=0.7053  val_balacc=0.565  val_F1=0.500\n",
      "[trial 47 | fold 3 | ep 050] loss=0.7016  val_balacc=0.582  val_F1=0.444\n",
      "[trial 47 | fold 3 | ep 060] loss=0.7013  val_balacc=0.512  val_F1=0.267\n",
      "[trial 47 | fold 3 | ep 070] loss=0.7218  val_balacc=0.512  val_F1=0.267\n",
      "[trial 47 | fold 3 | ep 080] loss=0.6870  val_balacc=0.512  val_F1=0.267\n",
      "[trial 47 | fold 3 | ep 090] loss=0.7318  val_balacc=0.512  val_F1=0.267\n",
      "[trial 47 | fold 3 | ep 100] loss=0.6958  val_balacc=0.512  val_F1=0.267\n",
      "Early stop @ epoch 102 (fold 3). Best BA was 0.612.\n",
      "fold 3: best val_balacc=0.612, val_F1=0.471\n",
      "[trial 47 | fold 4 | ep 000] loss=0.7728  val_balacc=0.500  val_F1=0.541\n",
      "[trial 47 | fold 4 | ep 010] loss=0.7132  val_balacc=0.500  val_F1=0.000\n",
      "[trial 47 | fold 4 | ep 020] loss=0.7630  val_balacc=0.500  val_F1=0.541\n",
      "[trial 47 | fold 4 | ep 030] loss=0.7014  val_balacc=0.500  val_F1=0.541\n",
      "[trial 47 | fold 4 | ep 040] loss=0.7142  val_balacc=0.500  val_F1=0.541\n",
      "[trial 47 | fold 4 | ep 050] loss=0.7416  val_balacc=0.500  val_F1=0.541\n",
      "[trial 47 | fold 4 | ep 060] loss=0.7036  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 62 (fold 4). Best BA was 0.571.\n",
      "fold 4: best val_balacc=0.571, val_F1=0.308\n",
      "TRIAL 47: mean bal-acc=0.6120.058, mean F1=0.4950.102\n",
      "\n",
      "--- Trial 48/50 ---\n",
      "Config: {'hidden': 32, 'layers': 4, 'heads': 4, 'dropout': 0.549, 'lr': np.float64(0.00011882343031794597), 'wd': np.float64(0.0002323181183822521), 'readout': 'mean'}\n",
      "[trial 48 | fold 0 | ep 000] loss=4.8369  val_balacc=0.378  val_F1=0.412\n",
      "[trial 48 | fold 0 | ep 010] loss=3.9663  val_balacc=0.472  val_F1=0.000\n",
      "[trial 48 | fold 0 | ep 020] loss=2.8580  val_balacc=0.472  val_F1=0.000\n",
      "[trial 48 | fold 0 | ep 030] loss=2.6521  val_balacc=0.500  val_F1=0.000\n",
      "[trial 48 | fold 0 | ep 040] loss=3.5972  val_balacc=0.500  val_F1=0.000\n",
      "[trial 48 | fold 0 | ep 050] loss=2.6913  val_balacc=0.500  val_F1=0.000\n",
      "[trial 48 | fold 0 | ep 060] loss=3.3365  val_balacc=0.500  val_F1=0.000\n",
      "[trial 48 | fold 0 | ep 070] loss=3.2264  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 78 (fold 0). Best BA was 0.500.\n",
      "fold 0: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 48 | fold 1 | ep 000] loss=5.0331  val_balacc=0.500  val_F1=0.500\n",
      "[trial 48 | fold 1 | ep 010] loss=4.0472  val_balacc=0.500  val_F1=0.381\n",
      "[trial 48 | fold 1 | ep 020] loss=3.7242  val_balacc=0.500  val_F1=0.500\n",
      "[trial 48 | fold 1 | ep 030] loss=2.6199  val_balacc=0.500  val_F1=0.500\n",
      "[trial 48 | fold 1 | ep 040] loss=2.8855  val_balacc=0.500  val_F1=0.500\n",
      "[trial 48 | fold 1 | ep 050] loss=3.1453  val_balacc=0.500  val_F1=0.500\n",
      "Early stop @ epoch 52 (fold 1). Best BA was 0.556.\n",
      "fold 1: best val_balacc=0.556, val_F1=0.375\n",
      "[trial 48 | fold 2 | ep 000] loss=8.8354  val_balacc=0.500  val_F1=0.000\n",
      "[trial 48 | fold 2 | ep 010] loss=4.7743  val_balacc=0.500  val_F1=0.000\n",
      "[trial 48 | fold 2 | ep 020] loss=3.6410  val_balacc=0.500  val_F1=0.000\n",
      "[trial 48 | fold 2 | ep 030] loss=3.0351  val_balacc=0.500  val_F1=0.000\n",
      "[trial 48 | fold 2 | ep 040] loss=4.5726  val_balacc=0.500  val_F1=0.000\n",
      "[trial 48 | fold 2 | ep 050] loss=2.7383  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 2). Best BA was 0.500.\n",
      "fold 2: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 48 | fold 3 | ep 000] loss=6.7130  val_balacc=0.500  val_F1=0.000\n",
      "[trial 48 | fold 3 | ep 010] loss=5.4984  val_balacc=0.500  val_F1=0.000\n",
      "[trial 48 | fold 3 | ep 020] loss=3.8197  val_balacc=0.500  val_F1=0.000\n",
      "[trial 48 | fold 3 | ep 030] loss=3.8833  val_balacc=0.500  val_F1=0.000\n",
      "[trial 48 | fold 3 | ep 040] loss=3.6788  val_balacc=0.500  val_F1=0.000\n",
      "[trial 48 | fold 3 | ep 050] loss=2.5510  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 3). Best BA was 0.500.\n",
      "fold 3: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 48 | fold 4 | ep 000] loss=10.3302  val_balacc=0.500  val_F1=0.000\n",
      "[trial 48 | fold 4 | ep 010] loss=4.6616  val_balacc=0.500  val_F1=0.000\n",
      "[trial 48 | fold 4 | ep 020] loss=4.4618  val_balacc=0.500  val_F1=0.000\n",
      "[trial 48 | fold 4 | ep 030] loss=4.6781  val_balacc=0.500  val_F1=0.000\n",
      "[trial 48 | fold 4 | ep 040] loss=4.5410  val_balacc=0.500  val_F1=0.000\n",
      "[trial 48 | fold 4 | ep 050] loss=3.7094  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 4). Best BA was 0.500.\n",
      "fold 4: best val_balacc=0.500, val_F1=0.000\n",
      "TRIAL 48: mean bal-acc=0.5110.022, mean F1=0.0750.150\n",
      "\n",
      "--- Trial 49/50 ---\n",
      "Config: {'hidden': 128, 'layers': 2, 'heads': 6, 'dropout': 0.433, 'lr': np.float64(0.0015809124698607762), 'wd': np.float64(0.0006618868210932445), 'readout': 'mean'}\n",
      "[trial 49 | fold 0 | ep 000] loss=1.7094  val_balacc=0.500  val_F1=0.000\n",
      "[trial 49 | fold 0 | ep 010] loss=0.6967  val_balacc=0.500  val_F1=0.000\n",
      "[trial 49 | fold 0 | ep 020] loss=0.6938  val_balacc=0.500  val_F1=0.000\n",
      "[trial 49 | fold 0 | ep 030] loss=0.6936  val_balacc=0.500  val_F1=0.000\n",
      "[trial 49 | fold 0 | ep 040] loss=0.6897  val_balacc=0.500  val_F1=0.000\n",
      "[trial 49 | fold 0 | ep 050] loss=0.6926  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 0). Best BA was 0.500.\n",
      "fold 0: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 49 | fold 1 | ep 000] loss=1.7248  val_balacc=0.500  val_F1=0.500\n",
      "[trial 49 | fold 1 | ep 010] loss=0.6975  val_balacc=0.500  val_F1=0.500\n",
      "[trial 49 | fold 1 | ep 020] loss=0.6861  val_balacc=0.500  val_F1=0.500\n",
      "[trial 49 | fold 1 | ep 030] loss=0.6829  val_balacc=0.528  val_F1=0.500\n",
      "[trial 49 | fold 1 | ep 040] loss=0.6820  val_balacc=0.361  val_F1=0.345\n",
      "[trial 49 | fold 1 | ep 050] loss=0.6810  val_balacc=0.361  val_F1=0.345\n",
      "[trial 49 | fold 1 | ep 060] loss=0.6804  val_balacc=0.361  val_F1=0.345\n",
      "[trial 49 | fold 1 | ep 070] loss=0.6837  val_balacc=0.361  val_F1=0.345\n",
      "[trial 49 | fold 1 | ep 080] loss=0.6827  val_balacc=0.361  val_F1=0.345\n",
      "Early stop @ epoch 80 (fold 1). Best BA was 0.528.\n",
      "fold 1: best val_balacc=0.528, val_F1=0.500\n",
      "[trial 49 | fold 2 | ep 000] loss=1.3862  val_balacc=0.500  val_F1=0.541\n",
      "[trial 49 | fold 2 | ep 010] loss=0.6913  val_balacc=0.500  val_F1=0.541\n",
      "[trial 49 | fold 2 | ep 020] loss=0.7002  val_balacc=0.474  val_F1=0.316\n",
      "[trial 49 | fold 2 | ep 030] loss=0.6826  val_balacc=0.424  val_F1=0.222\n",
      "[trial 49 | fold 2 | ep 040] loss=0.6761  val_balacc=0.435  val_F1=0.348\n",
      "[trial 49 | fold 2 | ep 050] loss=0.6740  val_balacc=0.424  val_F1=0.222\n",
      "[trial 49 | fold 2 | ep 060] loss=0.6682  val_balacc=0.424  val_F1=0.222\n",
      "[trial 49 | fold 2 | ep 070] loss=0.6712  val_balacc=0.424  val_F1=0.222\n",
      "Early stop @ epoch 72 (fold 2). Best BA was 0.735.\n",
      "fold 2: best val_balacc=0.735, val_F1=0.690\n",
      "[trial 49 | fold 3 | ep 000] loss=1.7700  val_balacc=0.500  val_F1=0.000\n",
      "[trial 49 | fold 3 | ep 010] loss=0.6991  val_balacc=0.500  val_F1=0.000\n",
      "[trial 49 | fold 3 | ep 020] loss=0.6906  val_balacc=0.576  val_F1=0.552\n",
      "[trial 49 | fold 3 | ep 030] loss=0.6872  val_balacc=0.491  val_F1=0.154\n",
      "[trial 49 | fold 3 | ep 040] loss=0.6895  val_balacc=0.491  val_F1=0.154\n",
      "[trial 49 | fold 3 | ep 050] loss=0.6826  val_balacc=0.491  val_F1=0.154\n",
      "[trial 49 | fold 3 | ep 060] loss=0.6866  val_balacc=0.491  val_F1=0.154\n",
      "[trial 49 | fold 3 | ep 070] loss=0.6850  val_balacc=0.491  val_F1=0.154\n",
      "Early stop @ epoch 70 (fold 3). Best BA was 0.576.\n",
      "fold 3: best val_balacc=0.576, val_F1=0.552\n",
      "[trial 49 | fold 4 | ep 000] loss=1.3025  val_balacc=0.500  val_F1=0.000\n",
      "[trial 49 | fold 4 | ep 010] loss=0.7003  val_balacc=0.500  val_F1=0.541\n",
      "[trial 49 | fold 4 | ep 020] loss=0.6852  val_balacc=0.494  val_F1=0.381\n",
      "[trial 49 | fold 4 | ep 030] loss=0.6780  val_balacc=0.521  val_F1=0.167\n",
      "[trial 49 | fold 4 | ep 040] loss=0.6764  val_balacc=0.571  val_F1=0.308\n",
      "[trial 49 | fold 4 | ep 050] loss=0.6808  val_balacc=0.571  val_F1=0.308\n",
      "[trial 49 | fold 4 | ep 060] loss=0.6750  val_balacc=0.571  val_F1=0.308\n",
      "[trial 49 | fold 4 | ep 070] loss=0.6676  val_balacc=0.571  val_F1=0.308\n",
      "Early stop @ epoch 73 (fold 4). Best BA was 0.571.\n",
      "fold 4: best val_balacc=0.571, val_F1=0.308\n",
      "TRIAL 49: mean bal-acc=0.5820.082, mean F1=0.4100.239\n",
      "\n",
      "--- Trial 50/50 ---\n",
      "Config: {'hidden': 32, 'layers': 4, 'heads': 2, 'dropout': 0.464, 'lr': np.float64(0.0026178930765921003), 'wd': np.float64(0.000731120796274812), 'readout': 'mean'}\n",
      "[trial 50 | fold 0 | ep 000] loss=5.4022  val_balacc=0.428  val_F1=0.457\n",
      "[trial 50 | fold 0 | ep 010] loss=0.7351  val_balacc=0.500  val_F1=0.000\n",
      "[trial 50 | fold 0 | ep 020] loss=0.6966  val_balacc=0.500  val_F1=0.000\n",
      "[trial 50 | fold 0 | ep 030] loss=0.7075  val_balacc=0.500  val_F1=0.000\n",
      "[trial 50 | fold 0 | ep 040] loss=0.7036  val_balacc=0.500  val_F1=0.000\n",
      "[trial 50 | fold 0 | ep 050] loss=0.6947  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 51 (fold 0). Best BA was 0.500.\n",
      "fold 0: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 50 | fold 1 | ep 000] loss=4.4793  val_balacc=0.500  val_F1=0.500\n",
      "[trial 50 | fold 1 | ep 010] loss=0.7066  val_balacc=0.500  val_F1=0.500\n",
      "[trial 50 | fold 1 | ep 020] loss=0.6876  val_balacc=0.500  val_F1=0.500\n",
      "[trial 50 | fold 1 | ep 030] loss=0.7019  val_balacc=0.500  val_F1=0.500\n",
      "[trial 50 | fold 1 | ep 040] loss=0.6924  val_balacc=0.500  val_F1=0.500\n",
      "[trial 50 | fold 1 | ep 050] loss=0.6858  val_balacc=0.500  val_F1=0.500\n",
      "Early stop @ epoch 50 (fold 1). Best BA was 0.500.\n",
      "fold 1: best val_balacc=0.500, val_F1=0.500\n",
      "[trial 50 | fold 2 | ep 000] loss=3.8988  val_balacc=0.500  val_F1=0.541\n",
      "[trial 50 | fold 2 | ep 010] loss=0.7069  val_balacc=0.500  val_F1=0.541\n",
      "[trial 50 | fold 2 | ep 020] loss=0.7289  val_balacc=0.500  val_F1=0.541\n",
      "[trial 50 | fold 2 | ep 030] loss=0.6916  val_balacc=0.500  val_F1=0.541\n",
      "[trial 50 | fold 2 | ep 040] loss=0.7067  val_balacc=0.500  val_F1=0.541\n",
      "[trial 50 | fold 2 | ep 050] loss=0.7131  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 50 (fold 2). Best BA was 0.500.\n",
      "fold 2: best val_balacc=0.500, val_F1=0.541\n",
      "[trial 50 | fold 3 | ep 000] loss=3.9977  val_balacc=0.500  val_F1=0.000\n",
      "[trial 50 | fold 3 | ep 010] loss=0.7249  val_balacc=0.500  val_F1=0.541\n",
      "[trial 50 | fold 3 | ep 020] loss=0.7032  val_balacc=0.500  val_F1=0.541\n",
      "[trial 50 | fold 3 | ep 030] loss=0.7078  val_balacc=0.500  val_F1=0.541\n",
      "[trial 50 | fold 3 | ep 040] loss=0.6915  val_balacc=0.500  val_F1=0.541\n",
      "[trial 50 | fold 3 | ep 050] loss=0.6969  val_balacc=0.500  val_F1=0.541\n",
      "Early stop @ epoch 50 (fold 3). Best BA was 0.500.\n",
      "fold 3: best val_balacc=0.500, val_F1=0.000\n",
      "[trial 50 | fold 4 | ep 000] loss=5.5773  val_balacc=0.500  val_F1=0.000\n",
      "[trial 50 | fold 4 | ep 010] loss=0.7106  val_balacc=0.500  val_F1=0.000\n",
      "[trial 50 | fold 4 | ep 020] loss=0.6720  val_balacc=0.500  val_F1=0.000\n",
      "[trial 50 | fold 4 | ep 030] loss=0.7030  val_balacc=0.500  val_F1=0.000\n",
      "[trial 50 | fold 4 | ep 040] loss=0.7043  val_balacc=0.500  val_F1=0.000\n",
      "[trial 50 | fold 4 | ep 050] loss=0.7036  val_balacc=0.500  val_F1=0.000\n",
      "Early stop @ epoch 50 (fold 4). Best BA was 0.500.\n",
      "fold 4: best val_balacc=0.500, val_F1=0.000\n",
      "TRIAL 50: mean bal-acc=0.5000.000, mean F1=0.2080.255\n"
     ]
    }
   ],
   "source": [
    "# --- Assume you have these variables from your setup ---\n",
    "# trainval_ids, y_trainval, G_overlap_dgl, node_meta_numeric,\n",
    "# orig_dyn_feat_dict, presence_mask_dict, all fixed HPs (vocabs, etc.)\n",
    "\n",
    "# --- 1. EFFICIENT DATASET SETUP (Create ONCE before the loops) ---\n",
    "# Create the master dataset for tuning.\n",
    "# We will update its dynamic features dictionary in each fold.\n",
    "y_trainval_series = pd.Series(y_trainval, index=trainval_ids)\n",
    "\n",
    "test_graph_directed = G_overlap_lcc_largest.to_directed()\n",
    "G_overlap_dgl = dgl.from_networkx(\n",
    "    test_graph_directed,  # <-- Pass the new directed graph\n",
    "    edge_attrs=['fast_weight', 'slow_weight']\n",
    ")\n",
    "# Combine the weights into a single 'feat' tensor for the GAT model\n",
    "G_overlap_dgl.edata['feat'] = torch.stack([\n",
    "    G_overlap_dgl.edata['fast_weight'],\n",
    "    G_overlap_dgl.edata['slow_weight']\n",
    "], dim=1).float()\n",
    "\n",
    "# dgl.add_self_loop(dgl.remove_self_loop(G_overlap_dgl_filtered))\n",
    "\n",
    "trainval_dataset = SingleGraphSampleDataset(\n",
    "    G_overlap_dgl,\n",
    "    node_meta_scaled,\n",
    "    orig_dyn_feat_dict, # Placeholder, will be updated\n",
    "    presence_mask_dict,\n",
    "    y_trainval_series\n",
    ")\n",
    "# Ensure y_trainval is a NumPy array for easy indexing later.\n",
    "y_trainval_np = np.array(y_trainval)\n",
    "results = []\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for trial in range(N_TRIALS):\n",
    "    cfg = sample_config()\n",
    "    print(f\"\\n--- Trial {trial+1}/{N_TRIALS} ---\\nConfig: {cfg}\")\n",
    "\n",
    "    fold_balacc, fold_f1 = [], []\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(trainval_ids, y_trainval)):\n",
    "\n",
    "        fold_train_ids = [trainval_ids[i] for i in train_idx]\n",
    "\n",
    "        # --- Dynamic features: Scale based on this fold's training data ---\n",
    "        dyn_feat_dict_scaled = scale_dyn_dict(\n",
    "            fold_train_ids,\n",
    "            trainval_ids,\n",
    "            orig_dyn_feat_dict\n",
    "        )\n",
    "        # dyn_dim = next(iter(dyn_feat_dict_scaled.values())).shape[-1]\n",
    "        \n",
    "        # --- THE EFFICIENCY FIX: Update the dataset instead of rebuilding it ---\n",
    "        trainval_dataset.dyn = dyn_feat_dict_scaled\n",
    "        # --- Subsets and DataLoaders ---\n",
    "        train_ds = Subset(trainval_dataset, train_idx)\n",
    "        val_ds = Subset(trainval_dataset, val_idx)\n",
    "        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn_filtered)\n",
    "        val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn_filtered)\n",
    "\n",
    "        # --- NOW, get the TRUE dimensions by inspecting the dataset ---\n",
    "        static_dim = trainval_dataset.g.ndata['static'].shape[1]\n",
    "        dyn_dim = next(iter(trainval_dataset.dyn.values())).shape[1]\n",
    "\n",
    "        # # --- MODEL (re-initialized for each fold) ---\n",
    "        # model = MaskedGIN(\n",
    "        #     static_dim=static_dim,\n",
    "        #     dyn_dim=dyn_dim,\n",
    "        #     hidden_dim=cfg['hidden'],\n",
    "        #     num_layers=cfg['layers'],\n",
    "        #     dropout=cfg['dropout'],\n",
    "        #     readout=cfg['readout'],\n",
    "        #     v_vocab=v_vocab_size,\n",
    "        #     j_vocab=j_vocab_size,\n",
    "        #     v_slow_vocab=v_slow_vocab_size,\n",
    "        #     j_slow_vocab=j_slow_vocab_size\n",
    "        # ).to(device)\n",
    "        \n",
    "        # Build and train model\n",
    "        model = MaskedGAT(\n",
    "            static_dim=static_dim,\n",
    "            dyn_dim=1,\n",
    "            hidden_dim=cfg[\"hidden\"],\n",
    "            num_layers=cfg[\"layers\"],\n",
    "            dropout=cfg[\"dropout\"],\n",
    "            readout=cfg[\"readout\"], \n",
    "            v_vocab=v_vocab_size,\n",
    "            j_vocab=j_vocab_size,\n",
    "            v_slow_vocab=v_slow_vocab_size,\n",
    "            j_slow_vocab=j_slow_vocab_size,\n",
    "            num_heads=cfg['heads']\n",
    "        ).to(device)\n",
    "\n",
    "        # --- LOSS (weights calculated from this fold's train set) ---\n",
    "        train_labels = y_trainval_np[train_idx] # <-- THE ROBUSTNESS FIX\n",
    "        class_counts = np.bincount(train_labels, minlength=2)\n",
    "        if 0 in class_counts: # Handle rare case of a missing class\n",
    "             weights = torch.tensor([1.0, 1.0], dtype=torch.float32)\n",
    "        else:\n",
    "             weights = 1.0 / torch.tensor(class_counts, dtype=torch.float32)\n",
    "        criterion = torch.nn.CrossEntropyLoss(weight=weights.to(device))\n",
    "\n",
    "        # --- OPTIMIZER, SCHEDULER, and TRAINING LOOP ---\n",
    "        opt = torch.optim.AdamW(model.parameters(), lr=cfg['lr'], weight_decay=cfg['wd'])\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='max', factor=0.2, patience=10)\n",
    "\n",
    "        max_epochs = 250\n",
    "        patience = 50\n",
    "        best_ba, best_f1 = 0.0, 0.0\n",
    "        epochs_since_best = 0\n",
    "        best_state = None\n",
    "\n",
    "        for epoch in range(max_epochs):\n",
    "            train_loss = train_one_epoch(model, train_loader, device, criterion, opt)\n",
    "            ba, f1 = evaluate(model, val_loader, device)   # returns balanced acc, F1\n",
    "\n",
    "            scheduler.step(ba)  # reduce LR on plateau of val BA\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"[trial {trial+1} | fold {fold} | ep {epoch:03d}] \"\n",
    "                      f\"loss={train_loss:.4f}  val_balacc={ba:.3f}  val_F1={f1:.3f}\")\n",
    "\n",
    "            if ba > best_ba:\n",
    "                best_ba, best_f1 = ba, f1\n",
    "                epochs_since_best = 0\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "            else:\n",
    "                epochs_since_best += 1\n",
    "                if epochs_since_best >= patience:\n",
    "                    print(f\"Early stop @ epoch {epoch} (fold {fold}). Best BA was {best_ba:.3f}.\")\n",
    "                    break # The stored best_ba is all that matters\n",
    "\n",
    "        fold_balacc.append(best_ba)\n",
    "        fold_f1.append(best_f1)\n",
    "        print(f\"fold {fold}: best val_balacc={best_ba:.3f}, val_F1={best_f1:.3f}\")\n",
    "\n",
    "    mean_ba, std_ba = np.mean(fold_balacc), np.std(fold_balacc)\n",
    "    mean_f1, std_f1 = np.mean(fold_f1), np.std(fold_f1)\n",
    "    print(f\"TRIAL {trial+1}: mean bal-acc={mean_ba:.3f}{std_ba:.3f}, \"\n",
    "          f\"mean F1={mean_f1:.3f}{std_f1:.3f}\")\n",
    "\n",
    "    results.append({\n",
    "        'cfg': cfg,\n",
    "        'mean_balacc': float(mean_ba), 'std_balacc': float(std_ba),\n",
    "        'mean_f1': float(mean_f1), 'std_f1': float(std_f1)\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "495b40f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average balanced accuracy across all configs: 0.563\n",
      "Average F1 score across all configs: 0.391\n",
      "Number of runs with BA = 0.5: 2\n",
      "Number of runs with F1 = 0.5: 0\n",
      "\n",
      "Best config by mean balanced accuracy:\n",
      " {'cfg': {'hidden': 32, 'layers': 4, 'heads': 4, 'dropout': 0.317, 'lr': np.float64(0.0009828793486963993), 'wd': np.float64(0.0005066065145603329), 'readout': 's2s'}, 'mean_balacc': 0.6642483660130718, 'std_balacc': 0.017095720495970548, 'mean_f1': 0.6048539222452266, 'std_f1': 0.02341519522291004}\n",
      "\n",
      "Top 5 configs:\n",
      "Rank 1: {'cfg': {'hidden': 32, 'layers': 4, 'heads': 4, 'dropout': 0.317, 'lr': np.float64(0.0009828793486963993), 'wd': np.float64(0.0005066065145603329), 'readout': 's2s'}, 'mean_balacc': 0.6642483660130718, 'std_balacc': 0.017095720495970548, 'mean_f1': 0.6048539222452266, 'std_f1': 0.02341519522291004}\n",
      "Rank 2: {'cfg': {'hidden': 32, 'layers': 2, 'heads': 4, 'dropout': 0.486, 'lr': np.float64(0.0009047098455470519), 'wd': np.float64(7.923781699209354e-05), 'readout': 's2s'}, 'mean_balacc': 0.629673202614379, 'std_balacc': 0.06691208192603589, 'mean_f1': 0.5163650793650794, 'std_f1': 0.13759977163111592}\n",
      "Rank 3: {'cfg': {'hidden': 32, 'layers': 3, 'heads': 2, 'dropout': 0.454, 'lr': np.float64(0.00029725181725389125), 'wd': np.float64(0.0004675187172203805), 'readout': 's2s'}, 'mean_balacc': 0.6284967320261439, 'std_balacc': 0.09949709074482403, 'mean_f1': 0.4905323505323505, 'std_f1': 0.19432336041592488}\n",
      "Rank 4: {'cfg': {'hidden': 64, 'layers': 4, 'heads': 6, 'dropout': 0.373, 'lr': np.float64(0.0006100511275091178), 'wd': np.float64(0.0001733267755987225), 'readout': 's2s'}, 'mean_balacc': 0.6188235294117648, 'std_balacc': 0.041693301752149234, 'mean_f1': 0.5111808561808562, 'std_f1': 0.10587502958310646}\n",
      "Rank 5: {'cfg': {'hidden': 128, 'layers': 2, 'heads': 6, 'dropout': 0.49, 'lr': np.float64(0.00028086700970397695), 'wd': np.float64(0.00047427879592892993), 'readout': 'mean'}, 'mean_balacc': 0.611830065359477, 'std_balacc': 0.05787263381450801, 'mean_f1': 0.4948753278165043, 'std_f1': 0.10241674109176527}\n"
     ]
    }
   ],
   "source": [
    "# ----- BEST CONFIG SELECTION -----\n",
    "results_sorted = sorted(results, key=lambda x: x['mean_balacc'], reverse=True)\n",
    "best_result = results_sorted[0]\n",
    "\n",
    "# --- summary stats ---\n",
    "all_bas = [r['mean_balacc'] for r in results]\n",
    "all_f1s = [r['mean_f1'] for r in results]  # assuming each dict has 'mean_f1'\n",
    "\n",
    "avg_ba = sum(all_bas) / len(all_bas)\n",
    "avg_f1 = sum(all_f1s) / len(all_f1s)\n",
    "\n",
    "count_half_ba = sum(1 for ba in all_bas if abs(ba - 0.5) < 1e-6)\n",
    "count_half_f1 = sum(1 for f1 in all_f1s if abs(f1 - 0.5) < 1e-6)\n",
    "\n",
    "print(f\"\\nAverage balanced accuracy across all configs: {avg_ba:.3f}\")\n",
    "print(f\"Average F1 score across all configs: {avg_f1:.3f}\")\n",
    "print(f\"Number of runs with BA = 0.5: {count_half_ba}\")\n",
    "print(f\"Number of runs with F1 = 0.5: {count_half_f1}\")\n",
    "\n",
    "print(\"\\nBest config by mean balanced accuracy:\\n\", best_result)\n",
    "\n",
    "print(\"\\nTop 5 configs:\")\n",
    "for i, res in enumerate(results_sorted[:5]):\n",
    "    print(f\"Rank {i+1}: {res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16d8e35",
   "metadata": {},
   "source": [
    "### K-fold On Entire Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9b1c35",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9396a253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import networkx as nx\n",
    "import dgl\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6082e824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clonotype():\n",
    "    mixcr_dir = \"/dsi/efroni-lab/sbm/OrrBavly/colon_data/new_mixcr/TRB/\"\n",
    "    meta_file = \"/home/dsi/orrbavly/GNN_project/data/colon_meta_time.csv\"\n",
    "\n",
    "    time_threshold = 750\n",
    "    meta_df = pd.read_csv(meta_file)\n",
    "    meta_df = meta_df[[\"Sample_ID\", \"extraction_time\"]]\n",
    "\n",
    "    # Columns to keep from MiXCR files\n",
    "    mixcr_cols = [\n",
    "        \"aaSeqCDR3\", \"nSeqCDR3\", \"readCount\", 'readFraction',\n",
    "        \"allVHitsWithScore\", \"allDHitsWithScore\", \"allJHitsWithScore\"\n",
    "    ]\n",
    "    clonotype_dfs = []\n",
    "\n",
    "    for fname in os.listdir(mixcr_dir):\n",
    "        if fname.endswith(\".tsv\") and os.path.isfile(os.path.join(mixcr_dir, fname)):\n",
    "            sample_prefix = fname.split(\"_\")[0]\n",
    "            file_path = os.path.join(mixcr_dir, fname)\n",
    "            \n",
    "            df = pd.read_csv(file_path, sep=\"\\t\", usecols=mixcr_cols)\n",
    "            df[\"Sample_ID\"] = sample_prefix\n",
    "            clonotype_dfs.append(df)\n",
    "\n",
    "    clonotype_df = pd.concat(clonotype_dfs, ignore_index=True)\n",
    "\n",
    "    # Merge with metadata to get extraction time\n",
    "    clonotype_df = clonotype_df.merge(meta_df, on=\"Sample_ID\", how=\"inner\")\n",
    "    # Create 'group' column: fast vs slow\n",
    "    clonotype_df[\"group\"] = clonotype_df[\"extraction_time\"].apply(lambda x: \"fast\" if x <= time_threshold else \"slow\")\n",
    "    return clonotype_df\n",
    "\n",
    "def create_corrolation(clonotype_df, graph_universe_ids,threshold=5):\n",
    "    sub = clonotype_df.loc[\n",
    "    clonotype_df['Sample_ID'].isin(graph_universe_ids) &\n",
    "    clonotype_df['group'].isin(['fast','slow'])].copy()\n",
    "\n",
    "    # A narrow view ONLY for the survivor/qualification logic\n",
    "    logic_df = sub[['Sample_ID','group','aaSeqCDR3']].dropna(subset=['aaSeqCDR3','group'])\n",
    "\n",
    "    # 1) survivors: AA present in BOTH groups\n",
    "    presence = (logic_df.groupby(['aaSeqCDR3','group'])['Sample_ID']\n",
    "                        .size().unstack(fill_value=0))\n",
    "    survivors = presence.index[(presence.get('fast',0) > 0) & (presence.get('slow',0) > 0)]\n",
    "\n",
    "    survivor_df = sub[sub['aaSeqCDR3'].isin(survivors)].copy()  # keeps ALL columns\n",
    "\n",
    "    threshold = threshold\n",
    "    # 2) qualify: AA must appear in  threshold UNIQUE samples in BOTH groups\n",
    "    counts = (survivor_df.groupby(['aaSeqCDR3','group'])['Sample_ID']\n",
    "            .nunique().unstack(fill_value=0))\n",
    "    qualified_tcrs = counts[(counts.get('fast',0) >= threshold) &\n",
    "                            (counts.get('slow',0) >= threshold)].index\n",
    "\n",
    "    # === keep these objects with ALL columns ===\n",
    "    qualified_df = sub[sub['aaSeqCDR3'].isin(qualified_tcrs)].copy()\n",
    "\n",
    "    fast_df = qualified_df[qualified_df['group'] == 'fast'].copy()\n",
    "    slow_df = qualified_df[qualified_df['group'] == 'slow'].copy()\n",
    "\n",
    "    # Expression matrices (needs 'readFraction' present in sub)\n",
    "    fast_expr = fast_df.pivot_table(index='aaSeqCDR3', columns='Sample_ID',\n",
    "                                    values='readFraction', fill_value=0)\n",
    "    slow_expr = slow_df.pivot_table(index='aaSeqCDR3', columns='Sample_ID',\n",
    "                                    values='readFraction', fill_value=0)    \n",
    "    #Pearson correlations\n",
    "    fast_expr_numpy = fast_expr.to_numpy()\n",
    "    fast_corr_numpy = np.corrcoef(fast_expr_numpy)\n",
    "    fast_corr = pd.DataFrame(fast_corr_numpy, index=fast_expr.index, columns=fast_expr.index)\n",
    "\n",
    "    slow_expr_numpy = slow_expr.to_numpy()\n",
    "    slow_corr_numpy = np.corrcoef(slow_expr_numpy)\n",
    "    slow_corr = pd.DataFrame(slow_corr_numpy, index=slow_expr.index, columns=slow_expr.index)\n",
    "    return fast_df, slow_df, fast_corr, slow_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4d2ca8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def intersection_graph_with_weights(G1, G2, attr1='weight', attr2='weight', new_attr1='fast_weight', new_attr2='slow_weight'):\n",
    "    \"\"\"\n",
    "    Returns a new graph with only edges present in both G1 and G2.\n",
    "    Edge attributes from G1 and G2 are preserved as new_attr1 and new_attr2.\n",
    "    Only nodes with at least one shared edge will be included.\n",
    "    \"\"\"\n",
    "    G_inter = nx.Graph()\n",
    "    # Create sets of edges as (nodeA, nodeB) sorted tuples for undirected comparison\n",
    "    edges1 = set(tuple(sorted(e)) for e in G1.edges())\n",
    "    edges2 = set(tuple(sorted(e)) for e in G2.edges())\n",
    "    shared_edges = edges1 & edges2\n",
    "    \n",
    "    for u, v in shared_edges:\n",
    "        # Copy correlation weights from both graphs\n",
    "        w1 = G1[u][v][attr1]\n",
    "        w2 = G2[u][v][attr2]\n",
    "        G_inter.add_edge(u, v, **{new_attr1: w1, new_attr2: w2})\n",
    "    return G_inter\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "def largest_component_size(G):\n",
    "    if G.number_of_nodes() == 0:\n",
    "        return 0\n",
    "    return max(len(c) for c in nx.connected_components(G))\n",
    "\n",
    "def tune_overlap_graph(\n",
    "    fast_corr, slow_corr, \n",
    "    target_size=80, \n",
    "    init_min_r_fast=None, \n",
    "    init_min_r_slow=None,\n",
    "    step=0.01, \n",
    "    max_iter=30, \n",
    "    tol=5, \n",
    "    verbose=True\n",
    "):\n",
    "    # Estimate good starting thresholds if not given\n",
    "    if init_min_r_fast is None:\n",
    "        fast_vals = fast_corr.values[np.triu_indices_from(fast_corr, k=1)]\n",
    "        init_min_r_fast = np.percentile(fast_vals, 99)  # or adjust\n",
    "    if init_min_r_slow is None:\n",
    "        slow_vals = slow_corr.values[np.triu_indices_from(slow_corr, k=1)]\n",
    "        init_min_r_slow = np.percentile(slow_vals, 99)\n",
    "        \n",
    "    min_r_fast = init_min_r_fast\n",
    "    min_r_slow = init_min_r_slow\n",
    "    best_diff = float('inf')\n",
    "    best_result = None\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        # Build graphs\n",
    "        G_fast = build_pos_corr_graph_faster(fast_corr, min_r_fast)\n",
    "        G_slow = build_pos_corr_graph_faster(slow_corr, min_r_slow)\n",
    "        # Intersect\n",
    "        G_overlap = intersection_graph_with_weights(G_fast, G_slow)\n",
    "        # Largest component size\n",
    "        lcc_size = largest_component_size(G_overlap)\n",
    "        diff = abs(lcc_size - target_size)\n",
    "        if verbose:\n",
    "            print(f\"Iter {i}: min_r_fast={min_r_fast:.4f}, min_r_slow={min_r_slow:.4f}, \"\n",
    "                  f\"LCC size={lcc_size}, nodes={G_overlap.number_of_nodes()}, edges={G_overlap.number_of_edges()}\")\n",
    "        # Save best result so far\n",
    "        if diff < best_diff:\n",
    "            best_diff = diff\n",
    "            best_result = (min_r_fast, min_r_slow, G_fast, G_slow, G_overlap, lcc_size)\n",
    "        # Converged?\n",
    "        if diff <= tol:\n",
    "            break\n",
    "        # Adjust thresholds:\n",
    "        if lcc_size > target_size:\n",
    "            # Too big: increase thresholds\n",
    "            min_r_fast += step\n",
    "            min_r_slow += step\n",
    "        else:\n",
    "            # Too small: decrease thresholds\n",
    "            min_r_fast -= step\n",
    "            min_r_slow -= step\n",
    "    # Return best result\n",
    "    return {\n",
    "        \"min_r_fast\": best_result[0],\n",
    "        \"min_r_slow\": best_result[1],\n",
    "        \"G_fast\": best_result[2],\n",
    "        \"G_slow\": best_result[3],\n",
    "        \"G_overlap\": best_result[4],\n",
    "        \"lcc_size\": best_result[5]\n",
    "    }\n",
    "\n",
    "def build_pos_corr_graph_faster(corr_matrix, min_r=None):\n",
    "    \"\"\"\n",
    "    Efficiently build a NetworkX graph from a correlation matrix using a threshold.\n",
    "    Precompute the edge list directly from the correlation matrix (using NumPy or pandas).\n",
    "    Only adds edges where r >= min_r (positive correlation).\n",
    "    \"\"\"\n",
    "    tcrs = np.array(corr_matrix.index)\n",
    "    # Get upper triangle indices (excluding diagonal)\n",
    "    mask = np.triu(np.ones(corr_matrix.shape, dtype=bool), k=1)\n",
    "    # Get pairs and correlations\n",
    "    i_idx, j_idx = np.where(mask)\n",
    "    corrs = corr_matrix.values[mask]\n",
    "    if min_r is not None:\n",
    "        select = np.where(corrs >= min_r)[0]\n",
    "        i_idx = i_idx[select]\n",
    "        j_idx = j_idx[select]\n",
    "        corrs = corrs[select]\n",
    "    # Build edge list\n",
    "    edge_list = [\n",
    "        (tcrs[i], tcrs[j], {'weight': float(corr)})\n",
    "        for i, j, corr in zip(i_idx, j_idx, corrs)\n",
    "    ]\n",
    "    # Create graph\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(edge_list)\n",
    "    return G\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save function\n",
    "def save_graph(graph, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(graph, f)\n",
    "\n",
    "# Load function\n",
    "def load_graph(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def create_node_meta_df_for_gnn_kfold(G_overlap, fast_df, slow_df):\n",
    "    # This function creates static (predetermined) information about nodes (TCRs) that will be later used\n",
    "    # as initial node embedding input in GNN algorithm.\n",
    "    \n",
    "    def aggregate_edge_weights(G, weight_key):\n",
    "        \"\"\"Return a dict mapping node -> sum of edge weights for that key.\"\"\"\n",
    "        return {\n",
    "            node: sum(G[node][nbr].get(weight_key, 0) for nbr in G.neighbors(node))\n",
    "            for node in G.nodes\n",
    "        }\n",
    "\n",
    "    sum_fast = aggregate_edge_weights(G_overlap, 'fast_weight')\n",
    "    avg_fast = {k: v / G_overlap.degree[k] if G_overlap.degree[k] else 0 for k, v in sum_fast.items()}\n",
    "    sum_slow = aggregate_edge_weights(G_overlap, 'slow_weight')\n",
    "    avg_slow = {k: v / G_overlap.degree[k] if G_overlap.degree[k] else 0 for k, v in sum_slow.items()}\n",
    "\n",
    "    overlap_node_meta_df = pd.DataFrame({'id': list(G_overlap.nodes())})\n",
    "    overlap_node_meta_df['sum_fast_weight'] = overlap_node_meta_df['id'].map(sum_fast)\n",
    "    overlap_node_meta_df['avg_fast_weight'] = overlap_node_meta_df['id'].map(avg_fast)\n",
    "    overlap_node_meta_df['sum_slow_weight'] = overlap_node_meta_df['id'].map(sum_slow)\n",
    "    overlap_node_meta_df['avg_slow_weight'] = overlap_node_meta_df['id'].map(avg_slow)\n",
    "\n",
    "    # Define function to clean allele info from V/J genes\n",
    "    def clean_vj(gene_str):\n",
    "        return re.sub(r\"\\*.*\", \"\", gene_str) if isinstance(gene_str, str) else gene_str\n",
    "\n",
    "    # Generate per-group metadata from fast_df\n",
    "    vj_map_fast = (\n",
    "        fast_df.groupby(\"aaSeqCDR3\")\n",
    "        .agg({\n",
    "            \"allVHitsWithScore\": lambda x: x.mode()[0] if not x.mode().empty else \"\",\n",
    "            \"allJHitsWithScore\": lambda x: x.mode()[0] if not x.mode().empty else \"\",\n",
    "            \"nSeqCDR3\": pd.Series.nunique,\n",
    "            \"Sample_ID\": pd.Series.nunique\n",
    "        })\n",
    "        .rename(columns={\n",
    "            \"allVHitsWithScore\": \"V_gene_fast\",\n",
    "            \"allJHitsWithScore\": \"J_gene_fast\",\n",
    "            \"nSeqCDR3\": \"codon_diversity_fast\",\n",
    "            \"Sample_ID\": \"sample_prevalence_fast\"\n",
    "        })\n",
    "    )\n",
    "\n",
    "    # Generate per-group metadata from slow_df\n",
    "    vj_map_slow = (\n",
    "        slow_df.groupby(\"aaSeqCDR3\")\n",
    "        .agg({\n",
    "            \"allVHitsWithScore\": lambda x: x.mode()[0] if not x.mode().empty else \"\",\n",
    "            \"allJHitsWithScore\": lambda x: x.mode()[0] if not x.mode().empty else \"\",\n",
    "            \"nSeqCDR3\": pd.Series.nunique,\n",
    "            \"Sample_ID\": pd.Series.nunique\n",
    "        })\n",
    "        .rename(columns={\n",
    "            \"allVHitsWithScore\": \"V_gene_slow\",\n",
    "            \"allJHitsWithScore\": \"J_gene_slow\",\n",
    "            \"nSeqCDR3\": \"codon_diversity_slow\",\n",
    "            \"Sample_ID\": \"sample_prevalence_slow\"\n",
    "        })\n",
    "    )\n",
    "\n",
    "    # Clean allele info\n",
    "    vj_map_fast[\"V_gene_fast\"] = vj_map_fast[\"V_gene_fast\"].apply(clean_vj)\n",
    "    vj_map_fast[\"J_gene_fast\"] = vj_map_fast[\"J_gene_fast\"].apply(clean_vj)\n",
    "    vj_map_slow[\"V_gene_slow\"] = vj_map_slow[\"V_gene_slow\"].apply(clean_vj)\n",
    "    vj_map_slow[\"J_gene_slow\"] = vj_map_slow[\"J_gene_slow\"].apply(clean_vj)\n",
    "\n",
    "    # Merge both metadata sets into it\n",
    "    overlap_node_meta_df = (\n",
    "        overlap_node_meta_df\n",
    "        .merge(vj_map_fast, left_on='id', right_index=True, how='left')\n",
    "        .merge(vj_map_slow, left_on='id', right_index=True, how='left')\n",
    "    )\n",
    "\n",
    "    # graph level stats\n",
    "    degree_dict = dict(G_overlap.degree())\n",
    "    betweenness = nx.betweenness_centrality(G_overlap)\n",
    "    closeness = nx.closeness_centrality(G_overlap)\n",
    "    try:\n",
    "        eigenvector = nx.eigenvector_centrality(G_overlap, max_iter=500)\n",
    "    except nx.PowerIterationFailedConvergence as e:\n",
    "        print(\"Eigenvector centrality did not converge! Trying with lower tolerance and more iterations...\")\n",
    "        # Try with higher max_iter or a higher tolerance (less strict convergence)\n",
    "        try:\n",
    "            eigenvector = nx.eigenvector_centrality(G_overlap, max_iter=1000, tol=1e-02)\n",
    "        except nx.PowerIterationFailedConvergence:\n",
    "            print(\"Still did not converge. Returning degree centrality as fallback.\")\n",
    "            # Fallback: use a simpler centrality\n",
    "            eigenvector = nx.degree_centrality(G_overlap)\n",
    "\n",
    "    # Convert metrics to Series with index as node IDs\n",
    "    degree_series = pd.Series(degree_dict, name='degree')\n",
    "    betweenness_series = pd.Series(betweenness, name='betweenness')\n",
    "    closeness_series = pd.Series(closeness, name='closeness')\n",
    "    eigenvector_series = pd.Series(eigenvector, name='eigenvector')\n",
    "\n",
    "    # Merge all into overlap_node_meta_df\n",
    "    overlap_node_meta_df = (\n",
    "        overlap_node_meta_df\n",
    "        .merge(degree_series, left_on='id', right_index=True, how='left')\n",
    "        .merge(betweenness_series, left_on='id', right_index=True, how='left')\n",
    "        .merge(closeness_series, left_on='id', right_index=True, how='left')\n",
    "        .merge(eigenvector_series, left_on='id', right_index=True, how='left')\n",
    "    )\n",
    "\n",
    "    # Step 1: Combine all V genes to create a unified vocabulary\n",
    "    all_v_genes = pd.concat([\n",
    "        overlap_node_meta_df['V_gene_fast'],\n",
    "        overlap_node_meta_df['V_gene_slow']\n",
    "    ]).dropna().unique()\n",
    "    # Step 2: Create mapping from V gene name to integer ID\n",
    "    unique_v_genes = sorted(set(all_v_genes))\n",
    "    v_gene_to_id = {v: i for i, v in enumerate(unique_v_genes)}\n",
    "    # Step 3: Map V_gene columns to integer IDs\n",
    "    overlap_node_meta_df['V_gene_fast_id'] = overlap_node_meta_df['V_gene_fast'].map(v_gene_to_id)\n",
    "    overlap_node_meta_df['V_gene_slow_id'] = overlap_node_meta_df['V_gene_slow'].map(v_gene_to_id)\n",
    "    # Step 1: Combine all J genes to create a unified vocabulary\n",
    "    all_j_genes = pd.concat([\n",
    "        overlap_node_meta_df['J_gene_fast'],\n",
    "        overlap_node_meta_df['J_gene_slow']\n",
    "    ]).dropna().unique()\n",
    "    unique_j_genes = sorted(set(all_j_genes))\n",
    "    j_gene_to_id = {j: i for i, j in enumerate(unique_j_genes)}\n",
    "    overlap_node_meta_df['J_gene_fast_id'] = overlap_node_meta_df['J_gene_fast'].map(j_gene_to_id)\n",
    "    overlap_node_meta_df['J_gene_slow_id'] = overlap_node_meta_df['J_gene_slow'].map(j_gene_to_id)\n",
    "\n",
    "    return overlap_node_meta_df\n",
    "\n",
    "\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "\n",
    "def add_tcr_biochem_features_kfold(df: pd.DataFrame, seq_col: str = \"id\") -> pd.DataFrame:\n",
    "    if seq_col not in df.columns:\n",
    "        raise KeyError(f\"Column '{seq_col}' not found in df\")\n",
    "\n",
    "    # Define which columns well produce\n",
    "    biochem_cols = [\n",
    "        \"length\", \"aromaticity\", \"instability_index\", \"isoelectric_point\",\n",
    "        \"gravy\", \"charge_7.0\", \"molecular_weight\", \"helix_frac\", \"turn_frac\", \"sheet_frac\"\n",
    "    ]\n",
    "\n",
    "    # Helper for one sequence\n",
    "    def analyze_sequence(seq: str):\n",
    "        try:\n",
    "            if not isinstance(seq, str) or not seq:\n",
    "                raise ValueError(\"empty\")\n",
    "            s = seq.upper()\n",
    "            # Optionally reject non-standard residues to avoid ProtParam errors\n",
    "            if any(c not in \"ACDEFGHIKLMNPQRSTVWY\" for c in s):\n",
    "                raise ValueError(\"non-standard AA\")\n",
    "            prot = ProteinAnalysis(s)\n",
    "            ss_h, ss_t, ss_s = prot.secondary_structure_fraction()  # helix, turn, sheet\n",
    "            return {\n",
    "                \"length\": len(s),\n",
    "                \"aromaticity\": prot.aromaticity(),\n",
    "                \"instability_index\": prot.instability_index(),\n",
    "                \"isoelectric_point\": prot.isoelectric_point(),\n",
    "                \"gravy\": prot.gravy(),\n",
    "                \"charge_7.0\": prot.charge_at_pH(7.0),\n",
    "                \"molecular_weight\": prot.molecular_weight(),\n",
    "                \"helix_frac\": ss_h,\n",
    "                \"turn_frac\": ss_t,\n",
    "                \"sheet_frac\": ss_s,\n",
    "            }\n",
    "        except Exception as e:\n",
    "            # Fill NaNs on any issue; keeps shapes consistent\n",
    "            print(f\"---  Error processing sequence: '{seq}' | Message: {e} ---\")           \n",
    "            return {k: np.nan for k in biochem_cols}\n",
    "\n",
    "    # mapping\n",
    "    uniq = df[seq_col].astype(str).unique()\n",
    "    feat_map = {seq: analyze_sequence(seq) for seq in uniq}\n",
    "\n",
    "    feats_df = pd.DataFrame.from_dict(feat_map, orient=\"index\")\n",
    "    feats_df.index.name = seq_col\n",
    "\n",
    "    # -- NaN handling --\n",
    "    nans_before = feats_df.isna().sum().sum()\n",
    "    if nans_before > 0:\n",
    "        print(f\"Imputing NaNs in biochemical features... ({nans_before} missing values detected)\")\n",
    "        # Calculate the mean for each feature column (ignoring NaNs)\n",
    "        column_means = feats_df.mean()\n",
    "        feats_df = feats_df.fillna(column_means)\n",
    "        # Check for any columns that are still all-NaN (entire column NaN)\n",
    "        for col in feats_df.columns:\n",
    "            if feats_df[col].isna().all():\n",
    "                print(f\"Column '{col}' was all NaN! Filling with zeros.\")\n",
    "        # Fill any remaining NaNs with 0\n",
    "        feats_df = feats_df.fillna(0)\n",
    "\n",
    "    # Merge back\n",
    "    out = df.merge(feats_df, left_on=seq_col, right_index=True, how=\"left\")\n",
    "    return out\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def build_orig_dyn_feat_dict_for_filtered(\n",
    "    clonotype_df: pd.DataFrame,\n",
    "    node_idx_map: dict,\n",
    "    all_samples: list\n",
    "    ) -> (dict, dict):\n",
    "\n",
    "    dyn_feat_dict = {}\n",
    "    presence_mask_dict = {}\n",
    "    num_nodes = len(node_idx_map)\n",
    "\n",
    "    # --- 1. Vectorized Pre-processing ---\n",
    "    \n",
    "    # Keep only rows corresponding to TCRs that are nodes in our graph.\n",
    "    relevant_tcrs = list(node_idx_map.keys())\n",
    "    filtered_df = clonotype_df[clonotype_df['aaSeqCDR3'].isin(relevant_tcrs)].copy() # <-- CHANGED\n",
    "\n",
    "    # Convert TCR strings to their integer indices all at once.\n",
    "    filtered_df['node_index'] = filtered_df['aaSeqCDR3'].map(node_idx_map) # <-- CHANGED\n",
    "    \n",
    "    # Calculate logFraction for all relevant TCRs at once.\n",
    "    filtered_df['logFraction'] = np.log10(filtered_df['readFraction'] + 1e-10)\n",
    "\n",
    "    # Group the pre-processed DataFrame for fast access.\n",
    "    grouped = filtered_df.groupby('Sample_ID') # <-- CHANGED\n",
    "    \n",
    "    # --- 2. Build Dictionaries ---\n",
    "    \n",
    "    for sample_id in all_samples:\n",
    "        dyn_features = torch.zeros(num_nodes, 1, dtype=torch.float32)\n",
    "        \n",
    "        try:\n",
    "            sample_df = grouped.get_group(sample_id)\n",
    "            \n",
    "            indices = torch.tensor(sample_df['node_index'].values, dtype=torch.long)\n",
    "            values = torch.tensor(sample_df['logFraction'].values, dtype=torch.float32)\n",
    "            \n",
    "            dyn_features[indices, 0] = values\n",
    "            presence_mask_dict[sample_id] = indices\n",
    "        except KeyError:\n",
    "            presence_mask_dict[sample_id] = torch.tensor([], dtype=torch.long)\n",
    "            \n",
    "        dyn_feat_dict[sample_id] = dyn_features\n",
    "            \n",
    "    return dyn_feat_dict, presence_mask_dict\n",
    "\n",
    "def collate_fn_filtered(batch):\n",
    "    \"\"\"\n",
    "    Batches samples by creating a unified binary mask for node presence.\n",
    "    \"\"\"\n",
    "    # 1. Unpack the batch. `mask_indices_list` now contains tensors of different lengths.\n",
    "    g_list, dyn_list, mask_indices_list, label_list = zip(*batch)\n",
    "\n",
    "    # 2. Handle the graph, dynamic features, and labels (your existing logic is correct).\n",
    "    g = g_list[0]  # All graphs are the same object.\n",
    "    dyn = torch.stack(dyn_list, dim=0)      # Shape: [batch_size, num_nodes, dyn_dim]\n",
    "    labels = torch.tensor(label_list)        # Shape: [batch_size]\n",
    "\n",
    "    # 3. --- THE FIX: Build the batch-level binary mask ---\n",
    "    batch_size = dyn.shape[0]\n",
    "    num_nodes = dyn.shape[1]\n",
    "\n",
    "    # Create an empty \"switchboard\" of all zeros.\n",
    "    mask = torch.zeros(batch_size, num_nodes, dtype=torch.float32) # Using float32 for broad compatibility.\n",
    "\n",
    "    # Iterate through each sample in the batch to flip the right switches.\n",
    "    for i, indices in enumerate(mask_indices_list):\n",
    "        # `i` is the sample's row in the batch.\n",
    "        # `indices` is the list of TCR node indices to turn on for that sample.\n",
    "        if indices.numel() > 0:  # Check if there are any present nodes.\n",
    "            mask[i, indices] = 1.0\n",
    "\n",
    "    return g, dyn, mask, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d1a39abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Number of devices: 2\n",
      "Using GPU device 0: NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "device = connect_gpu()\n",
    "\n",
    "cols_to_normalize = [\n",
    "    'sum_fast_weight', 'avg_fast_weight', 'sum_slow_weight',\n",
    "    'avg_slow_weight', 'codon_diversity_fast', 'sample_prevalence_fast',\n",
    "    'codon_diversity_slow', 'sample_prevalence_slow',\n",
    "    'degree', 'betweenness', 'closeness', 'eigenvector',\n",
    "    'length', 'aromaticity', 'instability_index', 'isoelectric_point',\n",
    "    'gravy', 'charge_7.0', 'molecular_weight',\n",
    "    'helix_frac', 'turn_frac', 'sheet_frac'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88e913d",
   "metadata": {},
   "source": [
    "#### RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b775d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_kfold_loop():\n",
    "    BEST_CONFIG = {\n",
    "        'hidden': 64,\n",
    "        'layers': 2,\n",
    "        'dropout': 0.411,\n",
    "        'lr': 0.004633064839793302,\n",
    "        'wd': 5.912267556002407e-05,\n",
    "        'readout': 'mean'\n",
    "    }\n",
    "    SEEDS = random.sample(range(0, 10000), 5)\n",
    "    # SEEDS = [5, 42, 123, 1024, 2025]\n",
    "    final_test_accuracies = []\n",
    "    final_test_f1_scores = []\n",
    "    print(\"Loading Clonotype Files...\")\n",
    "    clonotype_df = load_clonotype()\n",
    "    all_samples = clonotype_df['Sample_ID'].unique().tolist()\n",
    "    colon_meta = pd.read_csv(\"/home/dsi/orrbavly/GNN_project/data/metadata/colon_meta.csv\")\n",
    "    colon_meta = colon_meta.loc[:, ~colon_meta.columns.str.contains('^Unnamed')]\n",
    "    relevant_values = ['0', '1', '1a', '1b', '2']\n",
    "    # List of samples to exclude - did not undergo mixcr due to missing lanes.\n",
    "    exclude_samples = ['P2-S8', 'P2-S9', 'P2-S19', 'P8-S18'] # 'P4-S1'\n",
    "    colon_meta = colon_meta[colon_meta['N'].isin(relevant_values)]\n",
    "    colon_meta = colon_meta[~colon_meta['sample_id'].isin(exclude_samples)].copy()\n",
    "\n",
    "    valid_samples = colon_meta['sample_id'].tolist()\n",
    "\n",
    "    # --- 1. match labels to valid_samples ---\n",
    "    labels = []\n",
    "    for sid in valid_samples:\n",
    "        row = colon_meta.loc[colon_meta[\"sample_id\"] == sid, \"N\"]\n",
    "        if row.empty:\n",
    "            raise ValueError(f\"Sample {sid} not found in colon_meta\")\n",
    "        labels.append(0 if row.iloc[0] == \"0\" else 1)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # each r_state changes the results from here:\n",
    "    for r_state in SEEDS:\n",
    "        print(f\"Running Seed {r_state}\")\n",
    "        print(\"Loading data...\")\n",
    "        # split samples ids\n",
    "        trainval_ids, test_ids, y_trainval, y_test = train_test_split(\n",
    "            valid_samples, labels, test_size=0.15, stratify=labels, random_state=r_state)\n",
    "        unlabeled_ids = [s for s in all_samples if s not in valid_samples]\n",
    "        graph_universe_ids = trainval_ids + unlabeled_ids\n",
    "        # calculate corrolation and graph creation\n",
    "        fast_df, slow_df, fast_corr, slow_corr = create_corrolation(clonotype_df, graph_universe_ids,threshold=5)\n",
    "        G_fast_sub = build_pos_corr_graph_faster(fast_corr, min_r=0.565)\n",
    "        G_slow_sub = build_pos_corr_graph_faster(slow_corr, min_r=0.585)\n",
    "        G_overlap = intersection_graph_with_weights(G_fast_sub, G_slow_sub)\n",
    "        # Get all connected components sorted by size (largest to smallest)\n",
    "        components = sorted(nx.connected_components(G_overlap), key=len, reverse=True)\n",
    "        G_overlap_lcc_largest = G_overlap.subgraph(components[0]).copy()\n",
    "        overlap_node_meta_df = create_node_meta_df_for_gnn_kfold(G_overlap_lcc_largest, fast_df, slow_df)\n",
    "        overlap_node_meta_df = add_tcr_biochem_features_kfold(overlap_node_meta_df, seq_col=\"id\")\n",
    "\n",
    "        # load graph and meta\n",
    "        # overlap_node_meta_df = pd.read_csv(\"/dsi/efroni-lab/sbm/OrrBavly/colon_data/new_mixcr/tables/g_overlap_whole_node_meta_cor_3_87_89.csv\")\n",
    "        # G_overlap_lcc_largest = load_graph(\"/dsi/efroni-lab/sbm/OrrBavly/colon_data/graphs/G_overlap_whole_corr_3_87_89.pkl\")\n",
    "        # Load the run GNN model:\n",
    "        G_overlap_dgl_filtered = dgl.from_networkx(G_overlap_lcc_largest)\n",
    "\n",
    "        node_list = list(G_overlap_lcc_largest.nodes)\n",
    "        node_idx_filtered = {tcr: i for i, tcr in enumerate(node_list)}\n",
    "        scaler = StandardScaler()\n",
    "        node_meta_scaled = overlap_node_meta_df.copy()\n",
    "        node_meta_scaled[cols_to_normalize] = scaler.fit_transform(node_meta_scaled[cols_to_normalize])\n",
    "\n",
    "        # Now, create the train and validation sets for this single run\n",
    "        train_ids, val_ids, train_labels, val_labels = train_test_split(\n",
    "            trainval_ids,\n",
    "            y_trainval,\n",
    "            test_size=0.1,  #  ~10% of the 85% becomes validation set\n",
    "            stratify=y_trainval,\n",
    "            random_state=r_state)\n",
    "        \n",
    "        # Build dynamic features for ALL samples, but based on the NEW graph's nodes.\n",
    "        orig_dyn_feat_dict, presence_mask_dict = build_orig_dyn_feat_dict_for_filtered(\n",
    "            clonotype_df,\n",
    "            node_idx_filtered, \n",
    "            all_samples)\n",
    "        # Fit the scaler ONLY on the training data.\n",
    "        dyn_feat_dict_scaled = scale_dyn_dict(\n",
    "            train_ids,     # <-- Fit scaler on these samples\n",
    "            all_samples,   # <-- Transform all samples\n",
    "            orig_dyn_feat_dict)\n",
    "        \n",
    "        kfold_dataset = SingleGraphSampleDataset(\n",
    "            G_overlap_dgl_filtered,         # <-- NEW leak-proof graph\n",
    "            node_meta_scaled,     # <-- NEW node features\n",
    "            dyn_feat_dict_scaled,  # Placeholder, will be updated\n",
    "            presence_mask_dict,    # <-- The new presence masks\n",
    "            colon_meta.set_index(\"sample_id\")[\"N\"])\n",
    "        \n",
    "        train_set = sample_id_subset(kfold_dataset, train_ids)\n",
    "        val_set   = sample_id_subset(kfold_dataset, val_ids)\n",
    "        test_set  = sample_id_subset(kfold_dataset, test_ids)\n",
    "        BATCH_SIZE = 16\n",
    "        train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_fn_filtered)\n",
    "        val_loader   = DataLoader(val_set,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn_filtered)\n",
    "        test_loader  = DataLoader(test_set,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn_filtered)\n",
    "\n",
    "        v_fast_max_size = torch.max(G_overlap_dgl_filtered.ndata[\"V_gene_fast_id\"]).item() + 1\n",
    "        j_fast_max_size = torch.max(G_overlap_dgl_filtered.ndata[\"J_gene_fast_id\"]).item() + 1\n",
    "        v_slow_max_size = torch.max(G_overlap_dgl_filtered.ndata[\"V_gene_slow_id\"]).item() + 1\n",
    "        j_slow_max_size = torch.max(G_overlap_dgl_filtered.ndata[\"J_gene_slow_id\"]).item() + 1\n",
    "\n",
    "        static_dim = kfold_dataset.g.ndata['static'].shape[1]\n",
    "        dyn_dim = next(iter(kfold_dataset.dyn.values())).shape[1]\n",
    "\n",
    "        # Run MODEL\n",
    "        model = MaskedGIN(\n",
    "            static_dim=static_dim,\n",
    "            dyn_dim=dyn_dim,\n",
    "            hidden_dim=BEST_CONFIG['hidden'],\n",
    "            num_layers=BEST_CONFIG['layers'],\n",
    "            dropout=BEST_CONFIG['dropout'],\n",
    "            readout=BEST_CONFIG['readout'],\n",
    "            v_vocab=v_fast_max_size,\n",
    "            j_vocab=j_fast_max_size,\n",
    "            v_slow_vocab=v_slow_max_size,\n",
    "            j_slow_vocab=j_slow_max_size\n",
    "        ).to(device)\n",
    "\n",
    "        # Calculate weights ONLY from the training set to avoid data leakage.\n",
    "        class_counts = np.bincount(train_labels)\n",
    "        weights = 1.0 / torch.tensor(class_counts, dtype=torch.float32)\n",
    "        criterion = nn.CrossEntropyLoss(weight=weights.to(device))\n",
    "        opt = torch.optim.AdamW(model.parameters(), lr=BEST_CONFIG['lr'], weight_decay=BEST_CONFIG['wd'])\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='max', factor=0.2, patience=10)\n",
    "\n",
    "        # --- TRAINING LOOP ---\n",
    "        best_ba = 0\n",
    "        patience = 100\n",
    "        epochs_since_best = 0\n",
    "        max_epochs = 250\n",
    "        best_state = None\n",
    "        print(\"Running the Model\")\n",
    "        for epoch in range(max_epochs):\n",
    "            train_loss = train_one_epoch(model, train_loader, device, criterion, opt)\n",
    "            ba, f1 = evaluate(model, val_loader, device)\n",
    "            # The scheduler looks at the validation balanced accuracy to make a decision\n",
    "            scheduler.step(ba) \n",
    "            if epoch % 25 == 0:\n",
    "                print(f\"Epoch {epoch:02d} | Loss {train_loss:.4f} | Val bal-acc {ba:.3f} | Val F1 {f1:.3f}\")\n",
    "            if ba > best_ba:\n",
    "                best_ba = ba\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "                epochs_since_best = 0\n",
    "            else:\n",
    "                epochs_since_best += 1\n",
    "            if epochs_since_best >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}.\")\n",
    "                break\n",
    "\n",
    "        model.load_state_dict(best_state)\n",
    "        test_balacc, test_f1 = evaluate(model, test_loader, device)\n",
    "        print(f\"\\n--- Final Test Results ---\")\n",
    "        print(f\"Test Balanced Accuracy: {test_balacc:.4f}\")\n",
    "        print(f\"Test F1-Score:          {test_f1:.4f}\")\n",
    "        final_test_accuracies.append(test_balacc)\n",
    "        final_test_f1_scores.append(test_f1)\n",
    "\n",
    "    print(\"---  FINAL AGGREGATED RESULTS ---\")\n",
    "    mean_ba = np.mean(final_test_accuracies)\n",
    "    std_ba = np.std(final_test_accuracies)\n",
    "    mean_f1 = np.mean(final_test_f1_scores)\n",
    "    std_f1 = np.std(final_test_f1_scores)\n",
    "\n",
    "    print(f\"Based on {len(SEEDS)} random seed runs:\")\n",
    "    print(f\"Final Test Balanced Accuracy: {mean_ba:.4f}  {std_ba:.4f}\")\n",
    "    print(f\"Final Test F1-Score:          {mean_f1:.4f}  {std_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9110b498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Clonotype Files...\n",
      "Running Seed 8986\n",
      "Loading data...\n",
      "Running the Model\n",
      "Epoch 00 | Loss 0.7038 | Val bal-acc 0.444 | Val F1 0.000\n",
      "Epoch 25 | Loss 0.5671 | Val bal-acc 0.478 | Val F1 0.364\n",
      "Epoch 50 | Loss 0.4663 | Val bal-acc 0.478 | Val F1 0.364\n",
      "Epoch 75 | Loss 0.4814 | Val bal-acc 0.478 | Val F1 0.364\n",
      "Epoch 100 | Loss 0.4517 | Val bal-acc 0.478 | Val F1 0.364\n",
      "Early stopping at epoch 104.\n",
      "\n",
      "--- Final Test Results ---\n",
      "Test Balanced Accuracy: 0.4444\n",
      "Test F1-Score:          0.4167\n",
      "Running Seed 2124\n",
      "Loading data...\n",
      "Running the Model\n",
      "Epoch 00 | Loss 0.6983 | Val bal-acc 0.444 | Val F1 0.000\n",
      "Epoch 25 | Loss 0.5061 | Val bal-acc 0.689 | Val F1 0.600\n",
      "Epoch 50 | Loss 0.4296 | Val bal-acc 0.689 | Val F1 0.600\n",
      "Epoch 75 | Loss 0.4263 | Val bal-acc 0.689 | Val F1 0.600\n",
      "Epoch 100 | Loss 0.3857 | Val bal-acc 0.689 | Val F1 0.600\n",
      "Early stopping at epoch 111.\n",
      "\n",
      "--- Final Test Results ---\n",
      "Test Balanced Accuracy: 0.5222\n",
      "Test F1-Score:          0.5185\n",
      "Running Seed 4807\n",
      "Loading data...\n",
      "Running the Model\n",
      "Epoch 00 | Loss 0.7002 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 25 | Loss 0.4813 | Val bal-acc 0.533 | Val F1 0.400\n",
      "Epoch 50 | Loss 0.4180 | Val bal-acc 0.633 | Val F1 0.545\n",
      "Epoch 75 | Loss 0.4276 | Val bal-acc 0.633 | Val F1 0.545\n",
      "Epoch 100 | Loss 0.3786 | Val bal-acc 0.633 | Val F1 0.545\n",
      "Early stopping at epoch 114.\n",
      "\n",
      "--- Final Test Results ---\n",
      "Test Balanced Accuracy: 0.6556\n",
      "Test F1-Score:          0.5333\n",
      "Running Seed 1882\n",
      "Loading data...\n",
      "Running the Model\n",
      "Epoch 00 | Loss 0.6948 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 25 | Loss 0.6374 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 50 | Loss 0.4321 | Val bal-acc 0.578 | Val F1 0.500\n",
      "Epoch 75 | Loss 0.4337 | Val bal-acc 0.578 | Val F1 0.500\n",
      "Epoch 100 | Loss 0.3784 | Val bal-acc 0.578 | Val F1 0.500\n",
      "Early stopping at epoch 121.\n",
      "\n",
      "--- Final Test Results ---\n",
      "Test Balanced Accuracy: 0.4333\n",
      "Test F1-Score:          0.3158\n",
      "Running Seed 7832\n",
      "Loading data...\n",
      "Running the Model\n",
      "Epoch 00 | Loss 0.7028 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 25 | Loss 0.6066 | Val bal-acc 0.489 | Val F1 0.250\n",
      "Epoch 50 | Loss 0.4829 | Val bal-acc 0.589 | Val F1 0.444\n",
      "Epoch 75 | Loss 0.4713 | Val bal-acc 0.589 | Val F1 0.444\n",
      "Epoch 100 | Loss 0.4529 | Val bal-acc 0.589 | Val F1 0.444\n",
      "Early stopping at epoch 119.\n",
      "\n",
      "--- Final Test Results ---\n",
      "Test Balanced Accuracy: 0.4556\n",
      "Test F1-Score:          0.1538\n",
      "---  FINAL AGGREGATED RESULTS ---\n",
      "Based on 5 random seed runs:\n",
      "Final Test Balanced Accuracy: 0.5022  0.0827\n",
      "Final Test F1-Score:          0.3876  0.1407\n"
     ]
    }
   ],
   "source": [
    "main_kfold_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fb2050",
   "metadata": {},
   "source": [
    "# All-Samples Master Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1b2ce249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import networkx as nx\n",
    "import dgl\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b94c656e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212\n"
     ]
    }
   ],
   "source": [
    "mixcr_dir = \"/dsi/efroni-lab/sbm/OrrBavly/colon_data/new_mixcr/TRB/\"\n",
    "meta_file = \"/home/dsi/orrbavly/GNN_project/data/colon_meta_time.csv\"\n",
    "\n",
    "time_threshold = 750\n",
    "meta_df = pd.read_csv(meta_file)\n",
    "meta_df = meta_df[[\"Sample_ID\", \"extraction_time\"]]\n",
    "\n",
    "# Columns to keep from MiXCR files\n",
    "mixcr_cols = [\n",
    "    \"aaSeqCDR3\", \"nSeqCDR3\", \"readCount\", 'readFraction',\n",
    "    \"allVHitsWithScore\", \"allDHitsWithScore\", \"allJHitsWithScore\"\n",
    "]\n",
    "clonotype_dfs = []\n",
    "\n",
    "for fname in os.listdir(mixcr_dir):\n",
    "    if fname.endswith(\".tsv\") and os.path.isfile(os.path.join(mixcr_dir, fname)):\n",
    "        sample_prefix = fname.split(\"_\")[0]\n",
    "        file_path = os.path.join(mixcr_dir, fname)\n",
    "        \n",
    "        df = pd.read_csv(file_path, sep=\"\\t\", usecols=mixcr_cols)\n",
    "        df[\"Sample_ID\"] = sample_prefix\n",
    "        clonotype_dfs.append(df)\n",
    "\n",
    "clonotype_df = pd.concat(clonotype_dfs, ignore_index=True)\n",
    "\n",
    "# Merge with metadata to get extraction time\n",
    "clonotype_df = clonotype_df.merge(meta_df, on=\"Sample_ID\", how=\"inner\")\n",
    "# Create 'group' column: fast vs slow\n",
    "clonotype_df[\"group\"] = clonotype_df[\"extraction_time\"].apply(lambda x: \"fast\" if x <= time_threshold else \"slow\")\n",
    "\n",
    "all_samples = clonotype_df['Sample_ID'].unique().tolist()\n",
    "print(clonotype_df['Sample_ID'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df26158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter only by group, not by universe IDs\n",
    "# sub = clonotype_df.loc[\n",
    "#     clonotype_df['group'].isin(['fast','slow'])\n",
    "# ].copy()\n",
    "\n",
    "# # A narrow view ONLY for the survivor/qualification logic\n",
    "# logic_df = sub[['Sample_ID','group','aaSeqCDR3']].dropna(subset=['aaSeqCDR3','group'])\n",
    "\n",
    "# # 1) survivors: AA present in BOTH groups\n",
    "# presence = (logic_df.groupby(['aaSeqCDR3','group'])['Sample_ID']\n",
    "#                     .size().unstack(fill_value=0))\n",
    "# survivors = presence.index[(presence.get('fast',0) > 0) & (presence.get('slow',0) > 0)]\n",
    "\n",
    "# survivor_df = sub[sub['aaSeqCDR3'].isin(survivors)].copy()  # keeps ALL columns\n",
    "\n",
    "# threshold = 5\n",
    "# # 2) qualify: AA must appear in  threshold UNIQUE samples in BOTH groups\n",
    "# counts = (survivor_df.groupby(['aaSeqCDR3','group'])['Sample_ID']\n",
    "#           .nunique().unstack(fill_value=0))\n",
    "# qualified_tcrs = counts[(counts.get('fast',0) >= threshold) &\n",
    "#                         (counts.get('slow',0) >= threshold)].index\n",
    "\n",
    "# # === keep these objects with ALL columns ===\n",
    "# qualified_df = sub[sub['aaSeqCDR3'].isin(qualified_tcrs)].copy()\n",
    "\n",
    "# fast_df = qualified_df[qualified_df['group'] == 'fast'].copy()\n",
    "# slow_df = qualified_df[qualified_df['group'] == 'slow'].copy()\n",
    "\n",
    "# # Expression matrices (needs 'readFraction' present in sub)\n",
    "# fast_expr = fast_df.pivot_table(index='aaSeqCDR3', columns='Sample_ID',\n",
    "#                                 values='readFraction', fill_value=0)\n",
    "# slow_expr = slow_df.pivot_table(index='aaSeqCDR3', columns='Sample_ID',\n",
    "#                                 values='readFraction', fill_value=0)\n",
    "\n",
    "# #Pearson correlations\n",
    "# fast_corr = fast_expr.T.corr(method='pearson')\n",
    "# slow_corr = slow_expr.T.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771b9c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fast_corr.to_pickle(\"/dsi/efroni-lab/sbm/OrrBavly/colon_data/new_mixcr/tables/fast_corr_5.pkl\")\n",
    "# slow_corr.to_pickle(\"/dsi/efroni-lab/sbm/OrrBavly/colon_data/new_mixcr/tables/slow_corr_5.pkl\")\n",
    "# fast_df.to_pickle(\"/dsi/efroni-lab/sbm/OrrBavly/colon_data/new_mixcr/tables/qualified_fast_df_5.pkl\")\n",
    "# slow_df.to_pickle(\"/dsi/efroni-lab/sbm/OrrBavly/colon_data/new_mixcr/tables/qualified_slow_df_5.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6917f6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: min_r_fast=0.6150, min_r_slow=0.6350, LCC size=50, nodes=4503, edges=5291\n",
      "Iter 1: min_r_fast=0.6100, min_r_slow=0.6300, LCC size=52, nodes=4720, edges=5577\n",
      "Iter 2: min_r_fast=0.6050, min_r_slow=0.6250, LCC size=57, nodes=4949, edges=5884\n",
      "Iter 3: min_r_fast=0.6000, min_r_slow=0.6200, LCC size=67, nodes=5175, edges=6192\n",
      "Iter 4: min_r_fast=0.5950, min_r_slow=0.6150, LCC size=171, nodes=5438, edges=6544\n",
      "Iter 5: min_r_fast=0.5900, min_r_slow=0.6100, LCC size=180, nodes=5659, edges=6895\n",
      "Iter 6: min_r_fast=0.5850, min_r_slow=0.6050, LCC size=245, nodes=5905, edges=7268\n"
     ]
    }
   ],
   "source": [
    "# tune_results = tune_overlap_graph(\n",
    "#     fast_corr, slow_corr,\n",
    "#     init_min_r_fast= 0.615, \n",
    "#     init_min_r_slow= 0.635,\n",
    "#     target_size=250, # LCC target size\n",
    "#     step=0.005,    \n",
    "#     max_iter=40,\n",
    "#     tol=5,        \n",
    "#     verbose=True\n",
    "# )\n",
    "# G_overlap = tune_results[\"G_overlap\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5be1a978",
   "metadata": {},
   "outputs": [],
   "source": [
    "PUBLICITY_THRESHOLDS = [5,4,3]\n",
    "CORRELATION_THRESHOLDS_PER_PUBLICITY = {\n",
    "    5: [(0.590,0.610), (0.650, 0.670), (0.74,0.76)], # Educated guesses for the \"Public World\"\n",
    "    4: [(0.62, 0.64), (0.670, 0.690), (0.75,0.77)],  # Educated guesses for the \"Semi-Public World\"\n",
    "    3: [(0.76, 0.78), (0.82, 0.84), (0.87, 0.89)]\n",
    "}\n",
    "GOOD_ENOUGH_CONFIG = {\n",
    "    'hidden': 64, 'layers': 3, 'dropout': 0.407,\n",
    "    'lr': 0.00137, 'wd': 0.00126, 'readout': 's2s', 'heads': 2\n",
    "}\n",
    "\n",
    "OUTER_SEEDS = [0, 42, 123]\n",
    "corr_files_path = \"/dsi/efroni-lab/sbm/OrrBavly/colon_data/new_mixcr/tables/\"\n",
    "all_bake_off_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4532d1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "colon_meta = pd.read_csv(\"/home/dsi/orrbavly/GNN_project/data/metadata/colon_meta.csv\")\n",
    "colon_meta = colon_meta.loc[:, ~colon_meta.columns.str.contains('^Unnamed')]\n",
    "relevant_values = ['0', '1', '1a', '1b', '2']\n",
    "# List of samples to exclude - did not undergo mixcr due to missing lanes.\n",
    "exclude_samples = ['P2-S8', 'P2-S9', 'P2-S19', 'P8-S18'] # 'P4-S1'\n",
    "colon_meta = colon_meta[colon_meta['N'].isin(relevant_values)]\n",
    "colon_meta = colon_meta[~colon_meta['sample_id'].isin(exclude_samples)].copy()\n",
    "\n",
    "valid_samples = colon_meta['sample_id'].tolist()\n",
    "# --- 1. match labels to valid_samples ---\n",
    "labels = []\n",
    "for sid in valid_samples:\n",
    "    row = colon_meta.loc[colon_meta[\"sample_id\"] == sid, \"N\"]\n",
    "    if row.empty:\n",
    "        raise ValueError(f\"Sample {sid} not found in colon_meta\")\n",
    "    labels.append(0 if row.iloc[0] == \"0\" else 1)\n",
    "labels = np.array(labels)\n",
    "\n",
    "unlabeled_ids = [s for s in all_samples if s not in valid_samples]\n",
    "\n",
    "cols_to_normalize_meta = [\n",
    "    'sum_fast_weight', 'avg_fast_weight', 'sum_slow_weight',\n",
    "    'avg_slow_weight', 'codon_diversity_fast', 'sample_prevalence_fast',\n",
    "    'codon_diversity_slow', 'sample_prevalence_slow',\n",
    "    'degree', 'betweenness', 'closeness', 'eigenvector',\n",
    "    'length', 'aromaticity', 'instability_index', 'isoelectric_point',\n",
    "    'gravy', 'charge_7.0', 'molecular_weight',\n",
    "    'helix_frac', 'turn_frac', 'sheet_frac']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3c67ca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_and_meta(corr_tuple, min_threshold, corr_files_path, graph_universe_ids):\n",
    "    # load relevant fast/slor_corr and fast/slow_df.\n",
    "    sub = clonotype_df.loc[\n",
    "        clonotype_df['Sample_ID'].isin(graph_universe_ids) &\n",
    "        clonotype_df['group'].isin(['fast','slow'])\n",
    "    ].copy()\n",
    "    logic_df = sub[['Sample_ID','group','aaSeqCDR3']].dropna(subset=['aaSeqCDR3','group'])\n",
    "    # 1) survivors: AA present in BOTH groups\n",
    "    presence = (logic_df.groupby(['aaSeqCDR3','group'])['Sample_ID']\n",
    "                        .size().unstack(fill_value=0))\n",
    "    survivors = presence.index[(presence.get('fast',0) > 0) & (presence.get('slow',0) > 0)]\n",
    "    survivor_df = sub[sub['aaSeqCDR3'].isin(survivors)].copy()  # keeps ALL columns\n",
    "\n",
    "    threshold = min_threshold\n",
    "    # 2) qualify: AA must appear in  threshold UNIQUE samples in BOTH groups\n",
    "    counts = (survivor_df.groupby(['aaSeqCDR3','group'])['Sample_ID']\n",
    "            .nunique().unstack(fill_value=0))\n",
    "    qualified_tcrs = counts[(counts.get('fast',0) >= threshold) &\n",
    "                             (counts.get('slow',0) >= threshold)].index\n",
    "    qualified_df = sub[sub['aaSeqCDR3'].isin(qualified_tcrs)].copy()\n",
    "    fast_df = qualified_df[qualified_df['group'] == 'fast'].copy()\n",
    "    slow_df = qualified_df[qualified_df['group'] == 'slow'].copy()\n",
    "    # Expression matrices (needs 'readFraction' present in sub)\n",
    "    fast_expr = fast_df.pivot_table(index='aaSeqCDR3', columns='Sample_ID',\n",
    "                                    values='readFraction', fill_value=0)\n",
    "    slow_expr = slow_df.pivot_table(index='aaSeqCDR3', columns='Sample_ID',\n",
    "                                    values='readFraction', fill_value=0)\n",
    "\n",
    "    print(\"Calculating corrolation...\")            \n",
    "    fast_expr_numpy = fast_expr.to_numpy()\n",
    "    fast_corr_numpy = np.corrcoef(fast_expr_numpy)\n",
    "    fast_corr = pd.DataFrame(fast_corr_numpy, index=fast_expr.index, columns=fast_expr.index)\n",
    "    slow_expr_numpy = slow_expr.to_numpy()\n",
    "    slow_corr_numpy = np.corrcoef(slow_expr_numpy)\n",
    "    slow_corr = pd.DataFrame(slow_corr_numpy, index=slow_expr.index, columns=slow_expr.index)\n",
    "    print(\"Building overlap graphs...\")\n",
    "    # Create overlap graph and its lcc.\n",
    "    G_fast_sub = build_pos_corr_graph_faster(fast_corr, min_r=corr_tuple[0])\n",
    "    G_slow_sub = build_pos_corr_graph_faster(slow_corr, min_r=corr_tuple[1])\n",
    "    G_overlap = intersection_graph_with_weights(G_fast_sub, G_slow_sub)\n",
    "    # components = sorted(nx.connected_components(G_overlap), key=len, reverse=True)\n",
    "    # G_overlap_lcc_largest = G_overlap.subgraph(components[0]).copy()\n",
    "    # calculate node meta df\n",
    "    overlap_node_meta_df = create_node_meta_df_for_gnn_kfold(G_overlap, fast_df, slow_df)\n",
    "    overlap_node_meta_df = add_tcr_biochem_features_kfold(overlap_node_meta_df, seq_col=\"id\")\n",
    "    return G_overlap, overlap_node_meta_df\n",
    "\n",
    "def build_orig_dyn_feat_dict_for_filtered(\n",
    "    clonotype_df: pd.DataFrame,\n",
    "    node_idx_map: dict,\n",
    "    all_samples: list\n",
    ") -> (dict, dict):\n",
    "\n",
    "    dyn_feat_dict = {}\n",
    "    presence_mask_dict = {}\n",
    "    num_nodes = len(node_idx_map)\n",
    "\n",
    "    # Keep only rows corresponding to TCRs that are nodes in our graph.\n",
    "    relevant_tcrs = list(node_idx_map.keys())\n",
    "    filtered_df = clonotype_df[clonotype_df['aaSeqCDR3'].isin(relevant_tcrs)].copy() # <-- CHANGED\n",
    "    # Convert TCR strings to their integer indices all at once.\n",
    "    filtered_df['node_index'] = filtered_df['aaSeqCDR3'].map(node_idx_map) # <-- CHANGED\n",
    "    # Calculate logFraction for all relevant TCRs at once.\n",
    "    filtered_df['logFraction'] = np.log10(filtered_df['readFraction'] + 1e-10)\n",
    "    # Group the pre-processed DataFrame for fast access.\n",
    "    grouped = filtered_df.groupby('Sample_ID') # <-- CHANGED\n",
    "    \n",
    "    for sample_id in all_samples:\n",
    "        dyn_features = torch.zeros(num_nodes, 1, dtype=torch.float32) \n",
    "        try:\n",
    "            sample_df = grouped.get_group(sample_id)     \n",
    "            indices = torch.tensor(sample_df['node_index'].values, dtype=torch.long)\n",
    "            values = torch.tensor(sample_df['logFraction'].values, dtype=torch.float32)\n",
    "            \n",
    "            dyn_features[indices, 0] = values\n",
    "            presence_mask_dict[sample_id] = indices\n",
    "        except KeyError:\n",
    "            presence_mask_dict[sample_id] = torch.tensor([], dtype=torch.long)\n",
    "            \n",
    "        dyn_feat_dict[sample_id] = dyn_features \n",
    "    return dyn_feat_dict, presence_mask_dict\n",
    "\n",
    "def collate_fn_filtered(batch):\n",
    "    # 1. Unpack the batch. `mask_indices_list` now contains tensors of different lengths.\n",
    "    g_list, dyn_list, mask_indices_list, label_list = zip(*batch)\n",
    "    # 2. Handle the graph, dynamic features, and labels (your existing logic is correct).\n",
    "    g = g_list[0]  # All graphs are the same object.\n",
    "    dyn = torch.stack(dyn_list, dim=0)      # Shape: [batch_size, num_nodes, dyn_dim]\n",
    "    labels = torch.tensor(label_list)        # Shape: [batch_size]\n",
    "    # 3. --- THE FIX: Build the batch-level binary mask ---\n",
    "    batch_size = dyn.shape[0]\n",
    "    num_nodes = dyn.shape[1]\n",
    "    # Create an empty \"switchboard\" of all zeros.\n",
    "    mask = torch.zeros(batch_size, num_nodes, dtype=torch.float32) # Using float32 for broad compatibility.\n",
    "    # Iterate through each sample in the batch to flip the right switches.\n",
    "    for i, indices in enumerate(mask_indices_list):\n",
    "        # `i` is the sample's row in the batch.\n",
    "        # `indices` is the list of TCR node indices to turn on for that sample.\n",
    "        if indices.numel() > 0:  # Check if there are any present nodes.\n",
    "            mask[i, indices] = 1.0\n",
    "    return g, dyn, mask, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70d5f84",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ce912893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Number of devices: 2\n",
      "Using GPU device 0: NVIDIA A100 80GB PCIe\n",
      "\n",
      "========== RUNNING BAKE-OFF FOR OUTER SEED: 0 ==========\n",
      "\n",
      "---  Testing Recipe: Seed_0_Publicity_5_Corr_(0.59, 0.61) ---\n",
      "Building graph...\n",
      "Calculating corrolation...\n",
      "Building overlap graphs...\n",
      "---  Error processing sequence: 'CASSL_ETQYF' | Message: non-standard AA ---\n",
      "---  Error processing sequence: 'CA_HF' | Message: non-standard AA ---\n",
      "Imputing NaNs in biochemical features... (20 missing values detected)\n",
      "NetworkX graph created. Nodes: 5856, Edges: 7664\n",
      "Scaling...\n",
      "Starting k-fold...\n",
      "Epoch 00 | Loss 3.6878 | Val bal-acc 0.506 | Val F1 0.514\n",
      "Epoch 20 | Loss 1.2134 | Val bal-acc 0.528 | Val F1 0.541\n",
      "Epoch 40 | Loss 0.8344 | Val bal-acc 0.611 | Val F1 0.588\n",
      "Epoch 60 | Loss 0.8045 | Val bal-acc 0.611 | Val F1 0.588\n",
      "Early stopping at epoch 78.\n",
      "Fold 0: val bal-acc=0.611, F1=0.588\n",
      "Epoch 00 | Loss 4.8107 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.1186 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 40 | Loss 0.7496 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 60 | Loss 0.7941 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Early stopping at epoch 76.\n",
      "Fold 1: val bal-acc=0.624, F1=0.545\n",
      "Epoch 00 | Loss 3.6090 | Val bal-acc 0.588 | Val F1 0.588\n",
      "Epoch 20 | Loss 1.5368 | Val bal-acc 0.518 | Val F1 0.516\n",
      "Epoch 40 | Loss 1.3604 | Val bal-acc 0.403 | Val F1 0.125\n",
      "Epoch 60 | Loss 1.6150 | Val bal-acc 0.374 | Val F1 0.118\n",
      "Early stopping at epoch 61.\n",
      "Fold 2: val bal-acc=0.597, F1=0.581\n",
      "Epoch 00 | Loss 6.6905 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.3749 | Val bal-acc 0.424 | Val F1 0.222\n",
      "Epoch 40 | Loss 1.2438 | Val bal-acc 0.541 | Val F1 0.286\n",
      "Epoch 60 | Loss 1.0677 | Val bal-acc 0.512 | Val F1 0.267\n",
      "Early stopping at epoch 70.\n",
      "Fold 3: val bal-acc=0.641, F1=0.500\n",
      "Epoch 00 | Loss 3.2666 | Val bal-acc 0.556 | Val F1 0.500\n",
      "Epoch 20 | Loss 1.5056 | Val bal-acc 0.444 | Val F1 0.000\n",
      "Epoch 40 | Loss 1.0340 | Val bal-acc 0.444 | Val F1 0.457\n",
      "Epoch 60 | Loss 1.4003 | Val bal-acc 0.444 | Val F1 0.457\n",
      "Early stopping at epoch 61.\n",
      "Fold 4: val bal-acc=0.611, F1=0.538\n",
      "\n",
      "Seed_0_Publicity_5_Corr_(0.59, 0.61): KFold mean bal-acc 0.617  0.015\n",
      "Seed_0_Publicity_5_Corr_(0.59, 0.61): KFold mean F1      0.551  0.032\n",
      "\n",
      "---  Testing Recipe: Seed_0_Publicity_5_Corr_(0.65, 0.67) ---\n",
      "Building graph...\n",
      "Calculating corrolation...\n",
      "Building overlap graphs...\n",
      "---  Error processing sequence: 'CASSL_ETQYF' | Message: non-standard AA ---\n",
      "Imputing NaNs in biochemical features... (10 missing values detected)\n",
      "NetworkX graph created. Nodes: 3361, Edges: 3952\n",
      "Scaling...\n",
      "Starting k-fold...\n",
      "Epoch 00 | Loss 3.0913 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 20 | Loss 1.3939 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 40 | Loss 1.2464 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 60 | Loss 1.1617 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Early stopping at epoch 62.\n",
      "Fold 0: val bal-acc=0.589, F1=0.562\n",
      "Epoch 00 | Loss 4.0290 | Val bal-acc 0.559 | Val F1 0.571\n",
      "Epoch 20 | Loss 1.7299 | Val bal-acc 0.382 | Val F1 0.000\n",
      "Epoch 40 | Loss 1.5827 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 60 | Loss 1.6891 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Early stopping at epoch 60.\n",
      "Fold 1: val bal-acc=0.559, F1=0.571\n",
      "Epoch 00 | Loss 5.4984 | Val bal-acc 0.329 | Val F1 0.375\n",
      "Epoch 20 | Loss 1.4134 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 40 | Loss 1.5664 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 60 | Loss 1.3559 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Early stopping at epoch 74.\n",
      "Fold 2: val bal-acc=0.529, F1=0.556\n",
      "Epoch 00 | Loss 4.3994 | Val bal-acc 0.571 | Val F1 0.308\n",
      "Epoch 20 | Loss 1.7470 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 40 | Loss 1.1554 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 60 | Loss 1.1137 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Early stopping at epoch 60.\n",
      "Fold 3: val bal-acc=0.571, F1=0.308\n",
      "Epoch 00 | Loss 4.3292 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 20 | Loss 1.4341 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 40 | Loss 1.3547 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 60 | Loss 1.0246 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Early stopping at epoch 62.\n",
      "Fold 4: val bal-acc=0.556, F1=0.200\n",
      "\n",
      "Seed_0_Publicity_5_Corr_(0.65, 0.67): KFold mean bal-acc 0.561  0.020\n",
      "Seed_0_Publicity_5_Corr_(0.65, 0.67): KFold mean F1      0.439  0.155\n",
      "\n",
      "---  Testing Recipe: Seed_0_Publicity_5_Corr_(0.74, 0.76) ---\n",
      "Building graph...\n",
      "Calculating corrolation...\n",
      "Building overlap graphs...\n",
      "---  Error processing sequence: 'CASSL_ETQYF' | Message: non-standard AA ---\n",
      "Imputing NaNs in biochemical features... (10 missing values detected)\n",
      "NetworkX graph created. Nodes: 1289, Edges: 1411\n",
      "Scaling...\n",
      "Starting k-fold...\n",
      "Epoch 00 | Loss 3.7070 | Val bal-acc 0.333 | Val F1 0.333\n",
      "Epoch 20 | Loss 1.3613 | Val bal-acc 0.433 | Val F1 0.222\n",
      "Epoch 40 | Loss 0.9527 | Val bal-acc 0.511 | Val F1 0.333\n",
      "Epoch 60 | Loss 1.2380 | Val bal-acc 0.511 | Val F1 0.333\n",
      "Early stopping at epoch 64.\n",
      "Fold 0: val bal-acc=0.733, F1=0.667\n",
      "Epoch 00 | Loss 5.4297 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.3905 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 40 | Loss 1.2462 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 60 | Loss 1.2460 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Early stopping at epoch 60.\n",
      "Fold 1: val bal-acc=0.500, F1=0.541\n",
      "Epoch 00 | Loss 2.9704 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 20 | Loss 0.8899 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 40 | Loss 0.7378 | Val bal-acc 0.550 | Val F1 0.182\n",
      "Epoch 60 | Loss 0.8130 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Early stopping at epoch 77.\n",
      "Fold 2: val bal-acc=0.626, F1=0.600\n",
      "Epoch 00 | Loss 3.0475 | Val bal-acc 0.529 | Val F1 0.556\n",
      "Epoch 20 | Loss 0.9741 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 40 | Loss 1.0515 | Val bal-acc 0.529 | Val F1 0.556\n",
      "Epoch 60 | Loss 1.1590 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Early stopping at epoch 60.\n",
      "Fold 3: val bal-acc=0.529, F1=0.556\n",
      "Epoch 00 | Loss 4.4051 | Val bal-acc 0.500 | Val F1 0.500\n",
      "Epoch 20 | Loss 1.0660 | Val bal-acc 0.583 | Val F1 0.545\n",
      "Epoch 40 | Loss 0.9118 | Val bal-acc 0.639 | Val F1 0.560\n",
      "Epoch 60 | Loss 0.8303 | Val bal-acc 0.583 | Val F1 0.400\n",
      "Epoch 80 | Loss 0.7247 | Val bal-acc 0.611 | Val F1 0.429\n",
      "Epoch 100 | Loss 0.7307 | Val bal-acc 0.611 | Val F1 0.429\n",
      "Early stopping at epoch 105.\n",
      "Fold 4: val bal-acc=0.778, F1=0.700\n",
      "\n",
      "Seed_0_Publicity_5_Corr_(0.74, 0.76): KFold mean bal-acc 0.633  0.109\n",
      "Seed_0_Publicity_5_Corr_(0.74, 0.76): KFold mean F1      0.613  0.062\n",
      "\n",
      "---  Testing Recipe: Seed_0_Publicity_4_Corr_(0.62, 0.64) ---\n",
      "Building graph...\n",
      "Calculating corrolation...\n",
      "Building overlap graphs...\n",
      "---  Error processing sequence: 'CASSL_ETQYF' | Message: non-standard AA ---\n",
      "---  Error processing sequence: 'CA_HF' | Message: non-standard AA ---\n",
      "---  Error processing sequence: 'CASSL_DEQYF' | Message: non-standard AA ---\n",
      "---  Error processing sequence: 'CASSS_ETQYF' | Message: non-standard AA ---\n",
      "---  Error processing sequence: 'CASSQG_YEQYF' | Message: non-standard AA ---\n",
      "Imputing NaNs in biochemical features... (50 missing values detected)\n",
      "NetworkX graph created. Nodes: 9994, Edges: 14475\n",
      "Scaling...\n",
      "Starting k-fold...\n",
      "Epoch 00 | Loss 6.1411 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 20 | Loss 1.6952 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 40 | Loss 1.9343 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 60 | Loss 1.4112 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Early stopping at epoch 60.\n",
      "Fold 0: val bal-acc=0.500, F1=0.526\n",
      "Epoch 00 | Loss 5.0635 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.4722 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 40 | Loss 1.2901 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 60 | Loss 1.2613 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Early stopping at epoch 71.\n",
      "Fold 1: val bal-acc=0.644, F1=0.583\n",
      "Epoch 00 | Loss 6.4369 | Val bal-acc 0.479 | Val F1 0.514\n",
      "Epoch 20 | Loss 1.7027 | Val bal-acc 0.512 | Val F1 0.267\n",
      "Epoch 40 | Loss 0.8169 | Val bal-acc 0.412 | Val F1 0.000\n",
      "Epoch 60 | Loss 0.8240 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 80 | Loss 0.7158 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Early stopping at epoch 81.\n",
      "Fold 2: val bal-acc=0.585, F1=0.538\n",
      "Epoch 00 | Loss 5.8691 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 2.0491 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 40 | Loss 1.5606 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 60 | Loss 1.8803 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Early stopping at epoch 60.\n",
      "Fold 3: val bal-acc=0.500, F1=0.541\n",
      "Epoch 00 | Loss 3.9557 | Val bal-acc 0.500 | Val F1 0.485\n",
      "Epoch 20 | Loss 1.7441 | Val bal-acc 0.500 | Val F1 0.500\n",
      "Epoch 40 | Loss 0.8557 | Val bal-acc 0.500 | Val F1 0.485\n",
      "Epoch 60 | Loss 1.0510 | Val bal-acc 0.556 | Val F1 0.516\n",
      "Early stopping at epoch 70.\n",
      "Fold 4: val bal-acc=0.611, F1=0.522\n",
      "\n",
      "Seed_0_Publicity_4_Corr_(0.62, 0.64): KFold mean bal-acc 0.568  0.059\n",
      "Seed_0_Publicity_4_Corr_(0.62, 0.64): KFold mean F1      0.542  0.022\n",
      "\n",
      "---  Testing Recipe: Seed_0_Publicity_4_Corr_(0.67, 0.69) ---\n",
      "Building graph...\n",
      "Calculating corrolation...\n",
      "Building overlap graphs...\n",
      "---  Error processing sequence: 'CASSL_ETQYF' | Message: non-standard AA ---\n",
      "---  Error processing sequence: 'CASSS_ETQYF' | Message: non-standard AA ---\n",
      "---  Error processing sequence: 'CASSQG_YEQYF' | Message: non-standard AA ---\n",
      "Imputing NaNs in biochemical features... (30 missing values detected)\n",
      "NetworkX graph created. Nodes: 6382, Edges: 8384\n",
      "Scaling...\n",
      "Starting k-fold...\n",
      "Epoch 00 | Loss 5.3908 | Val bal-acc 0.433 | Val F1 0.438\n",
      "Epoch 20 | Loss 2.1616 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 40 | Loss 1.6579 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 60 | Loss 1.4240 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Early stopping at epoch 61.\n",
      "Fold 0: val bal-acc=0.550, F1=0.182\n",
      "Epoch 00 | Loss 5.8561 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 20 | Loss 1.8557 | Val bal-acc 0.462 | Val F1 0.143\n",
      "Epoch 40 | Loss 1.1059 | Val bal-acc 0.491 | Val F1 0.154\n",
      "Epoch 60 | Loss 1.3377 | Val bal-acc 0.491 | Val F1 0.154\n",
      "Early stopping at epoch 66.\n",
      "Fold 1: val bal-acc=0.588, F1=0.588\n",
      "Epoch 00 | Loss 4.6599 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.5738 | Val bal-acc 0.406 | Val F1 0.333\n",
      "Epoch 40 | Loss 1.2027 | Val bal-acc 0.344 | Val F1 0.111\n",
      "Epoch 60 | Loss 1.0067 | Val bal-acc 0.344 | Val F1 0.111\n",
      "Early stopping at epoch 66.\n",
      "Fold 2: val bal-acc=0.541, F1=0.286\n",
      "Epoch 00 | Loss 3.8753 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 2.1256 | Val bal-acc 0.438 | Val F1 0.452\n",
      "Epoch 40 | Loss 0.9446 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 60 | Loss 0.9853 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Early stopping at epoch 73.\n",
      "Fold 3: val bal-acc=0.559, F1=0.571\n",
      "Epoch 00 | Loss 4.8338 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 20 | Loss 1.5755 | Val bal-acc 0.556 | Val F1 0.421\n",
      "Epoch 40 | Loss 1.2764 | Val bal-acc 0.611 | Val F1 0.538\n",
      "Epoch 60 | Loss 1.3593 | Val bal-acc 0.472 | Val F1 0.316\n",
      "Epoch 80 | Loss 1.6804 | Val bal-acc 0.472 | Val F1 0.316\n",
      "Early stopping at epoch 93.\n",
      "Fold 4: val bal-acc=0.611, F1=0.538\n",
      "\n",
      "Seed_0_Publicity_4_Corr_(0.67, 0.69): KFold mean bal-acc 0.570  0.026\n",
      "Seed_0_Publicity_4_Corr_(0.67, 0.69): KFold mean F1      0.433  0.167\n",
      "\n",
      "---  Testing Recipe: Seed_0_Publicity_4_Corr_(0.75, 0.77) ---\n",
      "Building graph...\n",
      "Calculating corrolation...\n",
      "Building overlap graphs...\n",
      "---  Error processing sequence: 'CASSL_ETQYF' | Message: non-standard AA ---\n",
      "---  Error processing sequence: 'CASSQG_YEQYF' | Message: non-standard AA ---\n",
      "Imputing NaNs in biochemical features... (20 missing values detected)\n",
      "NetworkX graph created. Nodes: 2640, Edges: 3341\n",
      "Scaling...\n",
      "Starting k-fold...\n",
      "Epoch 00 | Loss 4.8359 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 20 | Loss 1.0421 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 40 | Loss 0.8831 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 60 | Loss 1.0737 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Early stopping at epoch 70.\n",
      "Fold 0: val bal-acc=0.572, F1=0.308\n",
      "Epoch 00 | Loss 3.9506 | Val bal-acc 0.447 | Val F1 0.429\n",
      "Epoch 20 | Loss 1.4903 | Val bal-acc 0.518 | Val F1 0.516\n",
      "Epoch 40 | Loss 1.3349 | Val bal-acc 0.612 | Val F1 0.471\n",
      "Epoch 60 | Loss 1.0860 | Val bal-acc 0.641 | Val F1 0.500\n",
      "Early stopping at epoch 74.\n",
      "Fold 1: val bal-acc=0.647, F1=0.625\n",
      "Epoch 00 | Loss 3.6145 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.5474 | Val bal-acc 0.559 | Val F1 0.571\n",
      "Epoch 40 | Loss 1.0034 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 60 | Loss 0.9000 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Early stopping at epoch 79.\n",
      "Fold 2: val bal-acc=0.559, F1=0.571\n",
      "Epoch 00 | Loss 3.7013 | Val bal-acc 0.518 | Val F1 0.516\n",
      "Epoch 20 | Loss 1.3316 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 40 | Loss 0.9235 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 60 | Loss 0.8911 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Early stopping at epoch 78.\n",
      "Fold 3: val bal-acc=0.703, F1=0.636\n",
      "Epoch 00 | Loss 2.4714 | Val bal-acc 0.417 | Val F1 0.370\n",
      "Epoch 20 | Loss 1.3749 | Val bal-acc 0.444 | Val F1 0.457\n",
      "Epoch 40 | Loss 0.9617 | Val bal-acc 0.500 | Val F1 0.500\n",
      "Epoch 60 | Loss 0.9788 | Val bal-acc 0.500 | Val F1 0.500\n",
      "Early stopping at epoch 70.\n",
      "Fold 4: val bal-acc=0.556, F1=0.375\n",
      "\n",
      "Seed_0_Publicity_4_Corr_(0.75, 0.77): KFold mean bal-acc 0.607  0.058\n",
      "Seed_0_Publicity_4_Corr_(0.75, 0.77): KFold mean F1      0.503  0.136\n",
      "\n",
      "---  Testing Recipe: Seed_0_Publicity_3_Corr_(0.76, 0.78) ---\n",
      "Building graph...\n",
      "Calculating corrolation...\n",
      "Building overlap graphs...\n",
      "---  Error processing sequence: 'CASSL_ETQYF' | Message: non-standard AA ---\n",
      "---  Error processing sequence: 'CASSS_YEQYF' | Message: non-standard AA ---\n",
      "---  Error processing sequence: 'CASSQ_ETQYF' | Message: non-standard AA ---\n",
      "---  Error processing sequence: 'CASSL_YGYTF' | Message: non-standard AA ---\n",
      "---  Error processing sequence: 'CASSQG_YEQYF' | Message: non-standard AA ---\n",
      "---  Error processing sequence: 'CAS_HF' | Message: non-standard AA ---\n",
      "Imputing NaNs in biochemical features... (60 missing values detected)\n",
      "NetworkX graph created. Nodes: 7856, Edges: 10954\n",
      "Scaling...\n",
      "Starting k-fold...\n",
      "Epoch 00 | Loss 3.9256 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 20 | Loss 1.3406 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 40 | Loss 0.9757 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 60 | Loss 1.1881 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Early stopping at epoch 67.\n",
      "Fold 0: val bal-acc=0.556, F1=0.556\n",
      "Epoch 00 | Loss 7.6345 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.1314 | Val bal-acc 0.741 | Val F1 0.667\n",
      "Epoch 40 | Loss 1.0673 | Val bal-acc 0.594 | Val F1 0.522\n",
      "Epoch 60 | Loss 1.0166 | Val bal-acc 0.468 | Val F1 0.467\n",
      "Epoch 80 | Loss 0.9005 | Val bal-acc 0.438 | Val F1 0.452\n",
      "Early stopping at epoch 80.\n",
      "Fold 1: val bal-acc=0.741, F1=0.667\n",
      "Epoch 00 | Loss 6.4044 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 20 | Loss 2.7323 | Val bal-acc 0.438 | Val F1 0.452\n",
      "Epoch 40 | Loss 1.6481 | Val bal-acc 0.459 | Val F1 0.485\n",
      "Epoch 60 | Loss 1.3883 | Val bal-acc 0.459 | Val F1 0.485\n",
      "Early stopping at epoch 75.\n",
      "Fold 2: val bal-acc=0.509, F1=0.529\n",
      "Epoch 00 | Loss 3.7287 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.5778 | Val bal-acc 0.612 | Val F1 0.471\n",
      "Epoch 40 | Loss 1.0967 | Val bal-acc 0.615 | Val F1 0.560\n",
      "Epoch 60 | Loss 1.1987 | Val bal-acc 0.556 | Val F1 0.519\n",
      "Epoch 80 | Loss 0.9258 | Val bal-acc 0.556 | Val F1 0.519\n",
      "Early stopping at epoch 91.\n",
      "Fold 3: val bal-acc=0.653, F1=0.571\n",
      "Epoch 00 | Loss 4.9263 | Val bal-acc 0.500 | Val F1 0.500\n",
      "Epoch 20 | Loss 1.9382 | Val bal-acc 0.472 | Val F1 0.471\n",
      "Epoch 40 | Loss 1.0809 | Val bal-acc 0.389 | Val F1 0.273\n",
      "Epoch 60 | Loss 0.9331 | Val bal-acc 0.389 | Val F1 0.273\n",
      "Early stopping at epoch 73.\n",
      "Fold 4: val bal-acc=0.611, F1=0.562\n",
      "\n",
      "Seed_0_Publicity_3_Corr_(0.76, 0.78): KFold mean bal-acc 0.614  0.080\n",
      "Seed_0_Publicity_3_Corr_(0.76, 0.78): KFold mean F1      0.577  0.047\n",
      "\n",
      "---  Testing Recipe: Seed_0_Publicity_3_Corr_(0.82, 0.84) ---\n",
      "Building graph...\n",
      "Calculating corrolation...\n",
      "Building overlap graphs...\n",
      "---  Error processing sequence: 'CASSL_ETQYF' | Message: non-standard AA ---\n",
      "---  Error processing sequence: 'CASSL_YGYTF' | Message: non-standard AA ---\n",
      "---  Error processing sequence: 'CASSQG_YEQYF' | Message: non-standard AA ---\n",
      "---  Error processing sequence: 'CASSQ_ETQYF' | Message: non-standard AA ---\n",
      "Imputing NaNs in biochemical features... (40 missing values detected)\n",
      "NetworkX graph created. Nodes: 3528, Edges: 4974\n",
      "Scaling...\n",
      "Starting k-fold...\n",
      "Epoch 00 | Loss 6.4679 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 20 | Loss 1.1193 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 40 | Loss 1.2561 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 60 | Loss 1.1751 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Early stopping at epoch 64.\n",
      "Fold 0: val bal-acc=0.611, F1=0.500\n",
      "Epoch 00 | Loss 4.2366 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 20 | Loss 0.9679 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 40 | Loss 0.8864 | Val bal-acc 0.353 | Val F1 0.000\n",
      "Epoch 60 | Loss 0.7787 | Val bal-acc 0.412 | Val F1 0.000\n",
      "Epoch 80 | Loss 0.9302 | Val bal-acc 0.412 | Val F1 0.000\n",
      "Early stopping at epoch 88.\n",
      "Fold 1: val bal-acc=0.544, F1=0.455\n",
      "Epoch 00 | Loss 4.9962 | Val bal-acc 0.441 | Val F1 0.000\n",
      "Epoch 20 | Loss 1.2691 | Val bal-acc 0.471 | Val F1 0.000\n",
      "Epoch 40 | Loss 1.0580 | Val bal-acc 0.471 | Val F1 0.000\n",
      "Epoch 60 | Loss 1.0717 | Val bal-acc 0.471 | Val F1 0.000\n",
      "Early stopping at epoch 71.\n",
      "Fold 2: val bal-acc=0.653, F1=0.571\n",
      "Epoch 00 | Loss 5.8351 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.4125 | Val bal-acc 0.521 | Val F1 0.167\n",
      "Epoch 40 | Loss 0.9204 | Val bal-acc 0.385 | Val F1 0.273\n",
      "Epoch 60 | Loss 0.7208 | Val bal-acc 0.406 | Val F1 0.333\n",
      "Early stopping at epoch 78.\n",
      "Fold 3: val bal-acc=0.621, F1=0.429\n",
      "Epoch 00 | Loss 4.2363 | Val bal-acc 0.500 | Val F1 0.500\n",
      "Epoch 20 | Loss 1.4218 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 40 | Loss 0.9278 | Val bal-acc 0.500 | Val F1 0.500\n",
      "Epoch 60 | Loss 0.8305 | Val bal-acc 0.500 | Val F1 0.500\n",
      "Early stopping at epoch 78.\n",
      "Fold 4: val bal-acc=0.583, F1=0.444\n",
      "\n",
      "Seed_0_Publicity_3_Corr_(0.82, 0.84): KFold mean bal-acc 0.602  0.037\n",
      "Seed_0_Publicity_3_Corr_(0.82, 0.84): KFold mean F1      0.480  0.052\n",
      "\n",
      "---  Testing Recipe: Seed_0_Publicity_3_Corr_(0.87, 0.89) ---\n",
      "Building graph...\n",
      "Calculating corrolation...\n",
      "Building overlap graphs...\n",
      "---  Error processing sequence: 'CASSL_ETQYF' | Message: non-standard AA ---\n",
      "Imputing NaNs in biochemical features... (10 missing values detected)\n",
      "NetworkX graph created. Nodes: 1455, Edges: 2274\n",
      "Scaling...\n",
      "Starting k-fold...\n",
      "Epoch 00 | Loss 3.1773 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 20 | Loss 1.7067 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 40 | Loss 1.1497 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 60 | Loss 1.1512 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Early stopping at epoch 60.\n",
      "Fold 0: val bal-acc=0.500, F1=0.526\n",
      "Epoch 00 | Loss 4.1309 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.1129 | Val bal-acc 0.529 | Val F1 0.556\n",
      "Epoch 40 | Loss 0.8929 | Val bal-acc 0.479 | Val F1 0.514\n",
      "Epoch 60 | Loss 0.7683 | Val bal-acc 0.538 | Val F1 0.545\n",
      "Epoch 80 | Loss 0.8146 | Val bal-acc 0.538 | Val F1 0.545\n",
      "Early stopping at epoch 91.\n",
      "Fold 1: val bal-acc=0.732, F1=0.667\n",
      "Epoch 00 | Loss 4.0088 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 20 | Loss 1.4543 | Val bal-acc 0.450 | Val F1 0.500\n",
      "Epoch 40 | Loss 1.2729 | Val bal-acc 0.309 | Val F1 0.333\n",
      "Epoch 60 | Loss 1.2571 | Val bal-acc 0.309 | Val F1 0.333\n",
      "Early stopping at epoch 60.\n",
      "Fold 2: val bal-acc=0.500, F1=0.000\n",
      "Epoch 00 | Loss 3.4998 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.4015 | Val bal-acc 0.450 | Val F1 0.500\n",
      "Epoch 40 | Loss 1.1236 | Val bal-acc 0.300 | Val F1 0.364\n",
      "Epoch 60 | Loss 1.2155 | Val bal-acc 0.300 | Val F1 0.364\n",
      "Early stopping at epoch 63.\n",
      "Fold 3: val bal-acc=0.671, F1=0.533\n",
      "Epoch 00 | Loss 3.0259 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 20 | Loss 1.7196 | Val bal-acc 0.500 | Val F1 0.333\n",
      "Epoch 40 | Loss 0.9698 | Val bal-acc 0.472 | Val F1 0.364\n",
      "Epoch 60 | Loss 1.1928 | Val bal-acc 0.472 | Val F1 0.400\n",
      "Early stopping at epoch 63.\n",
      "Fold 4: val bal-acc=0.556, F1=0.529\n",
      "\n",
      "Seed_0_Publicity_3_Corr_(0.87, 0.89): KFold mean bal-acc 0.592  0.094\n",
      "Seed_0_Publicity_3_Corr_(0.87, 0.89): KFold mean F1      0.451  0.232\n",
      "\n",
      "========== RUNNING BAKE-OFF FOR OUTER SEED: 42 ==========\n",
      "\n",
      "---  Testing Recipe: Seed_42_Publicity_5_Corr_(0.59, 0.61) ---\n",
      "Building graph...\n",
      "Calculating corrolation...\n",
      "Building overlap graphs...\n",
      "NetworkX graph created. Nodes: 5310, Edges: 6909\n",
      "Scaling...\n",
      "Starting k-fold...\n",
      "Epoch 00 | Loss 4.3016 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 20 | Loss 1.5191 | Val bal-acc 0.472 | Val F1 0.000\n",
      "Epoch 40 | Loss 1.5191 | Val bal-acc 0.472 | Val F1 0.000\n",
      "Epoch 60 | Loss 1.5002 | Val bal-acc 0.472 | Val F1 0.000\n",
      "Early stopping at epoch 60.\n",
      "Fold 0: val bal-acc=0.500, F1=0.000\n",
      "Epoch 00 | Loss 3.4013 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 20 | Loss 1.5750 | Val bal-acc 0.500 | Val F1 0.500\n",
      "Epoch 40 | Loss 1.2153 | Val bal-acc 0.500 | Val F1 0.500\n",
      "Epoch 60 | Loss 1.0286 | Val bal-acc 0.500 | Val F1 0.500\n",
      "Early stopping at epoch 67.\n",
      "Fold 1: val bal-acc=0.611, F1=0.552\n",
      "Epoch 00 | Loss 3.6177 | Val bal-acc 0.382 | Val F1 0.000\n",
      "Epoch 20 | Loss 1.4239 | Val bal-acc 0.388 | Val F1 0.400\n",
      "Epoch 40 | Loss 0.9061 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 60 | Loss 1.0799 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Early stopping at epoch 72.\n",
      "Fold 2: val bal-acc=0.621, F1=0.429\n",
      "Epoch 00 | Loss 4.1253 | Val bal-acc 0.429 | Val F1 0.471\n",
      "Epoch 20 | Loss 1.2229 | Val bal-acc 0.559 | Val F1 0.571\n",
      "Epoch 40 | Loss 0.8575 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 60 | Loss 1.1109 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Early stopping at epoch 71.\n",
      "Fold 3: val bal-acc=0.724, F1=0.667\n",
      "Epoch 00 | Loss 5.4979 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.2285 | Val bal-acc 0.471 | Val F1 0.000\n",
      "Epoch 40 | Loss 1.0930 | Val bal-acc 0.512 | Val F1 0.267\n",
      "Epoch 60 | Loss 0.9710 | Val bal-acc 0.512 | Val F1 0.267\n",
      "Early stopping at epoch 77.\n",
      "Fold 4: val bal-acc=0.568, F1=0.562\n",
      "\n",
      "Seed_42_Publicity_5_Corr_(0.59, 0.61): KFold mean bal-acc 0.605  0.073\n",
      "Seed_42_Publicity_5_Corr_(0.59, 0.61): KFold mean F1      0.442  0.233\n",
      "\n",
      "---  Testing Recipe: Seed_42_Publicity_5_Corr_(0.65, 0.67) ---\n",
      "Building graph...\n",
      "Calculating corrolation...\n",
      "Building overlap graphs...\n",
      "NetworkX graph created. Nodes: 3178, Edges: 3671\n",
      "Scaling...\n",
      "Starting k-fold...\n",
      "Epoch 00 | Loss 4.4016 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 20 | Loss 1.2999 | Val bal-acc 0.400 | Val F1 0.444\n",
      "Epoch 40 | Loss 1.5557 | Val bal-acc 0.428 | Val F1 0.457\n",
      "Epoch 60 | Loss 1.6541 | Val bal-acc 0.478 | Val F1 0.500\n",
      "Early stopping at epoch 62.\n",
      "Fold 0: val bal-acc=0.583, F1=0.476\n",
      "Epoch 00 | Loss 5.7077 | Val bal-acc 0.500 | Val F1 0.500\n",
      "Epoch 20 | Loss 1.2817 | Val bal-acc 0.500 | Val F1 0.500\n",
      "Epoch 40 | Loss 1.0979 | Val bal-acc 0.444 | Val F1 0.457\n",
      "Epoch 60 | Loss 1.0867 | Val bal-acc 0.444 | Val F1 0.457\n",
      "Early stopping at epoch 76.\n",
      "Fold 1: val bal-acc=0.583, F1=0.545\n",
      "Epoch 00 | Loss 3.0577 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 20 | Loss 1.6935 | Val bal-acc 0.588 | Val F1 0.588\n",
      "Epoch 40 | Loss 1.2608 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 60 | Loss 1.2247 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 80 | Loss 1.2656 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Early stopping at epoch 81.\n",
      "Fold 2: val bal-acc=0.626, F1=0.600\n",
      "Epoch 00 | Loss 2.8232 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 20 | Loss 1.4524 | Val bal-acc 0.529 | Val F1 0.556\n",
      "Epoch 40 | Loss 0.9087 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 60 | Loss 0.8525 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Early stopping at epoch 69.\n",
      "Fold 3: val bal-acc=0.588, F1=0.588\n",
      "Epoch 00 | Loss 4.0540 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.5286 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 40 | Loss 1.2903 | Val bal-acc 0.462 | Val F1 0.143\n",
      "Epoch 60 | Loss 0.9682 | Val bal-acc 0.441 | Val F1 0.000\n",
      "Early stopping at epoch 73.\n",
      "Fold 4: val bal-acc=0.694, F1=0.640\n",
      "\n",
      "Seed_42_Publicity_5_Corr_(0.65, 0.67): KFold mean bal-acc 0.615  0.043\n",
      "Seed_42_Publicity_5_Corr_(0.65, 0.67): KFold mean F1      0.570  0.056\n",
      "\n",
      "---  Testing Recipe: Seed_42_Publicity_5_Corr_(0.74, 0.76) ---\n",
      "Building graph...\n",
      "Calculating corrolation...\n",
      "Building overlap graphs...\n",
      "NetworkX graph created. Nodes: 1217, Edges: 1275\n",
      "Scaling...\n",
      "Starting k-fold...\n",
      "Epoch 00 | Loss 4.7824 | Val bal-acc 0.400 | Val F1 0.444\n",
      "Epoch 20 | Loss 1.0383 | Val bal-acc 0.294 | Val F1 0.174\n",
      "Epoch 40 | Loss 0.7894 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 60 | Loss 0.7338 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Early stopping at epoch 78.\n",
      "Fold 0: val bal-acc=0.522, F1=0.462\n",
      "Epoch 00 | Loss 3.5396 | Val bal-acc 0.500 | Val F1 0.500\n",
      "Epoch 20 | Loss 1.0069 | Val bal-acc 0.500 | Val F1 0.500\n",
      "Epoch 40 | Loss 0.9671 | Val bal-acc 0.500 | Val F1 0.500\n",
      "Epoch 60 | Loss 1.2340 | Val bal-acc 0.500 | Val F1 0.500\n",
      "Early stopping at epoch 65.\n",
      "Fold 1: val bal-acc=0.611, F1=0.562\n",
      "Epoch 00 | Loss 3.7436 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 20 | Loss 1.3403 | Val bal-acc 0.453 | Val F1 0.235\n",
      "Epoch 40 | Loss 0.8961 | Val bal-acc 0.438 | Val F1 0.452\n",
      "Epoch 60 | Loss 0.9010 | Val bal-acc 0.459 | Val F1 0.485\n",
      "Early stopping at epoch 71.\n",
      "Fold 2: val bal-acc=0.624, F1=0.545\n",
      "Epoch 00 | Loss 2.5854 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.4408 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 40 | Loss 1.1943 | Val bal-acc 0.529 | Val F1 0.556\n",
      "Epoch 60 | Loss 1.1728 | Val bal-acc 0.529 | Val F1 0.556\n",
      "Early stopping at epoch 72.\n",
      "Fold 3: val bal-acc=0.529, F1=0.556\n",
      "Epoch 00 | Loss 4.8710 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.0064 | Val bal-acc 0.438 | Val F1 0.452\n",
      "Epoch 40 | Loss 0.7878 | Val bal-acc 0.471 | Val F1 0.000\n",
      "Epoch 60 | Loss 0.7547 | Val bal-acc 0.471 | Val F1 0.000\n",
      "Early stopping at epoch 71.\n",
      "Fold 4: val bal-acc=0.553, F1=0.421\n",
      "\n",
      "Seed_42_Publicity_5_Corr_(0.74, 0.76): KFold mean bal-acc 0.568  0.042\n",
      "Seed_42_Publicity_5_Corr_(0.74, 0.76): KFold mean F1      0.509  0.057\n",
      "\n",
      "---  Testing Recipe: Seed_42_Publicity_4_Corr_(0.62, 0.64) ---\n",
      "Building graph...\n",
      "Calculating corrolation...\n",
      "Building overlap graphs...\n",
      "---  Error processing sequence: 'CA_HF' | Message: non-standard AA ---\n",
      "Imputing NaNs in biochemical features... (10 missing values detected)\n",
      "NetworkX graph created. Nodes: 9158, Edges: 13070\n",
      "Scaling...\n",
      "Starting k-fold...\n",
      "Epoch 00 | Loss 4.2452 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 20 | Loss 1.0797 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 40 | Loss 1.2024 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 60 | Loss 0.8295 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Early stopping at epoch 73.\n",
      "Fold 0: val bal-acc=0.600, F1=0.333\n",
      "Epoch 00 | Loss 7.3430 | Val bal-acc 0.528 | Val F1 0.500\n",
      "Epoch 20 | Loss 2.1170 | Val bal-acc 0.500 | Val F1 0.500\n",
      "Epoch 40 | Loss 1.9006 | Val bal-acc 0.333 | Val F1 0.364\n",
      "Epoch 60 | Loss 1.3784 | Val bal-acc 0.444 | Val F1 0.457\n",
      "Early stopping at epoch 60.\n",
      "Fold 1: val bal-acc=0.528, F1=0.500\n",
      "Epoch 00 | Loss 4.9595 | Val bal-acc 0.450 | Val F1 0.500\n",
      "Epoch 20 | Loss 1.7099 | Val bal-acc 0.424 | Val F1 0.222\n",
      "Epoch 40 | Loss 1.4743 | Val bal-acc 0.394 | Val F1 0.211\n",
      "Epoch 60 | Loss 1.2467 | Val bal-acc 0.394 | Val F1 0.211\n",
      "Early stopping at epoch 62.\n",
      "Fold 2: val bal-acc=0.632, F1=0.526\n",
      "Epoch 00 | Loss 4.4266 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.8327 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 40 | Loss 1.2187 | Val bal-acc 0.529 | Val F1 0.556\n",
      "Epoch 60 | Loss 1.1591 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Early stopping at epoch 67.\n",
      "Fold 3: val bal-acc=0.559, F1=0.571\n",
      "Epoch 00 | Loss 4.2440 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.1897 | Val bal-acc 0.612 | Val F1 0.471\n",
      "Epoch 40 | Loss 1.2367 | Val bal-acc 0.662 | Val F1 0.556\n",
      "Epoch 60 | Loss 1.2393 | Val bal-acc 0.512 | Val F1 0.267\n",
      "Epoch 80 | Loss 1.2194 | Val bal-acc 0.512 | Val F1 0.267\n",
      "Early stopping at epoch 86.\n",
      "Fold 4: val bal-acc=0.662, F1=0.556\n",
      "\n",
      "Seed_42_Publicity_4_Corr_(0.62, 0.64): KFold mean bal-acc 0.596  0.048\n",
      "Seed_42_Publicity_4_Corr_(0.62, 0.64): KFold mean F1      0.497  0.086\n",
      "\n",
      "---  Testing Recipe: Seed_42_Publicity_4_Corr_(0.67, 0.69) ---\n",
      "Building graph...\n",
      "Calculating corrolation...\n",
      "Building overlap graphs...\n",
      "NetworkX graph created. Nodes: 5995, Edges: 7648\n",
      "Scaling...\n",
      "Starting k-fold...\n",
      "Epoch 00 | Loss 6.1323 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 20 | Loss 1.8423 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 40 | Loss 0.8559 | Val bal-acc 0.667 | Val F1 0.556\n",
      "Epoch 60 | Loss 0.7853 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 80 | Loss 0.9330 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 100 | Loss 0.8910 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Early stopping at epoch 101.\n",
      "Fold 0: val bal-acc=0.739, F1=0.667\n",
      "Epoch 00 | Loss 3.5929 | Val bal-acc 0.500 | Val F1 0.500\n",
      "Epoch 20 | Loss 1.2766 | Val bal-acc 0.500 | Val F1 0.500\n",
      "Epoch 40 | Loss 1.4124 | Val bal-acc 0.500 | Val F1 0.500\n",
      "Epoch 60 | Loss 1.0393 | Val bal-acc 0.500 | Val F1 0.500\n",
      "Early stopping at epoch 63.\n",
      "Fold 1: val bal-acc=0.556, F1=0.516\n",
      "Epoch 00 | Loss 4.6910 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.4531 | Val bal-acc 0.503 | Val F1 0.333\n",
      "Epoch 40 | Loss 1.0997 | Val bal-acc 0.506 | Val F1 0.462\n",
      "Epoch 60 | Loss 1.0955 | Val bal-acc 0.476 | Val F1 0.444\n",
      "Epoch 80 | Loss 0.9869 | Val bal-acc 0.476 | Val F1 0.444\n",
      "Early stopping at epoch 83.\n",
      "Fold 2: val bal-acc=0.597, F1=0.581\n",
      "Epoch 00 | Loss 4.4082 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 20 | Loss 1.2874 | Val bal-acc 0.600 | Val F1 0.333\n",
      "Epoch 40 | Loss 0.9285 | Val bal-acc 0.541 | Val F1 0.286\n",
      "Epoch 60 | Loss 0.9693 | Val bal-acc 0.541 | Val F1 0.286\n",
      "Early stopping at epoch 72.\n",
      "Fold 3: val bal-acc=0.662, F1=0.556\n",
      "Epoch 00 | Loss 3.5533 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.1557 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 40 | Loss 0.9806 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 60 | Loss 0.9458 | Val bal-acc 0.529 | Val F1 0.556\n",
      "Early stopping at epoch 67.\n",
      "Fold 4: val bal-acc=0.653, F1=0.571\n",
      "\n",
      "Seed_42_Publicity_4_Corr_(0.67, 0.69): KFold mean bal-acc 0.641  0.062\n",
      "Seed_42_Publicity_4_Corr_(0.67, 0.69): KFold mean F1      0.578  0.049\n",
      "\n",
      "---  Testing Recipe: Seed_42_Publicity_4_Corr_(0.75, 0.77) ---\n",
      "Building graph...\n",
      "Calculating corrolation...\n",
      "Building overlap graphs...\n",
      "NetworkX graph created. Nodes: 2479, Edges: 2926\n",
      "Scaling...\n",
      "Starting k-fold...\n",
      "Epoch 00 | Loss 3.8964 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 20 | Loss 0.9326 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 40 | Loss 0.8815 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 60 | Loss 0.8398 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Early stopping at epoch 66.\n",
      "Fold 0: val bal-acc=0.728, F1=0.667\n",
      "Epoch 00 | Loss 4.2201 | Val bal-acc 0.500 | Val F1 0.500\n",
      "Epoch 20 | Loss 1.1715 | Val bal-acc 0.472 | Val F1 0.250\n",
      "Epoch 40 | Loss 0.8853 | Val bal-acc 0.556 | Val F1 0.375\n",
      "Epoch 60 | Loss 0.8825 | Val bal-acc 0.611 | Val F1 0.471\n",
      "Epoch 80 | Loss 0.8471 | Val bal-acc 0.639 | Val F1 0.526\n",
      "Epoch 100 | Loss 0.6626 | Val bal-acc 0.611 | Val F1 0.500\n",
      "Early stopping at epoch 111.\n",
      "Fold 1: val bal-acc=0.667, F1=0.556\n",
      "Epoch 00 | Loss 5.5127 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.3796 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 40 | Loss 0.8365 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 60 | Loss 0.8357 | Val bal-acc 0.568 | Val F1 0.562\n",
      "Epoch 80 | Loss 0.8981 | Val bal-acc 0.597 | Val F1 0.581\n",
      "Early stopping at epoch 88.\n",
      "Fold 2: val bal-acc=0.644, F1=0.583\n",
      "Epoch 00 | Loss 5.1962 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.4094 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 40 | Loss 1.1828 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 60 | Loss 0.8894 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Early stopping at epoch 67.\n",
      "Fold 3: val bal-acc=0.691, F1=0.588\n",
      "Epoch 00 | Loss 5.1807 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.6161 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 40 | Loss 1.1717 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 60 | Loss 1.1232 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Early stopping at epoch 66.\n",
      "Fold 4: val bal-acc=0.588, F1=0.588\n",
      "\n",
      "Seed_42_Publicity_4_Corr_(0.75, 0.77): KFold mean bal-acc 0.664  0.047\n",
      "Seed_42_Publicity_4_Corr_(0.75, 0.77): KFold mean F1      0.596  0.037\n",
      "\n",
      "---  Testing Recipe: Seed_42_Publicity_3_Corr_(0.76, 0.78) ---\n",
      "Building graph...\n",
      "Calculating corrolation...\n",
      "Building overlap graphs...\n",
      "---  Error processing sequence: 'CASSQS_YEQYF' | Message: non-standard AA ---\n",
      "---  Error processing sequence: 'CASSL_YGYTF' | Message: non-standard AA ---\n",
      "---  Error processing sequence: 'CASSS_YEQYF' | Message: non-standard AA ---\n",
      "---  Error processing sequence: 'CASSF_YEQYF' | Message: non-standard AA ---\n",
      "Imputing NaNs in biochemical features... (40 missing values detected)\n",
      "NetworkX graph created. Nodes: 7170, Edges: 10323\n",
      "Scaling...\n",
      "Starting k-fold...\n",
      "Epoch 00 | Loss 4.8727 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 20 | Loss 1.5445 | Val bal-acc 0.439 | Val F1 0.133\n",
      "Epoch 40 | Loss 1.2220 | Val bal-acc 0.422 | Val F1 0.333\n",
      "Epoch 60 | Loss 1.2756 | Val bal-acc 0.422 | Val F1 0.333\n",
      "Early stopping at epoch 61.\n",
      "Fold 0: val bal-acc=0.528, F1=0.541\n",
      "Epoch 00 | Loss 3.6435 | Val bal-acc 0.500 | Val F1 0.500\n",
      "Epoch 20 | Loss 1.7835 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 40 | Loss 1.4164 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 60 | Loss 1.2548 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Early stopping at epoch 75.\n",
      "Fold 1: val bal-acc=0.528, F1=0.500\n",
      "Epoch 00 | Loss 5.4474 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.6803 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 40 | Loss 1.7132 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 60 | Loss 1.1763 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Early stopping at epoch 63.\n",
      "Fold 2: val bal-acc=0.600, F1=0.333\n",
      "Epoch 00 | Loss 6.3569 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.4167 | Val bal-acc 0.459 | Val F1 0.485\n",
      "Epoch 40 | Loss 1.1716 | Val bal-acc 0.465 | Val F1 0.364\n",
      "Epoch 60 | Loss 0.8942 | Val bal-acc 0.494 | Val F1 0.381\n",
      "Early stopping at epoch 72.\n",
      "Fold 3: val bal-acc=0.603, F1=0.500\n",
      "Epoch 00 | Loss 4.0857 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.6417 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 40 | Loss 1.1868 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 60 | Loss 1.0575 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 80 | Loss 1.0311 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Early stopping at epoch 82.\n",
      "Fold 4: val bal-acc=0.626, F1=0.600\n",
      "\n",
      "Seed_42_Publicity_3_Corr_(0.76, 0.78): KFold mean bal-acc 0.577  0.041\n",
      "Seed_42_Publicity_3_Corr_(0.76, 0.78): KFold mean F1      0.495  0.089\n",
      "\n",
      "---  Testing Recipe: Seed_42_Publicity_3_Corr_(0.82, 0.84) ---\n",
      "Building graph...\n",
      "Calculating corrolation...\n",
      "Building overlap graphs...\n",
      "NetworkX graph created. Nodes: 3293, Edges: 4770\n",
      "Scaling...\n",
      "Starting k-fold...\n",
      "Epoch 00 | Loss 3.6546 | Val bal-acc 0.517 | Val F1 0.267\n",
      "Epoch 20 | Loss 1.5626 | Val bal-acc 0.550 | Val F1 0.182\n",
      "Epoch 40 | Loss 1.1701 | Val bal-acc 0.522 | Val F1 0.167\n",
      "Epoch 60 | Loss 1.7770 | Val bal-acc 0.522 | Val F1 0.167\n",
      "Early stopping at epoch 61.\n",
      "Fold 0: val bal-acc=0.567, F1=0.533\n",
      "Epoch 00 | Loss 3.1399 | Val bal-acc 0.500 | Val F1 0.500\n",
      "Epoch 20 | Loss 1.2612 | Val bal-acc 0.444 | Val F1 0.348\n",
      "Epoch 40 | Loss 1.5550 | Val bal-acc 0.500 | Val F1 0.485\n",
      "Epoch 60 | Loss 1.2014 | Val bal-acc 0.500 | Val F1 0.485\n",
      "Early stopping at epoch 74.\n",
      "Fold 1: val bal-acc=0.556, F1=0.516\n",
      "Epoch 00 | Loss 4.4550 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 20 | Loss 1.4568 | Val bal-acc 0.471 | Val F1 0.000\n",
      "Epoch 40 | Loss 1.1032 | Val bal-acc 0.491 | Val F1 0.154\n",
      "Epoch 60 | Loss 1.1875 | Val bal-acc 0.491 | Val F1 0.154\n",
      "Early stopping at epoch 63.\n",
      "Fold 2: val bal-acc=0.632, F1=0.526\n",
      "Epoch 00 | Loss 3.2168 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.0353 | Val bal-acc 0.385 | Val F1 0.273\n",
      "Epoch 40 | Loss 0.8714 | Val bal-acc 0.335 | Val F1 0.190\n",
      "Epoch 60 | Loss 0.7113 | Val bal-acc 0.365 | Val F1 0.200\n",
      "Early stopping at epoch 76.\n",
      "Fold 3: val bal-acc=0.621, F1=0.429\n",
      "Epoch 00 | Loss 4.4787 | Val bal-acc 0.559 | Val F1 0.571\n",
      "Epoch 20 | Loss 1.4359 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 40 | Loss 0.8755 | Val bal-acc 0.529 | Val F1 0.556\n",
      "Epoch 60 | Loss 0.9561 | Val bal-acc 0.529 | Val F1 0.556\n",
      "Early stopping at epoch 68.\n",
      "Fold 4: val bal-acc=0.632, F1=0.526\n",
      "\n",
      "Seed_42_Publicity_3_Corr_(0.82, 0.84): KFold mean bal-acc 0.602  0.033\n",
      "Seed_42_Publicity_3_Corr_(0.82, 0.84): KFold mean F1      0.506  0.039\n",
      "\n",
      "---  Testing Recipe: Seed_42_Publicity_3_Corr_(0.87, 0.89) ---\n",
      "Building graph...\n",
      "Calculating corrolation...\n",
      "Building overlap graphs...\n",
      "NetworkX graph created. Nodes: 1345, Edges: 2198\n",
      "Scaling...\n",
      "Starting k-fold...\n",
      "Epoch 00 | Loss 3.8948 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 20 | Loss 1.1387 | Val bal-acc 0.483 | Val F1 0.316\n",
      "Epoch 40 | Loss 0.9198 | Val bal-acc 0.494 | Val F1 0.444\n",
      "Epoch 60 | Loss 0.8570 | Val bal-acc 0.489 | Val F1 0.250\n",
      "Epoch 80 | Loss 0.8393 | Val bal-acc 0.489 | Val F1 0.250\n",
      "Early stopping at epoch 84.\n",
      "Fold 0: val bal-acc=0.572, F1=0.308\n",
      "Epoch 00 | Loss 3.3507 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 20 | Loss 1.0527 | Val bal-acc 0.361 | Val F1 0.375\n",
      "Epoch 40 | Loss 0.8630 | Val bal-acc 0.444 | Val F1 0.457\n",
      "Epoch 60 | Loss 1.0077 | Val bal-acc 0.444 | Val F1 0.457\n",
      "Early stopping at epoch 66.\n",
      "Fold 1: val bal-acc=0.528, F1=0.483\n",
      "Epoch 00 | Loss 3.5029 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 20 | Loss 1.2160 | Val bal-acc 0.559 | Val F1 0.571\n",
      "Epoch 40 | Loss 0.8986 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 60 | Loss 0.7839 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 80 | Loss 0.8633 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Early stopping at epoch 88.\n",
      "Fold 2: val bal-acc=0.656, F1=0.621\n",
      "Epoch 00 | Loss 3.8527 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.3370 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 40 | Loss 0.9348 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 60 | Loss 1.1571 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Early stopping at epoch 66.\n",
      "Fold 3: val bal-acc=0.529, F1=0.556\n",
      "Epoch 00 | Loss 3.3655 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.3071 | Val bal-acc 0.529 | Val F1 0.556\n",
      "Epoch 40 | Loss 0.7821 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 60 | Loss 0.7510 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Early stopping at epoch 75.\n",
      "Fold 4: val bal-acc=0.762, F1=0.700\n",
      "\n",
      "Seed_42_Publicity_3_Corr_(0.87, 0.89): KFold mean bal-acc 0.609  0.089\n",
      "Seed_42_Publicity_3_Corr_(0.87, 0.89): KFold mean F1      0.533  0.134\n",
      "\n",
      "========== RUNNING BAKE-OFF FOR OUTER SEED: 123 ==========\n",
      "\n",
      "---  Testing Recipe: Seed_123_Publicity_5_Corr_(0.59, 0.61) ---\n",
      "Building graph...\n",
      "Calculating corrolation...\n",
      "Building overlap graphs...\n",
      "---  Error processing sequence: 'CASSL_ETQYF' | Message: non-standard AA ---\n",
      "---  Error processing sequence: 'CA_HF' | Message: non-standard AA ---\n",
      "Imputing NaNs in biochemical features... (20 missing values detected)\n",
      "NetworkX graph created. Nodes: 4581, Edges: 5124\n",
      "Scaling...\n",
      "Starting k-fold...\n",
      "Epoch 00 | Loss 6.0341 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 20 | Loss 1.7337 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 40 | Loss 1.4044 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 60 | Loss 1.7280 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Early stopping at epoch 73.\n",
      "Fold 0: val bal-acc=0.528, F1=0.541\n",
      "Epoch 00 | Loss 3.8382 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 20 | Loss 1.2132 | Val bal-acc 0.509 | Val F1 0.529\n",
      "Epoch 40 | Loss 0.8691 | Val bal-acc 0.438 | Val F1 0.452\n",
      "Epoch 60 | Loss 1.0415 | Val bal-acc 0.438 | Val F1 0.452\n",
      "Early stopping at epoch 69.\n",
      "Fold 1: val bal-acc=0.568, F1=0.562\n",
      "Epoch 00 | Loss 4.7225 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 20 | Loss 1.3447 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 40 | Loss 1.4313 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 60 | Loss 1.4979 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Early stopping at epoch 62.\n",
      "Fold 2: val bal-acc=0.571, F1=0.308\n",
      "Epoch 00 | Loss 3.7568 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.1138 | Val bal-acc 0.482 | Val F1 0.250\n",
      "Epoch 40 | Loss 1.0144 | Val bal-acc 0.488 | Val F1 0.500\n",
      "Epoch 60 | Loss 1.1243 | Val bal-acc 0.488 | Val F1 0.500\n",
      "Early stopping at epoch 65.\n",
      "Fold 3: val bal-acc=0.671, F1=0.533\n",
      "Epoch 00 | Loss 4.2065 | Val bal-acc 0.500 | Val F1 0.500\n",
      "Epoch 20 | Loss 1.4394 | Val bal-acc 0.500 | Val F1 0.500\n",
      "Epoch 40 | Loss 1.2168 | Val bal-acc 0.472 | Val F1 0.471\n",
      "Epoch 60 | Loss 1.1760 | Val bal-acc 0.472 | Val F1 0.471\n",
      "Early stopping at epoch 63.\n",
      "Fold 4: val bal-acc=0.556, F1=0.529\n",
      "\n",
      "Seed_123_Publicity_5_Corr_(0.59, 0.61): KFold mean bal-acc 0.578  0.048\n",
      "Seed_123_Publicity_5_Corr_(0.59, 0.61): KFold mean F1      0.495  0.094\n",
      "\n",
      "---  Testing Recipe: Seed_123_Publicity_5_Corr_(0.65, 0.67) ---\n",
      "Building graph...\n",
      "Calculating corrolation...\n",
      "Building overlap graphs...\n",
      "---  Error processing sequence: 'CA_HF' | Message: non-standard AA ---\n",
      "---  Error processing sequence: 'CASSL_ETQYF' | Message: non-standard AA ---\n",
      "Imputing NaNs in biochemical features... (20 missing values detected)\n",
      "NetworkX graph created. Nodes: 2606, Edges: 2526\n",
      "Scaling...\n",
      "Starting k-fold...\n",
      "Epoch 00 | Loss 5.4424 | Val bal-acc 0.444 | Val F1 0.000\n",
      "Epoch 20 | Loss 1.8826 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 40 | Loss 1.3563 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 60 | Loss 1.4249 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Early stopping at epoch 61.\n",
      "Fold 0: val bal-acc=0.589, F1=0.562\n",
      "Epoch 00 | Loss 7.1941 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.8670 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 40 | Loss 1.1321 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 60 | Loss 1.5902 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Early stopping at epoch 61.\n",
      "Fold 1: val bal-acc=0.509, F1=0.529\n",
      "Epoch 00 | Loss 4.6891 | Val bal-acc 0.318 | Val F1 0.296\n",
      "Epoch 20 | Loss 1.6085 | Val bal-acc 0.635 | Val F1 0.593\n",
      "Epoch 40 | Loss 0.9253 | Val bal-acc 0.559 | Val F1 0.571\n",
      "Epoch 60 | Loss 0.8296 | Val bal-acc 0.559 | Val F1 0.571\n",
      "Early stopping at epoch 68.\n",
      "Fold 2: val bal-acc=0.644, F1=0.583\n",
      "Epoch 00 | Loss 5.5421 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.2215 | Val bal-acc 0.532 | Val F1 0.353\n",
      "Epoch 40 | Loss 1.1823 | Val bal-acc 0.447 | Val F1 0.429\n",
      "Epoch 60 | Loss 0.7450 | Val bal-acc 0.447 | Val F1 0.429\n",
      "Early stopping at epoch 69.\n",
      "Fold 3: val bal-acc=0.700, F1=0.571\n",
      "Epoch 00 | Loss 3.2478 | Val bal-acc 0.528 | Val F1 0.514\n",
      "Epoch 20 | Loss 1.3105 | Val bal-acc 0.639 | Val F1 0.500\n",
      "Epoch 40 | Loss 1.4616 | Val bal-acc 0.528 | Val F1 0.400\n",
      "Epoch 60 | Loss 1.4512 | Val bal-acc 0.528 | Val F1 0.400\n",
      "Early stopping at epoch 62.\n",
      "Fold 4: val bal-acc=0.694, F1=0.588\n",
      "\n",
      "Seed_123_Publicity_5_Corr_(0.65, 0.67): KFold mean bal-acc 0.627  0.072\n",
      "Seed_123_Publicity_5_Corr_(0.65, 0.67): KFold mean F1      0.567  0.021\n",
      "\n",
      "---  Testing Recipe: Seed_123_Publicity_5_Corr_(0.74, 0.76) ---\n",
      "Building graph...\n",
      "Calculating corrolation...\n",
      "Building overlap graphs...\n",
      "---  Error processing sequence: 'CASSL_ETQYF' | Message: non-standard AA ---\n",
      "Imputing NaNs in biochemical features... (10 missing values detected)\n",
      "NetworkX graph created. Nodes: 950, Edges: 829\n",
      "Scaling...\n",
      "Starting k-fold...\n",
      "Epoch 00 | Loss 3.9186 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 20 | Loss 1.2330 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 40 | Loss 1.1379 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 60 | Loss 0.9340 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Early stopping at epoch 63.\n",
      "Fold 0: val bal-acc=0.539, F1=0.516\n",
      "Epoch 00 | Loss 2.5108 | Val bal-acc 0.529 | Val F1 0.556\n",
      "Epoch 20 | Loss 1.2640 | Val bal-acc 0.438 | Val F1 0.452\n",
      "Epoch 40 | Loss 0.9382 | Val bal-acc 0.538 | Val F1 0.545\n",
      "Epoch 60 | Loss 0.9473 | Val bal-acc 0.479 | Val F1 0.514\n",
      "Early stopping at epoch 70.\n",
      "Fold 1: val bal-acc=0.641, F1=0.500\n",
      "Epoch 00 | Loss 4.7734 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.3255 | Val bal-acc 0.450 | Val F1 0.500\n",
      "Epoch 40 | Loss 0.7957 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 60 | Loss 0.8332 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Early stopping at epoch 73.\n",
      "Fold 2: val bal-acc=0.621, F1=0.429\n",
      "Epoch 00 | Loss 3.3541 | Val bal-acc 0.550 | Val F1 0.182\n",
      "Epoch 20 | Loss 1.3687 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 40 | Loss 0.9360 | Val bal-acc 0.550 | Val F1 0.182\n",
      "Epoch 60 | Loss 1.0183 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Early stopping at epoch 66.\n",
      "Fold 3: val bal-acc=0.559, F1=0.571\n",
      "Epoch 00 | Loss 1.9767 | Val bal-acc 0.583 | Val F1 0.400\n",
      "Epoch 20 | Loss 1.7137 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 40 | Loss 1.2117 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 60 | Loss 1.0527 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Early stopping at epoch 60.\n",
      "Fold 4: val bal-acc=0.583, F1=0.400\n",
      "\n",
      "Seed_123_Publicity_5_Corr_(0.74, 0.76): KFold mean bal-acc 0.589  0.038\n",
      "Seed_123_Publicity_5_Corr_(0.74, 0.76): KFold mean F1      0.483  0.062\n",
      "\n",
      "---  Testing Recipe: Seed_123_Publicity_4_Corr_(0.62, 0.64) ---\n",
      "Building graph...\n",
      "Calculating corrolation...\n",
      "Building overlap graphs...\n",
      "---  Error processing sequence: 'CASSL_ETQYF' | Message: non-standard AA ---\n",
      "---  Error processing sequence: 'CA_HF' | Message: non-standard AA ---\n",
      "---  Error processing sequence: 'CASSQG_YEQYF' | Message: non-standard AA ---\n",
      "Imputing NaNs in biochemical features... (30 missing values detected)\n",
      "NetworkX graph created. Nodes: 7752, Edges: 9524\n",
      "Scaling...\n",
      "Starting k-fold...\n",
      "Epoch 00 | Loss 5.4238 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 20 | Loss 1.4418 | Val bal-acc 0.561 | Val F1 0.421\n",
      "Epoch 40 | Loss 1.1967 | Val bal-acc 0.639 | Val F1 0.526\n",
      "Epoch 60 | Loss 1.0337 | Val bal-acc 0.500 | Val F1 0.417\n",
      "Epoch 80 | Loss 1.2405 | Val bal-acc 0.528 | Val F1 0.435\n",
      "Early stopping at epoch 82.\n",
      "Fold 0: val bal-acc=0.672, F1=0.533\n",
      "Epoch 00 | Loss 5.3493 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 20 | Loss 1.5327 | Val bal-acc 0.491 | Val F1 0.154\n",
      "Epoch 40 | Loss 1.0549 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 60 | Loss 1.1015 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Early stopping at epoch 68.\n",
      "Fold 1: val bal-acc=0.582, F1=0.444\n",
      "Epoch 00 | Loss 5.3297 | Val bal-acc 0.541 | Val F1 0.286\n",
      "Epoch 20 | Loss 1.7708 | Val bal-acc 0.468 | Val F1 0.467\n",
      "Epoch 40 | Loss 1.0710 | Val bal-acc 0.529 | Val F1 0.556\n",
      "Epoch 60 | Loss 0.8442 | Val bal-acc 0.529 | Val F1 0.556\n",
      "Early stopping at epoch 71.\n",
      "Fold 2: val bal-acc=0.721, F1=0.625\n",
      "Epoch 00 | Loss 5.3501 | Val bal-acc 0.559 | Val F1 0.571\n",
      "Epoch 20 | Loss 1.7893 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 40 | Loss 1.4217 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 60 | Loss 1.8701 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Early stopping at epoch 60.\n",
      "Fold 3: val bal-acc=0.559, F1=0.571\n",
      "Epoch 00 | Loss 3.9348 | Val bal-acc 0.500 | Val F1 0.500\n",
      "Epoch 20 | Loss 1.4455 | Val bal-acc 0.500 | Val F1 0.500\n",
      "Epoch 40 | Loss 0.8762 | Val bal-acc 0.472 | Val F1 0.471\n",
      "Epoch 60 | Loss 0.9080 | Val bal-acc 0.528 | Val F1 0.514\n",
      "Early stopping at epoch 73.\n",
      "Fold 4: val bal-acc=0.639, F1=0.500\n",
      "\n",
      "Seed_123_Publicity_4_Corr_(0.62, 0.64): KFold mean bal-acc 0.635  0.059\n",
      "Seed_123_Publicity_4_Corr_(0.62, 0.64): KFold mean F1      0.535  0.061\n",
      "\n",
      "---  Testing Recipe: Seed_123_Publicity_4_Corr_(0.67, 0.69) ---\n",
      "Building graph...\n",
      "Calculating corrolation...\n",
      "Building overlap graphs...\n",
      "---  Error processing sequence: 'CASSL_ETQYF' | Message: non-standard AA ---\n",
      "---  Error processing sequence: 'CASSQG_YEQYF' | Message: non-standard AA ---\n",
      "Imputing NaNs in biochemical features... (20 missing values detected)\n",
      "NetworkX graph created. Nodes: 4895, Edges: 5351\n",
      "Scaling...\n",
      "Starting k-fold...\n",
      "Epoch 00 | Loss 4.7925 | Val bal-acc 0.561 | Val F1 0.545\n",
      "Epoch 20 | Loss 1.5163 | Val bal-acc 0.617 | Val F1 0.471\n",
      "Epoch 40 | Loss 1.3666 | Val bal-acc 0.533 | Val F1 0.529\n",
      "Epoch 60 | Loss 1.4392 | Val bal-acc 0.533 | Val F1 0.529\n",
      "Early stopping at epoch 64.\n",
      "Fold 0: val bal-acc=0.683, F1=0.609\n",
      "Epoch 00 | Loss 5.2067 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 2.1641 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 40 | Loss 1.0501 | Val bal-acc 0.509 | Val F1 0.529\n",
      "Epoch 60 | Loss 1.3736 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 80 | Loss 1.3168 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Early stopping at epoch 83.\n",
      "Fold 1: val bal-acc=0.594, F1=0.522\n",
      "Epoch 00 | Loss 4.8142 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.6886 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 40 | Loss 0.9539 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 60 | Loss 0.9376 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Early stopping at epoch 70.\n",
      "Fold 2: val bal-acc=0.574, F1=0.476\n",
      "Epoch 00 | Loss 5.8131 | Val bal-acc 0.624 | Val F1 0.545\n",
      "Epoch 20 | Loss 1.8384 | Val bal-acc 0.456 | Val F1 0.400\n",
      "Epoch 40 | Loss 1.7086 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 60 | Loss 1.8884 | Val bal-acc 0.529 | Val F1 0.556\n",
      "Early stopping at epoch 60.\n",
      "Fold 3: val bal-acc=0.624, F1=0.545\n",
      "Epoch 00 | Loss 4.3031 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 20 | Loss 2.4293 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 40 | Loss 1.3216 | Val bal-acc 0.444 | Val F1 0.000\n",
      "Epoch 60 | Loss 1.5291 | Val bal-acc 0.444 | Val F1 0.000\n",
      "Early stopping at epoch 61.\n",
      "Fold 4: val bal-acc=0.583, F1=0.333\n",
      "\n",
      "Seed_123_Publicity_4_Corr_(0.67, 0.69): KFold mean bal-acc 0.612  0.040\n",
      "Seed_123_Publicity_4_Corr_(0.67, 0.69): KFold mean F1      0.497  0.092\n",
      "\n",
      "---  Testing Recipe: Seed_123_Publicity_4_Corr_(0.75, 0.77) ---\n",
      "Building graph...\n",
      "Calculating corrolation...\n",
      "Building overlap graphs...\n",
      "---  Error processing sequence: 'CASSL_ETQYF' | Message: non-standard AA ---\n",
      "---  Error processing sequence: 'CASSQG_YEQYF' | Message: non-standard AA ---\n",
      "Imputing NaNs in biochemical features... (20 missing values detected)\n",
      "NetworkX graph created. Nodes: 2030, Edges: 1952\n",
      "Scaling...\n",
      "Starting k-fold...\n",
      "Epoch 00 | Loss 4.8999 | Val bal-acc 0.539 | Val F1 0.353\n",
      "Epoch 20 | Loss 1.0561 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 40 | Loss 1.0623 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 60 | Loss 1.0192 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Early stopping at epoch 68.\n",
      "Fold 0: val bal-acc=0.622, F1=0.571\n",
      "Epoch 00 | Loss 5.0585 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.5006 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 40 | Loss 1.1117 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 60 | Loss 1.0229 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Early stopping at epoch 64.\n",
      "Fold 1: val bal-acc=0.603, F1=0.500\n",
      "Epoch 00 | Loss 6.6617 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.0268 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 40 | Loss 1.0020 | Val bal-acc 0.403 | Val F1 0.125\n",
      "Epoch 60 | Loss 1.1364 | Val bal-acc 0.432 | Val F1 0.133\n",
      "Early stopping at epoch 68.\n",
      "Fold 2: val bal-acc=0.547, F1=0.533\n",
      "Epoch 00 | Loss 2.9172 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 20 | Loss 1.5175 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 40 | Loss 1.2561 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 60 | Loss 1.0870 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Early stopping at epoch 62.\n",
      "Fold 3: val bal-acc=0.582, F1=0.444\n",
      "Epoch 00 | Loss 4.4867 | Val bal-acc 0.556 | Val F1 0.500\n",
      "Epoch 20 | Loss 1.5915 | Val bal-acc 0.472 | Val F1 0.000\n",
      "Epoch 40 | Loss 1.3151 | Val bal-acc 0.472 | Val F1 0.000\n",
      "Epoch 60 | Loss 1.1613 | Val bal-acc 0.472 | Val F1 0.000\n",
      "Early stopping at epoch 63.\n",
      "Fold 4: val bal-acc=0.694, F1=0.588\n",
      "\n",
      "Seed_123_Publicity_4_Corr_(0.75, 0.77): KFold mean bal-acc 0.610  0.049\n",
      "Seed_123_Publicity_4_Corr_(0.75, 0.77): KFold mean F1      0.527  0.052\n",
      "\n",
      "---  Testing Recipe: Seed_123_Publicity_3_Corr_(0.76, 0.78) ---\n",
      "Building graph...\n",
      "Calculating corrolation...\n",
      "Building overlap graphs...\n",
      "---  Error processing sequence: 'CASSQG_YEQYF' | Message: non-standard AA ---\n",
      "---  Error processing sequence: 'CASSL_ETQYF' | Message: non-standard AA ---\n",
      "Imputing NaNs in biochemical features... (20 missing values detected)\n",
      "NetworkX graph created. Nodes: 6286, Edges: 7398\n",
      "Scaling...\n",
      "Starting k-fold...\n",
      "Epoch 00 | Loss 3.7857 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 20 | Loss 1.9515 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 40 | Loss 1.1665 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Epoch 60 | Loss 1.2894 | Val bal-acc 0.500 | Val F1 0.526\n",
      "Early stopping at epoch 65.\n",
      "Fold 0: val bal-acc=0.528, F1=0.541\n",
      "Epoch 00 | Loss 5.2899 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.4486 | Val bal-acc 0.429 | Val F1 0.471\n",
      "Epoch 40 | Loss 1.0510 | Val bal-acc 0.529 | Val F1 0.556\n",
      "Epoch 60 | Loss 0.8140 | Val bal-acc 0.529 | Val F1 0.556\n",
      "Early stopping at epoch 68.\n",
      "Fold 1: val bal-acc=0.568, F1=0.562\n",
      "Epoch 00 | Loss 3.7504 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.0821 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 40 | Loss 1.0451 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 60 | Loss 0.9706 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Early stopping at epoch 69.\n",
      "Fold 2: val bal-acc=0.565, F1=0.500\n",
      "Epoch 00 | Loss 5.3610 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.7880 | Val bal-acc 0.441 | Val F1 0.000\n",
      "Epoch 40 | Loss 1.0860 | Val bal-acc 0.568 | Val F1 0.562\n",
      "Epoch 60 | Loss 1.1931 | Val bal-acc 0.568 | Val F1 0.562\n",
      "Early stopping at epoch 69.\n",
      "Fold 3: val bal-acc=0.571, F1=0.308\n",
      "Epoch 00 | Loss 7.3741 | Val bal-acc 0.500 | Val F1 0.500\n",
      "Epoch 20 | Loss 1.4989 | Val bal-acc 0.500 | Val F1 0.500\n",
      "Epoch 40 | Loss 1.7120 | Val bal-acc 0.500 | Val F1 0.500\n",
      "Epoch 60 | Loss 1.2253 | Val bal-acc 0.500 | Val F1 0.500\n",
      "Early stopping at epoch 63.\n",
      "Fold 4: val bal-acc=0.722, F1=0.625\n",
      "\n",
      "Seed_123_Publicity_3_Corr_(0.76, 0.78): KFold mean bal-acc 0.591  0.068\n",
      "Seed_123_Publicity_3_Corr_(0.76, 0.78): KFold mean F1      0.507  0.108\n",
      "\n",
      "---  Testing Recipe: Seed_123_Publicity_3_Corr_(0.82, 0.84) ---\n",
      "Building graph...\n",
      "Calculating corrolation...\n",
      "Building overlap graphs...\n",
      "---  Error processing sequence: 'CASSL_ETQYF' | Message: non-standard AA ---\n",
      "Imputing NaNs in biochemical features... (10 missing values detected)\n",
      "NetworkX graph created. Nodes: 2754, Edges: 3097\n",
      "Scaling...\n",
      "Starting k-fold...\n",
      "Epoch 00 | Loss 3.2126 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 20 | Loss 1.4556 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 40 | Loss 1.2043 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 60 | Loss 1.2347 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Early stopping at epoch 60.\n",
      "Fold 0: val bal-acc=0.500, F1=0.000\n",
      "Epoch 00 | Loss 4.0565 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 20 | Loss 1.5312 | Val bal-acc 0.450 | Val F1 0.500\n",
      "Epoch 40 | Loss 0.9960 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 60 | Loss 0.9942 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 80 | Loss 1.0322 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Early stopping at epoch 86.\n",
      "Fold 1: val bal-acc=0.594, F1=0.522\n",
      "Epoch 00 | Loss 5.0711 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.2997 | Val bal-acc 0.388 | Val F1 0.400\n",
      "Epoch 40 | Loss 1.0763 | Val bal-acc 0.388 | Val F1 0.400\n",
      "Epoch 60 | Loss 1.0268 | Val bal-acc 0.388 | Val F1 0.400\n",
      "Early stopping at epoch 62.\n",
      "Fold 2: val bal-acc=0.588, F1=0.588\n",
      "Epoch 00 | Loss 5.3848 | Val bal-acc 0.400 | Val F1 0.457\n",
      "Epoch 20 | Loss 1.1506 | Val bal-acc 0.556 | Val F1 0.519\n",
      "Epoch 40 | Loss 0.9433 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 60 | Loss 0.8180 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Early stopping at epoch 71.\n",
      "Fold 3: val bal-acc=0.612, F1=0.471\n",
      "Epoch 00 | Loss 7.7927 | Val bal-acc 0.417 | Val F1 0.424\n",
      "Epoch 20 | Loss 1.9319 | Val bal-acc 0.444 | Val F1 0.143\n",
      "Epoch 40 | Loss 1.2643 | Val bal-acc 0.417 | Val F1 0.286\n",
      "Epoch 60 | Loss 1.1480 | Val bal-acc 0.417 | Val F1 0.286\n",
      "Early stopping at epoch 62.\n",
      "Fold 4: val bal-acc=0.778, F1=0.696\n",
      "\n",
      "Seed_123_Publicity_3_Corr_(0.82, 0.84): KFold mean bal-acc 0.614  0.090\n",
      "Seed_123_Publicity_3_Corr_(0.82, 0.84): KFold mean F1      0.455  0.240\n",
      "\n",
      "---  Testing Recipe: Seed_123_Publicity_3_Corr_(0.87, 0.89) ---\n",
      "Building graph...\n",
      "Calculating corrolation...\n",
      "Building overlap graphs...\n",
      "---  Error processing sequence: 'CASSL_ETQYF' | Message: non-standard AA ---\n",
      "Imputing NaNs in biochemical features... (10 missing values detected)\n",
      "NetworkX graph created. Nodes: 1067, Edges: 1326\n",
      "Scaling...\n",
      "Starting k-fold...\n",
      "Epoch 00 | Loss 3.9298 | Val bal-acc 0.556 | Val F1 0.556\n",
      "Epoch 20 | Loss 0.9298 | Val bal-acc 0.406 | Val F1 0.211\n",
      "Epoch 40 | Loss 1.0417 | Val bal-acc 0.450 | Val F1 0.486\n",
      "Epoch 60 | Loss 0.9194 | Val bal-acc 0.400 | Val F1 0.444\n",
      "Early stopping at epoch 66.\n",
      "Fold 0: val bal-acc=0.683, F1=0.609\n",
      "Epoch 00 | Loss 4.3066 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.3467 | Val bal-acc 0.509 | Val F1 0.529\n",
      "Epoch 40 | Loss 0.7955 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 60 | Loss 0.7971 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Early stopping at epoch 77.\n",
      "Fold 1: val bal-acc=0.594, F1=0.522\n",
      "Epoch 00 | Loss 3.2261 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 20 | Loss 1.3151 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 40 | Loss 0.8217 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 60 | Loss 0.9380 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Epoch 80 | Loss 0.9189 | Val bal-acc 0.500 | Val F1 0.000\n",
      "Early stopping at epoch 96.\n",
      "Fold 2: val bal-acc=0.694, F1=0.640\n",
      "Epoch 00 | Loss 2.7708 | Val bal-acc 0.538 | Val F1 0.545\n",
      "Epoch 20 | Loss 1.4258 | Val bal-acc 0.526 | Val F1 0.500\n",
      "Epoch 40 | Loss 1.1742 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Epoch 60 | Loss 1.0302 | Val bal-acc 0.500 | Val F1 0.541\n",
      "Early stopping at epoch 76.\n",
      "Fold 3: val bal-acc=0.621, F1=0.429\n",
      "Epoch 00 | Loss 3.2352 | Val bal-acc 0.639 | Val F1 0.581\n",
      "Epoch 20 | Loss 1.8538 | Val bal-acc 0.417 | Val F1 0.400\n",
      "Epoch 40 | Loss 1.3741 | Val bal-acc 0.417 | Val F1 0.400\n",
      "Epoch 60 | Loss 1.2273 | Val bal-acc 0.417 | Val F1 0.400\n",
      "Early stopping at epoch 60.\n",
      "Fold 4: val bal-acc=0.639, F1=0.581\n",
      "\n",
      "Seed_123_Publicity_3_Corr_(0.87, 0.89): KFold mean bal-acc 0.646  0.038\n",
      "Seed_123_Publicity_3_Corr_(0.87, 0.89): KFold mean F1      0.556  0.075\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "device = connect_gpu()\n",
    "\n",
    "for outer_seed in OUTER_SEEDS:\n",
    "    print(f\"\\n{'='*10} RUNNING BAKE-OFF FOR OUTER SEED: {outer_seed} {'='*10}\")\n",
    "    train_val_ids, test_ids, y_train_val, y_test = train_test_split(\n",
    "        valid_samples, labels, test_size=0.15, stratify=labels, random_state=outer_seed)\n",
    "    y_train_val_np = np.array(y_train_val) # For efficient indexing later\n",
    "    graph_universe_ids = train_val_ids + unlabeled_ids\n",
    "    for min_threshold in PUBLICITY_THRESHOLDS:\n",
    "        for corr_thresholds in CORRELATION_THRESHOLDS_PER_PUBLICITY[min_threshold]:\n",
    "            recipe_name = f\"Seed_{outer_seed}_Publicity_{min_threshold}_Corr_{corr_thresholds}\"\n",
    "            print(f\"\\n---  Testing Recipe: {recipe_name} ---\")\n",
    "            # 1. Create graph and node_meta file\n",
    "            print(\"Building graph...\")\n",
    "            test_graph, node_meta_df = create_graph_and_meta(\n",
    "                corr_thresholds, min_threshold, corr_files_path, graph_universe_ids\n",
    "            )\n",
    "            print(f\"NetworkX graph created. Nodes: {test_graph.number_of_nodes()}, Edges: {test_graph.number_of_edges()}\")\n",
    "            node_list = list(test_graph.nodes)\n",
    "            node_idx_filtered = {tcr: i for i, tcr in enumerate(node_list)}\n",
    "\n",
    "            test_graph_directed = test_graph.to_directed()\n",
    "            G_overlap_dgl_kfold = dgl.from_networkx(\n",
    "                test_graph_directed,  # <-- Pass the new directed graph\n",
    "                edge_attrs=['fast_weight', 'slow_weight']\n",
    "            )\n",
    "            # Combine the weights into a single 'feat' tensor for the GAT model\n",
    "            G_overlap_dgl_kfold.edata['feat'] = torch.stack([\n",
    "                G_overlap_dgl_kfold.edata['fast_weight'],\n",
    "                G_overlap_dgl_kfold.edata['slow_weight']\n",
    "            ], dim=1).float()\n",
    "            \n",
    "            # G_overlap_dgl_kfold = dgl.from_networkx(test_graph)\n",
    "\n",
    "            print(\"Scaling...\")\n",
    "            scaler = StandardScaler()\n",
    "            node_meta_scaled = node_meta_df.copy()\n",
    "            node_meta_scaled[cols_to_normalize_meta] = scaler.fit_transform(\n",
    "                node_meta_scaled[cols_to_normalize_meta]\n",
    "            )\n",
    "            # Build full dyn_feat_dict (all samples)\n",
    "            orig_dyn_feat_dict, presence_mask_dict = build_orig_dyn_feat_dict_for_filtered(\n",
    "                clonotype_df, node_idx_filtered, all_samples)\n",
    "            \n",
    "            # KFold cross-validation within the train/val split!\n",
    "            n_splits = 5\n",
    "            kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=outer_seed)\n",
    "            \n",
    "            # Create the master dataset for this recipe. Pass unscaled dyn feats.\n",
    "            y_train_val_series = pd.Series(y_train_val, index=train_val_ids)\n",
    "            trainval_dataset = SingleGraphSampleDataset(\n",
    "                G_overlap_dgl_kfold, node_meta_scaled,\n",
    "                orig_dyn_feat_dict, presence_mask_dict,\n",
    "                y_train_val_series\n",
    "            )\n",
    "\n",
    "            # --- FIX 2 & 3: Get TRUE dimensions and vocab sizes AFTER dataset creation ---\n",
    "            static_dim = trainval_dataset.g.ndata['static'].shape[1]\n",
    "            dyn_dim = 1 # Assuming this is fixed\n",
    "            v_vocab_size = trainval_dataset.g.ndata['V_gene_fast_id'].max().item() + 1\n",
    "            j_vocab_size = trainval_dataset.g.ndata['J_gene_fast_id'].max().item() + 1\n",
    "            v_slow_vocab_size = trainval_dataset.g.ndata['V_gene_slow_id'].max().item() + 1\n",
    "            j_slow_vocab_size = trainval_dataset.g.ndata['J_gene_slow_id'].max().item() + 1\n",
    "\n",
    "            fold_balacc = []\n",
    "            fold_f1 = []\n",
    "            print(\"Starting k-fold...\")\n",
    "            for fold, (train_idx, val_idx) in enumerate(kfold.split(train_val_ids, y_train_val_series)):\n",
    "                this_train_ids = [train_val_ids[i] for i in train_idx]\n",
    "                this_val_ids   = [train_val_ids[i] for i in val_idx]\n",
    "                # 2. Scale dynamic features: fit only on fold's train, apply to all\n",
    "                dyn_feat_dict_scaled = scale_dyn_dict(\n",
    "                    this_train_ids, all_samples, orig_dyn_feat_dict)\n",
    "\n",
    "                # Efficiently update dataset\n",
    "                trainval_dataset.dyn = dyn_feat_dict_scaled\n",
    "\n",
    "                train_set = sample_id_subset(trainval_dataset, this_train_ids)\n",
    "                val_set   = sample_id_subset(trainval_dataset, this_val_ids)\n",
    "\n",
    "                train_loader = DataLoader(train_set, batch_size=16, shuffle=True, collate_fn=collate_fn_filtered)\n",
    "                val_loader   = DataLoader(val_set,   batch_size=16, shuffle=False, collate_fn=collate_fn_filtered)\n",
    "\n",
    "                v_fast_max_size = torch.max(G_overlap_dgl_kfold.ndata[\"V_gene_fast_id\"]).item() + 1 \n",
    "                j_fast_max_size = torch.max(G_overlap_dgl_kfold.ndata[\"J_gene_fast_id\"]).item() + 1 \n",
    "                v_slow_max_size = torch.max(G_overlap_dgl_kfold.ndata[\"V_gene_slow_id\"]).item() + 1\n",
    "                j_slow_max_size = torch.max(G_overlap_dgl_kfold.ndata[\"J_gene_slow_id\"]).item() + 1\n",
    "\n",
    "                # Build and train model\n",
    "                model = MaskedGAT(\n",
    "                    static_dim=static_dim,\n",
    "                    dyn_dim=1,\n",
    "                    hidden_dim=GOOD_ENOUGH_CONFIG[\"hidden\"],\n",
    "                    num_layers=GOOD_ENOUGH_CONFIG[\"layers\"],\n",
    "                    dropout=GOOD_ENOUGH_CONFIG[\"dropout\"],\n",
    "                    readout=GOOD_ENOUGH_CONFIG[\"readout\"], \n",
    "                    v_vocab=v_fast_max_size,\n",
    "                    j_vocab=j_fast_max_size,\n",
    "                    v_slow_vocab=v_slow_max_size,\n",
    "                    j_slow_vocab=j_slow_max_size,\n",
    "                    num_heads=GOOD_ENOUGH_CONFIG['heads']\n",
    "                ).to(device)\n",
    "\n",
    "                # Get train labels simply and robustly\n",
    "                train_labels_fold = y_train_val_np[train_idx]\n",
    "                # Calculate weights ONLY from the training set to avoid data leakage.\n",
    "                class_counts = np.bincount(train_labels_fold)\n",
    "                weights = 1.0 / torch.tensor(class_counts, dtype=torch.float32)\n",
    "                criterion = nn.CrossEntropyLoss(weight=weights.to(device))\n",
    "                opt = torch.optim.AdamW(model.parameters(), lr=GOOD_ENOUGH_CONFIG[\"lr\"], weight_decay=GOOD_ENOUGH_CONFIG[\"wd\"])\n",
    "                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='max', factor=0.2, patience=10)\n",
    "\n",
    "                best_ba = 0; best_f1 = 0\n",
    "                patience = 60\n",
    "                epochs_since_best = 0\n",
    "                max_epochs = 200\n",
    "                best_state = None\n",
    "                for epoch in range(max_epochs):\n",
    "                    train_loss = train_one_epoch(model, train_loader, device, criterion, opt)\n",
    "                    ba, f1 = evaluate(model, val_loader, device)\n",
    "                    # The scheduler looks at the validation balanced accuracy to make a decision\n",
    "                    scheduler.step(ba) \n",
    "                    if epoch % 20 == 0:\n",
    "                        print(f\"Epoch {epoch:02d} | Loss {train_loss:.4f} | Val bal-acc {ba:.3f} | Val F1 {f1:.3f}\")\n",
    "                    if ba > best_ba:\n",
    "                        best_ba = ba; best_f1 = f1\n",
    "                        best_state = copy.deepcopy(model.state_dict())\n",
    "                        epochs_since_best = 0\n",
    "                    else:\n",
    "                        epochs_since_best += 1\n",
    "                    if epochs_since_best >= patience:\n",
    "                        print(f\"Early stopping at epoch {epoch}.\")\n",
    "                        break\n",
    "\n",
    "                fold_balacc.append(best_ba)\n",
    "                fold_f1.append(best_f1)\n",
    "                print(f\"Fold {fold}: val bal-acc={best_ba:.3f}, F1={best_f1:.3f}\")\n",
    "\n",
    "            print(f\"\\n{recipe_name}: KFold mean bal-acc {np.mean(fold_balacc):.3f}  {np.std(fold_balacc):.3f}\")\n",
    "            print(f\"{recipe_name}: KFold mean F1      {np.mean(fold_f1):.3f}  {np.std(fold_f1):.3f}\")\n",
    "            all_bake_off_results.append({\n",
    "                'outer_seed': outer_seed,\n",
    "                'recipe_name': recipe_name,\n",
    "                'mean_ba_for_seed': np.mean(fold_balacc)\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5f69bad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---  FINAL CHAMPIONSHIP RESULTS  ---\n",
      "Average performance of each graph recipe across all outer seeds:\n",
      "recipe_name\n",
      "Seed_42_Publicity_4_Corr_(0.75, 0.77)     0.663595\n",
      "Seed_123_Publicity_3_Corr_(0.87, 0.89)    0.646209\n",
      "Seed_42_Publicity_4_Corr_(0.67, 0.69)     0.641242\n",
      "Seed_123_Publicity_4_Corr_(0.62, 0.64)    0.634575\n",
      "Seed_0_Publicity_5_Corr_(0.74, 0.76)      0.633399\n",
      "Seed_123_Publicity_5_Corr_(0.65, 0.67)    0.627255\n",
      "Seed_0_Publicity_5_Corr_(0.59, 0.61)      0.616797\n",
      "Seed_42_Publicity_5_Corr_(0.65, 0.67)     0.615098\n",
      "Seed_123_Publicity_3_Corr_(0.82, 0.84)    0.614379\n",
      "Seed_0_Publicity_3_Corr_(0.76, 0.78)      0.613922\n",
      "Seed_123_Publicity_4_Corr_(0.67, 0.69)    0.611569\n",
      "Seed_123_Publicity_4_Corr_(0.75, 0.77)    0.609804\n",
      "Seed_42_Publicity_3_Corr_(0.87, 0.89)     0.609412\n",
      "Seed_0_Publicity_4_Corr_(0.75, 0.77)      0.607320\n",
      "Seed_42_Publicity_5_Corr_(0.59, 0.61)     0.604575\n",
      "Seed_0_Publicity_3_Corr_(0.82, 0.84)      0.602418\n",
      "Seed_42_Publicity_3_Corr_(0.82, 0.84)     0.601503\n",
      "Seed_42_Publicity_4_Corr_(0.62, 0.64)     0.596144\n",
      "Seed_0_Publicity_3_Corr_(0.87, 0.89)      0.591699\n",
      "Seed_123_Publicity_3_Corr_(0.76, 0.78)    0.590588\n",
      "Seed_123_Publicity_5_Corr_(0.74, 0.76)    0.588562\n",
      "Seed_123_Publicity_5_Corr_(0.59, 0.61)    0.578431\n",
      "Seed_42_Publicity_3_Corr_(0.76, 0.78)     0.576993\n",
      "Seed_0_Publicity_4_Corr_(0.67, 0.69)      0.569869\n",
      "Seed_0_Publicity_4_Corr_(0.62, 0.64)      0.568105\n",
      "Seed_42_Publicity_5_Corr_(0.74, 0.76)     0.567843\n",
      "Seed_0_Publicity_5_Corr_(0.65, 0.67)      0.560654\n",
      "Name: mean_ba_for_seed, dtype: float64\n",
      "\n",
      "And the winner is... Seed_42_Publicity_4_Corr_(0.75, 0.77)!\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(all_bake_off_results)\n",
    "\n",
    "# Use groupby to calculate the final average performance for each recipe across all seeds\n",
    "final_scores = results_df.groupby('recipe_name')['mean_ba_for_seed'].mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n---  FINAL CHAMPIONSHIP RESULTS  ---\")\n",
    "print(\"Average performance of each graph recipe across all outer seeds:\")\n",
    "print(final_scores)\n",
    "\n",
    "best_recipe_name = final_scores.index[0]\n",
    "print(f\"\\nAnd the winner is... {best_recipe_name}!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
